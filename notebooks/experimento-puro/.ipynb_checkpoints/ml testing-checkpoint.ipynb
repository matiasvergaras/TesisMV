{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c0175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import sys\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "\n",
    "# Data treatment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Base classifiers\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "\n",
    "# Multilabel classifiers - Problem Transformation\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.ensemble import RakelD\n",
    "\n",
    "# Multilabel classifiers - Algorithm Adaptation\n",
    "from skmultilearn.adapt import BRkNNaClassifier\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.adapt import MLTSVM\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Embedding classifiers\n",
    "#from skmultilearn.embedding import OpenNetworkEmbedder, CLEMS, SKLearnEmbedder\n",
    "#from sklearn.manifold import SpectralEmbedding\n",
    "#from skmultilearn.cluster import LabelCooccurrenceGraphBuilder\n",
    "#from skmultilearn.embedding import EmbeddingClassifier\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04df5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(mlb_estimator, xtrain, ytrain, xtest, ytest, model=None):\n",
    "    \"\"\"Builds a multilabel estimator and runs it over a given train and test data,\n",
    "       with an optional base classifier model.\n",
    "\n",
    "    Parameters:\n",
    "    mlb_estimator (mlb classifier): a PROBLEM_TRANSFORMATION or ALGORITHM_ADAPTATION \n",
    "                                    method from sklearn-multilabel\n",
    "    xtrain, ytrain, xtest, ytest (np arrays): train and test data\n",
    "    model (Base Estimator): optional, ignored if mlb_estimator is part of \n",
    "                            ALGORITHM_ADAPTATION methods. Base classifier to be \n",
    "                            used with the PROBLEM_TRANSFORMATION methods.\n",
    "\n",
    "    Returns:\n",
    "    (dict, np.array): dict with metrics (exact match, hamming loss and score) \n",
    "                      and array of predictions.\n",
    "    \"\"\"\n",
    "    xtrain = sparse.csr_matrix(xtrain)\n",
    "    ytrain = sparse.csr_matrix(ytrain)\n",
    "    xtest = sparse.csr_matrix(xtest)\n",
    "    ytest = sparse.csr_matrix(ytest)\n",
    "    if model:\n",
    "      clf = mlb_estimator(model)\n",
    "    else:\n",
    "      clf = mlb_estimator\n",
    "    clf.fit(xtrain, ytrain)\n",
    "    clf_predictions = clf.predict(xtest)\n",
    "    return clf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ace72a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSF_METHODS = {\"BR\": BinaryRelevance, \"LP\": LabelPowerset,\n",
    "                  \"CC\": ClassifierChain, \"RakelD\": RakelD}\n",
    "mlknn = MLkNN(k=1, s=1)\n",
    "mltsvm = MLTSVM(c_k=4)\n",
    "brknna = BRkNNaClassifier(k=1)\n",
    "ADAPT_METHODS = {\"BRkNN\": brknna, \"MLkNN\": mlknn, \"MLTSVM\": mltsvm}\n",
    "BASE_CLASSIFIERS = {\"LR\": LogisticRegression(solver='lbfgs'),\n",
    "                    \"SVC\": svm.SVC(), \"DT\": tree.DecisionTreeClassifier(), \"GNB\": GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12812c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'augmented_train_df.json'\n",
    "val_filename = 'val_df.json'\n",
    "test_filename = 'test_df.json'\n",
    "K=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce244bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando con i=26\n",
      "Usando top_labels previamente generados para 26 labels\n",
      "top labels previamente generado contiene 26 etiquetas\n",
      "-Probando suerte con BRkNN\n",
      "---AVG Micro F1: 0.2156\n",
      "---AVG Micro recall: 0.2168\n",
      "---AVG Micro precision: 0.2145\n",
      "---AVG Accuracy: 0.0155\n",
      "---AVG Hamming Loss: 0.2048\n",
      "---AVG Exact Match Ratio: 0.0155\n",
      "---AVG Hamming Score: 0.1169\n",
      "---AVG 1-Match Ratio: 0.433\n",
      "---AVG 2-Match Ratio: 0.2474\n",
      "---AVG 3-Match Ratio: 0.0361\n",
      "---AVG 4-Match Ratio: 0.0052\n",
      "---AVG 5-Match Ratio: 0.0052\n",
      "\n",
      "-Probando suerte con MLkNN\n",
      "---AVG Micro F1: 0.2156\n",
      "---AVG Micro recall: 0.2168\n",
      "---AVG Micro precision: 0.2145\n",
      "---AVG Accuracy: 0.0155\n",
      "---AVG Hamming Loss: 0.2048\n",
      "---AVG Exact Match Ratio: 0.0155\n",
      "---AVG Hamming Score: 0.1169\n",
      "---AVG 1-Match Ratio: 0.433\n",
      "---AVG 2-Match Ratio: 0.2474\n",
      "---AVG 3-Match Ratio: 0.0361\n",
      "---AVG 4-Match Ratio: 0.0052\n",
      "---AVG 5-Match Ratio: 0.0052\n",
      "\n",
      "-Probando suerte con MLTSVM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m pruner\u001b[38;5;241m.\u001b[39mfilter_df(labels_train) \u001b[38;5;66;03m# reduce labels to most freq\u001b[39;00m\n\u001b[0;32m    134\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m pruner\u001b[38;5;241m.\u001b[39mfilter_df(labels_test) \u001b[38;5;66;03m# in both train and test\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m predictions_i \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m metrics \u001b[38;5;241m=\u001b[39m KunischMetrics(Y_test\u001b[38;5;241m.\u001b[39mvalues, predictions_i)\n\u001b[0;32m    139\u001b[0m micro_f1 \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mf1(average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(mlb_estimator, xtrain, ytrain, xtest, ytest, model)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m   clf \u001b[38;5;241m=\u001b[39m mlb_estimator\n\u001b[1;32m---> 25\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m clf_predictions \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(xtest)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clf_predictions\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV38\\lib\\site-packages\\skmultilearn\\adapt\\mltsvm.py:118\u001b[0m, in \u001b[0;36mMLTSVM.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    115\u001b[0m Q_k \u001b[38;5;241m=\u001b[39m (Q_k \u001b[38;5;241m+\u001b[39m Q_k\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Calculate other\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m alpha_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_successive_overrelaxation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msor_omega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk_bk[label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mQ_knoPrefixGk\u001b[38;5;241m.\u001b[39mdot(alpha_k)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV38\\lib\\site-packages\\skmultilearn\\adapt\\mltsvm.py:150\u001b[0m, in \u001b[0;36mMLTSVM._successive_overrelaxation\u001b[1;34m(self, omegaW, Q)\u001b[0m\n\u001b[0;32m    148\u001b[0m oldAlpha \u001b[38;5;241m=\u001b[39m oldnew_alpha\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, small_l):  \u001b[38;5;66;03m# It's from last alpha to first\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     oldnew_alpha[j] \u001b[38;5;241m=\u001b[39m oldAlpha[j] \u001b[38;5;241m-\u001b[39m omegaW \u001b[38;5;241m*\u001b[39m D_inv[j] \u001b[38;5;241m*\u001b[39m (\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43moldnew_alpha\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    151\u001b[0m oldnew_alpha \u001b[38;5;241m=\u001b[39m oldnew_alpha\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_k)\n\u001b[0;32m    152\u001b[0m alfa_norm_change \u001b[38;5;241m=\u001b[39m norm(oldnew_alpha \u001b[38;5;241m-\u001b[39m oldAlpha)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "#LABELS_IN_STUDY = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24,\n",
    "#            26, 30, 34, 38, 40, 50, 60, 80, 100]\n",
    "LABELS_IN_STUDY = [26]\n",
    "#for i in range(26, 300, 10):\n",
    "#    LABELS_IN_STUDY.append(i)\n",
    "\n",
    "exp_exact_match = {}\n",
    "exp_hscore = {}\n",
    "exp_hloss = {}\n",
    "exp_f1 = {}\n",
    "exp_precision = {}\n",
    "exp_recall = {}\n",
    "exp_acc = {}\n",
    "exp_1mr = {}\n",
    "exp_2mr = {}\n",
    "exp_3mr = {}\n",
    "exp_4mr = {}\n",
    "exp_5mr = {}\n",
    "\n",
    "for meth_name in TRANSF_METHODS.keys():\n",
    "  for base_name in BASE_CLASSIFIERS.keys():\n",
    "    exp_exact_match[meth_name + \"_\" + base_name] = []\n",
    "    exp_hscore[meth_name + \"_\" + base_name] = []\n",
    "    exp_hloss[meth_name + \"_\" + base_name] = []\n",
    "    exp_f1[meth_name + \"_\" + base_name] = []\n",
    "    exp_precision[meth_name + \"_\" + base_name] = []\n",
    "    exp_recall[meth_name + \"_\" + base_name] = []\n",
    "    exp_acc[meth_name + \"_\" + base_name] = []\n",
    "    exp_1mr[meth_name + \"_\" + base_name] = []\n",
    "    exp_2mr[meth_name + \"_\" + base_name] = []\n",
    "    exp_3mr[meth_name + \"_\" + base_name] = []\n",
    "    exp_4mr[meth_name + \"_\" + base_name] = []\n",
    "    exp_5mr[meth_name + \"_\" + base_name] = []\n",
    "\n",
    "for meth_name in ADAPT_METHODS.keys():\n",
    "  exp_exact_match[meth_name] = []\n",
    "  exp_hscore[meth_name] = []\n",
    "  exp_hloss[meth_name] = []\n",
    "  exp_f1[meth_name] = []\n",
    "  exp_precision[meth_name] = []\n",
    "  exp_recall[meth_name] = []\n",
    "  exp_acc[meth_name] = []\n",
    "  exp_1mr[meth_name] = []\n",
    "  exp_2mr[meth_name] = []\n",
    "  exp_3mr[meth_name] = []\n",
    "  exp_4mr[meth_name] = []\n",
    "  exp_5mr[meth_name] = []\n",
    "\n",
    "PREVIOUS_LABELS = 0\n",
    "USED_FREQS = []\n",
    "\n",
    "output_dir = os.path.join('outputs', 'base-rn18')\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "for i in LABELS_IN_STUDY:\n",
    "  pruner = KunischPruner(i)\n",
    "  print(\"Comenzando con i={}\".format(i))\n",
    "    \n",
    "  # Carga o generacion de top labels\n",
    "  top_labels = None\n",
    "  if not os.path.isfile(os.path.join('labels', f'top_{i}L.pickle')):\n",
    "        save = input(f\"Se creará un archivo nuevo para {i} labels con el fold 0. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            labels_dir = os.path.join('labels')\n",
    "            train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "            top_labels = pruner.filter_labels(train_labels)\n",
    "            with open(os.path.join('labels', f'top_{i}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "  else:\n",
    "        print(f\"Usando top_labels previamente generados para {i} labels\")\n",
    "        with open(os.path.join('labels', f'top_{i}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "        print(f\"top labels previamente generado contiene {len(top_labels)} etiquetas\")\n",
    "  \n",
    "  pruner.set_top_labels(top_labels)\n",
    "        \n",
    "  if len(top_labels) == PREVIOUS_LABELS:\n",
    "        print(f\"Al intentar usar {i} labels, se repitió el valor previo {PREVIOUS_LABELS}. Saltando iteración.\")\n",
    "        continue\n",
    "        \n",
    "  PREVIOUS_LABELS = len(top_labels)\n",
    "  USED_FREQS.append(i)\n",
    "  \n",
    "\n",
    "  for meth_name, method in ADAPT_METHODS.items():\n",
    "        print(\"-Probando suerte con\", meth_name)\n",
    "\n",
    "        sum_f1 = 0\n",
    "        sum_recall = 0\n",
    "        sum_precision = 0\n",
    "        sum_acc = 0\n",
    "        sum_hl = 0\n",
    "        sum_emr = 0\n",
    "        sum_hs = 0\n",
    "        \n",
    "        sum_mr1 = 0\n",
    "        sum_mr2 = 0\n",
    "        sum_mr3 = 0\n",
    "        sum_mr4 = 0\n",
    "        sum_mr5 = 0\n",
    "\n",
    "\n",
    "        features_dir = os.path.join('features', 'rn18')\n",
    "        labels_dir = os.path.join('labels')\n",
    "        \n",
    "        features_train = pd.read_json(os.path.join(features_dir, train_filename), orient='index')\n",
    "        features_val = pd.read_json(os.path.join(features_dir, val_filename), orient='index')\n",
    "        features_test = pd.read_json(os.path.join(features_dir, test_filename), orient='index')\n",
    "\n",
    "        labels_train = pd.read_json(os.path.join(labels_dir, train_filename), orient='index')\n",
    "        labels_val = pd.read_json(os.path.join(labels_dir, val_filename), orient='index')\n",
    "        labels_test = pd.read_json(os.path.join(labels_dir, test_filename), orient='index')\n",
    "\n",
    "                \n",
    "        #shuffled = labels_train.index.values\n",
    "        #random.shuffle(shuffled)\n",
    "        \n",
    "        #labels_train.set_index(pd.Index(shuffled))\n",
    "        #display(labels_train)\n",
    "        \n",
    "        #shuffled_test = labels_test.index.values\n",
    "        #random.shuffle(shuffled_test)\n",
    "        #labels_test.set_index(pd.Index(shuffled_test))\n",
    "\n",
    "        # Dataset creation\n",
    "        X_train = features_train\n",
    "        X_test = features_test\n",
    "\n",
    "        Y_train = pruner.filter_df(labels_train) # reduce labels to most freq\n",
    "        Y_test = pruner.filter_df(labels_test) # in both train and test\n",
    "\n",
    "        predictions_i = build_model(method, X_train, Y_train, X_test, Y_test)\n",
    "        metrics = KunischMetrics(Y_test.values, predictions_i)\n",
    "\n",
    "        micro_f1 = metrics.f1(average='micro')\n",
    "        micro_recall = metrics.recall(average='micro')\n",
    "        micro_precision = metrics.precision(average='micro')\n",
    "        acc = metrics.acc()\n",
    "        hl = metrics.hl()\n",
    "        emr = metrics.emr()\n",
    "        hs = metrics.hs()\n",
    "\n",
    "        mr1 = metrics.mr1()\n",
    "        mr2 = metrics.mr2()\n",
    "        mr3 = metrics.mr3()\n",
    "        mr4 = metrics.mr4()\n",
    "        mr5 = metrics.mr5()\n",
    "\n",
    "        #print(\"---Micro F1:\", micro_f1)\n",
    "        #print(\"---Micro recall:\", micro_recall)\n",
    "        #print(\"---Micro precision:\", micro_precision)\n",
    "        #print(\"---Accuracy:\", acc)\n",
    "        #print(\"---Hamming Loss:\", hl)\n",
    "        #print(\"---Exact Match Ratio:\", emr)\n",
    "        #print(\"---Hamming Score:\", hs)\n",
    "        #print(\"---5-Match Ratio:\", mr5)\n",
    "\n",
    "        sum_f1 += micro_f1\n",
    "        sum_recall += micro_recall\n",
    "        sum_precision += micro_precision\n",
    "        sum_acc += acc\n",
    "        sum_hl += hl \n",
    "        sum_emr += emr\n",
    "        sum_hs += hs\n",
    "\n",
    "        sum_mr1 += mr1\n",
    "        sum_mr2 += mr2\n",
    "        sum_mr3 += mr3\n",
    "        sum_mr4 += mr4\n",
    "        sum_mr5 += mr5\n",
    "        #print(\"\")\n",
    "\n",
    "        avg_f1 = round(sum_f1/K, 4)\n",
    "        avg_recall = round(sum_recall/K, 4)\n",
    "        avg_precision = round(sum_precision/K, 4)\n",
    "        avg_acc = round(sum_acc/K, 4)\n",
    "        avg_hl = round(sum_hl/K, 4)\n",
    "        avg_emr = round(sum_emr/K, 4)\n",
    "        avg_hs = round(sum_hs/K, 4)\n",
    "        \n",
    "        avg_mr1 = round(sum_mr1/K, 4)\n",
    "        avg_mr2 = round(sum_mr2/K, 4)\n",
    "        avg_mr3 = round(sum_mr3/K, 4)\n",
    "        avg_mr4 = round(sum_mr4/K, 4)\n",
    "        avg_mr5 = round(sum_mr5/K, 4)\n",
    "        \n",
    "        print(\"---AVG Micro F1:\", avg_f1)\n",
    "        print(\"---AVG Micro recall:\", avg_recall)\n",
    "        print(\"---AVG Micro precision:\", avg_precision)\n",
    "        print(\"---AVG Accuracy:\", avg_acc)\n",
    "        print(\"---AVG Hamming Loss:\", avg_hl)\n",
    "        print(\"---AVG Exact Match Ratio:\", avg_emr)\n",
    "        print(\"---AVG Hamming Score:\", avg_hs)\n",
    "        print(\"---AVG 1-Match Ratio:\", avg_mr1)\n",
    "        print(\"---AVG 2-Match Ratio:\", avg_mr2)\n",
    "        print(\"---AVG 3-Match Ratio:\", avg_mr3)\n",
    "        print(\"---AVG 4-Match Ratio:\", avg_mr4)\n",
    "        print(\"---AVG 5-Match Ratio:\", avg_mr5)\n",
    "            \n",
    "        exp_exact_match[meth_name].append(avg_emr)\n",
    "        exp_hscore[meth_name].append(avg_hs)\n",
    "        exp_hloss[meth_name].append(avg_hl)\n",
    "        exp_precision[meth_name].append(avg_precision)\n",
    "        exp_recall[meth_name].append(avg_recall)\n",
    "        exp_f1[meth_name].append(avg_f1)\n",
    "        exp_acc[meth_name].append(avg_acc)\n",
    "        \n",
    "        exp_1mr[meth_name].append(avg_mr1)\n",
    "        exp_2mr[meth_name].append(avg_mr2)\n",
    "        exp_3mr[meth_name].append(avg_mr3)\n",
    "        exp_4mr[meth_name].append(avg_mr4)\n",
    "        exp_5mr[meth_name].append(avg_mr5)\n",
    "        \n",
    "        print(\"\")\n",
    "        \n",
    "  # Linear regression and SVC will raise error if Y_train is composed by only one class\n",
    "  for meth_name, method in TRANSF_METHODS.items():\n",
    "        for base_name, classifier in BASE_CLASSIFIERS.items():\n",
    "            print(\"-Probando suerte con\", meth_name, base_name)\n",
    "\n",
    "            sum_f1 = 0\n",
    "            sum_recall = 0\n",
    "            sum_precision = 0\n",
    "            sum_acc = 0\n",
    "            sum_hl = 0\n",
    "            sum_emr = 0\n",
    "            sum_hs = 0\n",
    "            \n",
    "            sum_mr1 = 0\n",
    "            sum_mr2 = 0\n",
    "            sum_mr3 = 0\n",
    "            sum_mr4 = 0\n",
    "            sum_mr5 = 0\n",
    "\n",
    "\n",
    "            features_dir =  os.path.join('features', 'rn18')\n",
    "            labels_dir = os.path.join('labels')\n",
    "            features_train = pd.read_json(os.path.join(features_dir, train_filename), orient='index')\n",
    "            features_val = pd.read_json(os.path.join(features_dir, val_filename), orient='index')\n",
    "            features_test = pd.read_json(os.path.join(features_dir, test_filename), orient='index')\n",
    "            labels_train = pd.read_json(os.path.join(labels_dir, train_filename), orient='index')\n",
    "            labels_val = pd.read_json(os.path.join(labels_dir, val_filename), orient='index')\n",
    "            labels_test = pd.read_json(os.path.join(labels_dir, test_filename), orient='index')\n",
    "\n",
    "            # Dataset creation\n",
    "            X_train = features_train\n",
    "            X_test = features_test \n",
    "\n",
    "            Y_train = pruner.filter_df(labels_train) # reduce labels to most freq\n",
    "            Y_test = pruner.filter_df(labels_test) # in both train and test\n",
    "\n",
    "            predictions_i = build_model(method, X_train, Y_train, X_test, Y_test, model=classifier)\n",
    "            metrics = KunischMetrics(Y_test.values, predictions_i)\n",
    "\n",
    "            micro_f1 = metrics.f1(average='micro')\n",
    "            micro_recall = metrics.recall(average='micro')\n",
    "            micro_precision = metrics.precision(average='micro')\n",
    "            acc = metrics.acc()\n",
    "            hl = metrics.hl()\n",
    "            emr = metrics.emr()\n",
    "            hs = metrics.hs()\n",
    "\n",
    "            mr1 = metrics.mr1()\n",
    "            mr2 = metrics.mr2()\n",
    "            mr3 = metrics.mr3()\n",
    "            mr4 = metrics.mr4()\n",
    "            mr5 = metrics.mr5()\n",
    "            #print(\"---Micro F1:\", micro_f1)\n",
    "            #print(\"---Micro recall:\", micro_recall)\n",
    "            #print(\"---Micro precision:\", micro_precision)\n",
    "            #print(\"---Accuracy:\", acc)\n",
    "            #print(\"---Hamming Loss:\", hl)\n",
    "            #print(\"---Exact Match Ratio:\", emr)\n",
    "            #print(\"---Hamming Score:\", hs)\n",
    "            #print(\"---5-Match Ratio:\", mr5)\n",
    "\n",
    "            sum_f1 += micro_f1\n",
    "            sum_recall += micro_recall\n",
    "            sum_precision += micro_precision\n",
    "            sum_acc += acc\n",
    "            sum_hl += hl \n",
    "            sum_emr += emr\n",
    "            sum_hs += hs\n",
    "                \n",
    "            sum_mr1 += mr1\n",
    "            sum_mr2 += mr2\n",
    "            sum_mr3 += mr3\n",
    "            sum_mr4 += mr4\n",
    "            sum_mr5 += mr5\n",
    "            #print(\"\")\n",
    "\n",
    "            avg_f1 = round(sum_f1/K, 4)\n",
    "            avg_recall = round(sum_recall/K, 4)\n",
    "            avg_precision = round(sum_precision/K, 4)\n",
    "            avg_acc = round(sum_acc/K, 4)\n",
    "            avg_hl = round(sum_hl/K, 4)\n",
    "            avg_emr = round(sum_emr/K, 4)\n",
    "            avg_hs = round(sum_hs/K, 4)\n",
    "            \n",
    "            avg_mr1 = round(sum_mr1/K, 4)\n",
    "            avg_mr2 = round(sum_mr2/K, 4)\n",
    "            avg_mr3 = round(sum_mr3/K, 4)\n",
    "            avg_mr4 = round(sum_mr4/K, 4)\n",
    "            avg_mr5 = round(sum_mr5/K, 4)\n",
    "\n",
    "            print(\"---AVG Micro F1:\", avg_f1)\n",
    "            print(\"---AVG Micro recall:\", avg_recall)\n",
    "            print(\"---AVG Micro precision:\", avg_precision)\n",
    "            print(\"---AVG Accuracy:\", avg_acc)\n",
    "            print(\"---AVG Hamming Loss:\", avg_hl)\n",
    "            print(\"---AVG Exact Match Ratio:\", avg_emr)\n",
    "            print(\"---AVG Hamming Score:\", avg_hs)\n",
    "            print(\"---AVG 1-Match Ratio:\", avg_mr1)\n",
    "            print(\"---AVG 2-Match Ratio:\", avg_mr2)\n",
    "            print(\"---AVG 3-Match Ratio:\", avg_mr3)\n",
    "            print(\"---AVG 4-Match Ratio:\", avg_mr4)\n",
    "            print(\"---AVG 5-Match Ratio:\", avg_mr5)\n",
    "\n",
    "            exp_exact_match[meth_name + \"_\" + base_name].append(avg_emr)\n",
    "            exp_hscore[meth_name + \"_\" + base_name].append(avg_hs)\n",
    "            exp_hloss[meth_name + \"_\" + base_name].append(avg_hl)\n",
    "            exp_precision[meth_name + \"_\" + base_name].append(avg_precision)\n",
    "            exp_recall[meth_name + \"_\" + base_name].append(avg_recall)\n",
    "            exp_f1[meth_name + \"_\" + base_name].append(avg_f1)\n",
    "            exp_acc[meth_name + \"_\" + base_name].append(avg_acc)\n",
    "            exp_1mr[meth_name + \"_\" + base_name].append(avg_mr1)\n",
    "            exp_2mr[meth_name + \"_\" + base_name].append(avg_mr2)\n",
    "            exp_3mr[meth_name + \"_\" + base_name].append(avg_mr3)\n",
    "            exp_4mr[meth_name + \"_\" + base_name].append(avg_mr4)\n",
    "            exp_5mr[meth_name + \"_\" + base_name].append(avg_mr5)\n",
    "            print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64fa4606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 26)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
