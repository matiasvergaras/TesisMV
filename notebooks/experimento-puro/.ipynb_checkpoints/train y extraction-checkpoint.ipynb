{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b31545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, scipy.io\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c1378fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_dir = os.path.join('.', 'patterns')\n",
    "labels_dir = os.path.join('.', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34153577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circular ornaments', 'lozenge', 'pictographics', 'rectangular ornaments', 'strokes and lines', 'triangular ornaments']\n"
     ]
    }
   ],
   "source": [
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(pathDataset + 'train', \n",
    "                                                    transform = transforms.Compose([\n",
    "                                                        transforms.RandomVerticalFlip(),\n",
    "                                                        transforms.RandomHorizontalFlip(),\n",
    "                                                        transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(pathDataset + 'val',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(256),\n",
    "                                                                    transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "print(class_names)\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=30, output_path = 'model.pth', save_each = -1, patience=15):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    bad_epochs = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds ==  labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        print('Train Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset)\n",
    "        print('Val Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch    \n",
    "\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs == patience:\n",
    "                print(f\"Se agotó la paciencia. Mejor época: {best_epoch}.\")\n",
    "                break\n",
    "                \n",
    "        if save_each > -1 and epoch%save_each == 0:\n",
    "            path = output_path.split(\"/\")\n",
    "            filename =  path[-1]\n",
    "            epoch_filename =filename.split(\".\")[0] + \"_e\" + str(epoch) + \".\" + filename.split(\".\")[1]\n",
    "            new_path = path[:-1]\n",
    "            new_path.append(epoch_filename)\n",
    "            new_path = '/'.join(new_path)\n",
    "            torch.save(model.state_dict(), new_path)\n",
    "            print(\"Saving model at epoch {} as {}\".format(epoch, new_path))\n",
    "\n",
    "            \n",
    "    print('Best accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3667d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "Train Loss: 1.7595  Acc: 0.4345\n",
      "Val Loss: 9.4330  Acc: 0.1667\n",
      "Epoch 1/99\n",
      "----------\n",
      "Train Loss: 1.2775  Acc: 0.5476\n",
      "Val Loss: 1.5296  Acc: 0.5256\n",
      "Epoch 2/99\n",
      "----------\n",
      "Train Loss: 1.0852  Acc: 0.6210\n",
      "Val Loss: 1.4251  Acc: 0.5769\n",
      "Epoch 3/99\n",
      "----------\n",
      "Train Loss: 1.0449  Acc: 0.6389\n",
      "Val Loss: 1.5339  Acc: 0.5256\n",
      "Epoch 4/99\n",
      "----------\n",
      "Train Loss: 0.9776  Acc: 0.6567\n",
      "Val Loss: 1.3959  Acc: 0.5513\n",
      "Epoch 5/99\n",
      "----------\n",
      "Train Loss: 1.0975  Acc: 0.6111\n",
      "Val Loss: 1.3077  Acc: 0.6410\n",
      "Epoch 6/99\n",
      "----------\n",
      "Train Loss: 0.9773  Acc: 0.6548\n",
      "Val Loss: 1.1755  Acc: 0.6282\n",
      "Epoch 7/99\n",
      "----------\n",
      "Train Loss: 0.7582  Acc: 0.7361\n",
      "Val Loss: 1.4057  Acc: 0.6667\n",
      "Epoch 8/99\n",
      "----------\n",
      "Train Loss: 0.9748  Acc: 0.6667\n",
      "Val Loss: 1.2111  Acc: 0.5897\n",
      "Epoch 9/99\n",
      "----------\n",
      "Train Loss: 0.8376  Acc: 0.7083\n",
      "Val Loss: 1.1917  Acc: 0.7179\n",
      "Epoch 10/99\n",
      "----------\n",
      "Train Loss: 0.7543  Acc: 0.7381\n",
      "Val Loss: 0.9608  Acc: 0.7436\n",
      "Epoch 11/99\n",
      "----------\n",
      "Train Loss: 0.7487  Acc: 0.7401\n",
      "Val Loss: 0.9415  Acc: 0.7051\n",
      "Epoch 12/99\n",
      "----------\n",
      "Train Loss: 0.8215  Acc: 0.7282\n",
      "Val Loss: 1.3837  Acc: 0.7051\n",
      "Epoch 13/99\n",
      "----------\n",
      "Train Loss: 0.8398  Acc: 0.7063\n",
      "Val Loss: 1.2670  Acc: 0.6282\n",
      "Epoch 14/99\n",
      "----------\n",
      "Train Loss: 0.7848  Acc: 0.7302\n",
      "Val Loss: 1.0996  Acc: 0.6667\n",
      "Epoch 15/99\n",
      "----------\n",
      "Train Loss: 0.7943  Acc: 0.7381\n",
      "Val Loss: 1.1507  Acc: 0.6282\n",
      "Epoch 16/99\n",
      "----------\n",
      "Train Loss: 0.7698  Acc: 0.7321\n",
      "Val Loss: 1.5538  Acc: 0.5897\n",
      "Epoch 17/99\n",
      "----------\n",
      "Train Loss: 0.7901  Acc: 0.7321\n",
      "Val Loss: 1.0767  Acc: 0.6923\n",
      "Epoch 18/99\n",
      "----------\n",
      "Train Loss: 0.6685  Acc: 0.7857\n",
      "Val Loss: 0.9385  Acc: 0.6795\n",
      "Epoch 19/99\n",
      "----------\n",
      "Train Loss: 0.7237  Acc: 0.7520\n",
      "Val Loss: 1.0755  Acc: 0.7051\n",
      "Epoch 20/99\n",
      "----------\n",
      "Train Loss: 0.6788  Acc: 0.7718\n",
      "Val Loss: 0.7756  Acc: 0.7436\n",
      "Epoch 21/99\n",
      "----------\n",
      "Train Loss: 0.6603  Acc: 0.7758\n",
      "Val Loss: 1.1983  Acc: 0.6538\n",
      "Epoch 22/99\n",
      "----------\n",
      "Train Loss: 0.5393  Acc: 0.8294\n",
      "Val Loss: 1.0926  Acc: 0.6923\n",
      "Epoch 23/99\n",
      "----------\n",
      "Train Loss: 0.6075  Acc: 0.7956\n",
      "Val Loss: 1.2657  Acc: 0.6795\n",
      "Epoch 24/99\n",
      "----------\n",
      "Train Loss: 0.6890  Acc: 0.7718\n",
      "Val Loss: 1.1418  Acc: 0.7179\n",
      "Epoch 25/99\n",
      "----------\n",
      "Train Loss: 0.6226  Acc: 0.7877\n",
      "Val Loss: 0.9361  Acc: 0.7308\n",
      "Se agotó la paciencia. Mejor época: 10.\n",
      "Best accuracy: 0.7436\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m model_ft \u001b[38;5;241m=\u001b[39m train_model(model_ft, criterion, optimizer, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     24\u001b[0m                        save_each\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output_path\u001b[38;5;241m=\u001b[39moutput_path)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# save best model\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:377\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    375\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'models'"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ft = model_ft.fc.in_features\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "groups = [{'params': model_ft.conv1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.bn1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer2.parameters(),'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer3.parameters(), 'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer4.parameters(),'lr':learning_rate},\n",
    "            {'params': model_ft.fc.parameters(), 'lr':learning_rate}]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr = 0.0015)\n",
    "\n",
    "output_path = 'models/base_rn18.pth'\n",
    "# change save_each and output_path to get partial outputs\n",
    "model_ft = train_model(model_ft, criterion, optimizer, num_epochs=100,\n",
    "                       save_each=-1, output_path=output_path)\n",
    "\n",
    "# save best model\n",
    "torch.save(model_ft.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28737cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7012  Acc: 0.7629\n"
     ]
    }
   ],
   "source": [
    "model = output_path\n",
    "\n",
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(pathDataset + 'test',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(224),\n",
    "                                                                    #transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "output_dim = 6\n",
    "model_ft.fc = nn.Linear(num_ft, output_dim)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft.eval()\n",
    "running_loss = 0.0\n",
    "running_corrects = 0.0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(test_dataset)\n",
    "epoch_acc = running_corrects / len(test_dataset)\n",
    "print('Test Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232fdf9",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a298a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files:504\n",
      "Files:78\n",
      "Files:194\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10a</th>\n",
       "      <td>0.043922</td>\n",
       "      <td>0.438790</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.413530</td>\n",
       "      <td>1.033888</td>\n",
       "      <td>1.425813</td>\n",
       "      <td>2.464794</td>\n",
       "      <td>2.822518</td>\n",
       "      <td>0.327268</td>\n",
       "      <td>2.642690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986162</td>\n",
       "      <td>0.511597</td>\n",
       "      <td>1.491543</td>\n",
       "      <td>1.570246</td>\n",
       "      <td>1.970436</td>\n",
       "      <td>0.596666</td>\n",
       "      <td>0.013391</td>\n",
       "      <td>0.688872</td>\n",
       "      <td>1.930300</td>\n",
       "      <td>2.450351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10b</th>\n",
       "      <td>0.362146</td>\n",
       "      <td>0.464590</td>\n",
       "      <td>0.127147</td>\n",
       "      <td>0.513911</td>\n",
       "      <td>0.456054</td>\n",
       "      <td>0.456919</td>\n",
       "      <td>0.613942</td>\n",
       "      <td>0.897593</td>\n",
       "      <td>0.727839</td>\n",
       "      <td>0.993983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685357</td>\n",
       "      <td>0.484074</td>\n",
       "      <td>0.518595</td>\n",
       "      <td>1.072349</td>\n",
       "      <td>2.006864</td>\n",
       "      <td>1.752992</td>\n",
       "      <td>0.477887</td>\n",
       "      <td>1.221545</td>\n",
       "      <td>1.052200</td>\n",
       "      <td>0.479289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10c</th>\n",
       "      <td>0.474254</td>\n",
       "      <td>1.758468</td>\n",
       "      <td>0.570486</td>\n",
       "      <td>0.409202</td>\n",
       "      <td>0.051586</td>\n",
       "      <td>0.105452</td>\n",
       "      <td>0.403314</td>\n",
       "      <td>0.476372</td>\n",
       "      <td>0.848582</td>\n",
       "      <td>0.205794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086466</td>\n",
       "      <td>1.045404</td>\n",
       "      <td>0.341057</td>\n",
       "      <td>0.478624</td>\n",
       "      <td>1.017079</td>\n",
       "      <td>0.487478</td>\n",
       "      <td>0.423797</td>\n",
       "      <td>1.231365</td>\n",
       "      <td>0.396027</td>\n",
       "      <td>0.057165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10e</th>\n",
       "      <td>0.244714</td>\n",
       "      <td>0.577980</td>\n",
       "      <td>0.151823</td>\n",
       "      <td>0.289470</td>\n",
       "      <td>0.130480</td>\n",
       "      <td>0.313273</td>\n",
       "      <td>0.260450</td>\n",
       "      <td>0.380469</td>\n",
       "      <td>0.730706</td>\n",
       "      <td>0.422372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469919</td>\n",
       "      <td>0.593789</td>\n",
       "      <td>0.186124</td>\n",
       "      <td>0.576883</td>\n",
       "      <td>1.709014</td>\n",
       "      <td>0.757188</td>\n",
       "      <td>0.458053</td>\n",
       "      <td>1.712341</td>\n",
       "      <td>0.606818</td>\n",
       "      <td>0.144883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11a</th>\n",
       "      <td>0.293450</td>\n",
       "      <td>0.619049</td>\n",
       "      <td>0.095415</td>\n",
       "      <td>0.067594</td>\n",
       "      <td>0.027575</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>1.565796</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016275</td>\n",
       "      <td>0.367695</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.648703</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>0.684499</td>\n",
       "      <td>1.231126</td>\n",
       "      <td>0.011604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96h</th>\n",
       "      <td>0.770769</td>\n",
       "      <td>1.818958</td>\n",
       "      <td>1.157073</td>\n",
       "      <td>0.107323</td>\n",
       "      <td>0.105401</td>\n",
       "      <td>0.118392</td>\n",
       "      <td>1.395383</td>\n",
       "      <td>2.020592</td>\n",
       "      <td>0.409299</td>\n",
       "      <td>1.013556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249506</td>\n",
       "      <td>2.160490</td>\n",
       "      <td>1.344587</td>\n",
       "      <td>0.348210</td>\n",
       "      <td>1.579399</td>\n",
       "      <td>0.452328</td>\n",
       "      <td>0.068747</td>\n",
       "      <td>0.913906</td>\n",
       "      <td>0.104572</td>\n",
       "      <td>0.446956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9a</th>\n",
       "      <td>0.152805</td>\n",
       "      <td>0.218222</td>\n",
       "      <td>0.045043</td>\n",
       "      <td>0.057803</td>\n",
       "      <td>0.096010</td>\n",
       "      <td>1.143477</td>\n",
       "      <td>1.549839</td>\n",
       "      <td>2.302843</td>\n",
       "      <td>0.423475</td>\n",
       "      <td>2.282072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081713</td>\n",
       "      <td>0.409658</td>\n",
       "      <td>0.563725</td>\n",
       "      <td>1.192688</td>\n",
       "      <td>2.347401</td>\n",
       "      <td>0.432017</td>\n",
       "      <td>0.048915</td>\n",
       "      <td>0.552882</td>\n",
       "      <td>2.373051</td>\n",
       "      <td>2.593569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9b</th>\n",
       "      <td>0.115341</td>\n",
       "      <td>0.540795</td>\n",
       "      <td>0.048241</td>\n",
       "      <td>0.585017</td>\n",
       "      <td>0.315587</td>\n",
       "      <td>0.945200</td>\n",
       "      <td>1.351170</td>\n",
       "      <td>2.774725</td>\n",
       "      <td>0.373930</td>\n",
       "      <td>2.246647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409086</td>\n",
       "      <td>0.705647</td>\n",
       "      <td>1.119104</td>\n",
       "      <td>1.559311</td>\n",
       "      <td>2.738190</td>\n",
       "      <td>1.006094</td>\n",
       "      <td>0.023455</td>\n",
       "      <td>1.138020</td>\n",
       "      <td>2.416279</td>\n",
       "      <td>2.388109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9c</th>\n",
       "      <td>0.115330</td>\n",
       "      <td>0.706171</td>\n",
       "      <td>0.058463</td>\n",
       "      <td>1.110772</td>\n",
       "      <td>0.481322</td>\n",
       "      <td>0.922383</td>\n",
       "      <td>1.162173</td>\n",
       "      <td>2.579768</td>\n",
       "      <td>0.602944</td>\n",
       "      <td>1.953690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501731</td>\n",
       "      <td>0.571615</td>\n",
       "      <td>1.208405</td>\n",
       "      <td>1.780763</td>\n",
       "      <td>2.537515</td>\n",
       "      <td>1.254454</td>\n",
       "      <td>0.069168</td>\n",
       "      <td>1.301270</td>\n",
       "      <td>2.601785</td>\n",
       "      <td>2.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d</th>\n",
       "      <td>0.020692</td>\n",
       "      <td>0.064446</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.037519</td>\n",
       "      <td>0.347206</td>\n",
       "      <td>2.454232</td>\n",
       "      <td>2.441526</td>\n",
       "      <td>4.735280</td>\n",
       "      <td>0.180497</td>\n",
       "      <td>4.700363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172130</td>\n",
       "      <td>0.211454</td>\n",
       "      <td>1.714322</td>\n",
       "      <td>2.062657</td>\n",
       "      <td>3.856583</td>\n",
       "      <td>0.358296</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.656203</td>\n",
       "      <td>4.139779</td>\n",
       "      <td>5.181756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "10a  0.043922  0.438790  0.019845  0.413530  1.033888  1.425813  2.464794   \n",
       "10b  0.362146  0.464590  0.127147  0.513911  0.456054  0.456919  0.613942   \n",
       "10c  0.474254  1.758468  0.570486  0.409202  0.051586  0.105452  0.403314   \n",
       "10e  0.244714  0.577980  0.151823  0.289470  0.130480  0.313273  0.260450   \n",
       "11a  0.293450  0.619049  0.095415  0.067594  0.027575  0.008564  0.003278   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "96h  0.770769  1.818958  1.157073  0.107323  0.105401  0.118392  1.395383   \n",
       "9a   0.152805  0.218222  0.045043  0.057803  0.096010  1.143477  1.549839   \n",
       "9b   0.115341  0.540795  0.048241  0.585017  0.315587  0.945200  1.351170   \n",
       "9c   0.115330  0.706171  0.058463  1.110772  0.481322  0.922383  1.162173   \n",
       "9d   0.020692  0.064446  0.004796  0.037519  0.347206  2.454232  2.441526   \n",
       "\n",
       "          7         8         9    ...       502       503       504  \\\n",
       "10a  2.822518  0.327268  2.642690  ...  0.986162  0.511597  1.491543   \n",
       "10b  0.897593  0.727839  0.993983  ...  0.685357  0.484074  0.518595   \n",
       "10c  0.476372  0.848582  0.205794  ...  0.086466  1.045404  0.341057   \n",
       "10e  0.380469  0.730706  0.422372  ...  0.469919  0.593789  0.186124   \n",
       "11a  0.006090  1.565796  0.042254  ...  0.016275  0.367695  0.000443   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "96h  2.020592  0.409299  1.013556  ...  0.249506  2.160490  1.344587   \n",
       "9a   2.302843  0.423475  2.282072  ...  0.081713  0.409658  0.563725   \n",
       "9b   2.774725  0.373930  2.246647  ...  0.409086  0.705647  1.119104   \n",
       "9c   2.579768  0.602944  1.953690  ...  0.501731  0.571615  1.208405   \n",
       "9d   4.735280  0.180497  4.700363  ...  0.172130  0.211454  1.714322   \n",
       "\n",
       "          505       506       507       508       509       510       511  \n",
       "10a  1.570246  1.970436  0.596666  0.013391  0.688872  1.930300  2.450351  \n",
       "10b  1.072349  2.006864  1.752992  0.477887  1.221545  1.052200  0.479289  \n",
       "10c  0.478624  1.017079  0.487478  0.423797  1.231365  0.396027  0.057165  \n",
       "10e  0.576883  1.709014  0.757188  0.458053  1.712341  0.606818  0.144883  \n",
       "11a  0.026875  0.648703  0.051497  0.684499  1.231126  0.011604  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "96h  0.348210  1.579399  0.452328  0.068747  0.913906  0.104572  0.446956  \n",
       "9a   1.192688  2.347401  0.432017  0.048915  0.552882  2.373051  2.593569  \n",
       "9b   1.559311  2.738190  1.006094  0.023455  1.138020  2.416279  2.388109  \n",
       "9c   1.780763  2.537515  1.254454  0.069168  1.301270  2.601785  2.357000  \n",
       "9d   2.062657  3.856583  0.358296  0.005878  0.656203  4.139779  5.181756  \n",
       "\n",
       "[504 rows x 512 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PatternDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, build_classification=False, name_cla='output.cla'):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.namefiles = []\n",
    "\n",
    "\n",
    "        self.classes = sorted(os.listdir(self.root_dir))\n",
    "\n",
    "        for cl in self.classes:\n",
    "            for pat in os.listdir(os.path.join(self.root_dir, cl)):\n",
    "                self.namefiles.append((pat, cl))\n",
    "\n",
    "        print(f'Files:{len(self.namefiles)}')\n",
    "        self.namefiles = sorted(self.namefiles, key = lambda x: x[0])\n",
    "\n",
    "        if build_classification:\n",
    "            dictClasses = dict()\n",
    "\n",
    "            for cl in self.classes:\n",
    "                dictClasses[cl] = []\n",
    "\n",
    "            for index, (name, cl) in enumerate(self.namefiles):\n",
    "                dictClasses[cl].append((name, index))\n",
    "\n",
    "            with open(name_cla, 'w') as f:\n",
    "                f.write('PSB 1\\n')\n",
    "                f.write(f'{len(self.classes)} {len(self.namefiles)}\\n')\n",
    "                f.write('\\n')\n",
    "                for cl in self.classes:\n",
    "                    f.write(f'{cl} 0 {len(dictClasses[cl])}\\n')\n",
    "                    for item in dictClasses[cl]:\n",
    "                        f.write(f'{item[1]}\\n')\n",
    "                    f.write('\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.namefiles)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.namefiles[index][1], self.namefiles[index][0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return self.namefiles[index], image\n",
    "\n",
    "def imshow(inp, title = None):\n",
    "    inp = inp.cpu().detach()\n",
    "    inp = np.squeeze(inp)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    plt.imshow(inp)\n",
    "    plt.show()\n",
    "\n",
    "def get_vector(model,layer, dim_embedding, x):\n",
    "\n",
    "  my_embedding = torch.zeros(dim_embedding)\n",
    "\n",
    "  def copy_data(m,i,o):\n",
    "    my_embedding.copy_(o.data.squeeze())\n",
    "\n",
    "  h = layer.register_forward_hook(copy_data)\n",
    "  model(x)\n",
    "  h.remove()\n",
    "\n",
    "  return my_embedding\n",
    "\n",
    "DEVICE = 0\n",
    "# 0 3090 (o 1060 en local)\n",
    "# 1 y 2 2080\n",
    "\n",
    "random.seed(30)\n",
    "\n",
    "\n",
    "model_path ='models/base_rn18.pth'\n",
    "features_dir = 'features/rn18'\n",
    "\n",
    "output_train = os.path.join(features_dir, \"augmented_train_df.json\")\n",
    "output_val = os.path.join(features_dir, \"val_df.json\")\n",
    "output_test = os.path.join(features_dir, \"test_df.json\")\n",
    "\n",
    "train_df = pd.read_json(os.path.join(labels_dir, \"augmented_train_df.json\"), orient='index')\n",
    "val_df = pd.read_json(os.path.join(labels_dir, \"val_df.json\"), orient='index')\n",
    "test_df = pd.read_json(os.path.join(labels_dir, \"test_df.json\"), orient='index')\n",
    "\n",
    "train_pts = train_df.index.values\n",
    "val_pts = val_df.index.values\n",
    "test_pts = test_df.index.values\n",
    "\n",
    "device = ('cuda:0' if torch.cuda.is_available() else None)\n",
    "if device is None:\n",
    "    raise Exception(\"La GPU solicitada no está disponible\")\n",
    "\n",
    "my_transform = transforms.Compose([ transforms.Resize(224),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize(mean=[0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "                    ])\n",
    "\n",
    "dataTrain = PatternDataset(root_dir=os.path.join(patterns_dir, 'train'), transform=my_transform)\n",
    "dataVal = PatternDataset(root_dir=os.path.join(patterns_dir, 'val'), transform=my_transform)\n",
    "dataTest = PatternDataset(root_dir=os.path.join(patterns_dir, 'test'), transform=my_transform)\n",
    "\n",
    "loaderTrain = DataLoader(dataTrain)\n",
    "loaderVal = DataLoader(dataVal)\n",
    "loaderTest = DataLoader(dataTest)\n",
    "\n",
    "model = models.resnet18(pretrained = True)\n",
    "\n",
    "dim = model.fc.in_features\n",
    "\n",
    "output_dim = 6\n",
    "model.fc = nn.Linear(dim, output_dim)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "except RuntimeError as e:\n",
    "    print('Ignoring \"' + str(e) + '\"')\n",
    "\n",
    "layer = model._modules.get('avgpool')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "#features = []\n",
    "features_train = {}\n",
    "features_val = {}\n",
    "features_test = {}\n",
    "\n",
    "\n",
    "for name, img in loaderTrain:\n",
    "  feat = get_vector(model, layer, dim, img.to(device))\n",
    "  namefile = name[0][0]\n",
    "  code, rest = namefile.split('.')\n",
    "  #print(code)\n",
    "  #imshow(img)\n",
    "  features_train[code] = feat.numpy().tolist()\n",
    "  #features.append(feat.numpy())\n",
    "\n",
    "for name, img in loaderVal:\n",
    "  feat = get_vector(model, layer, dim, img.to(device))\n",
    "  namefile = name[0][0]\n",
    "  code, rest = namefile.split('.')\n",
    "  #print(code)\n",
    "  #imshow(img) \n",
    "  features_val[code] = feat.numpy().tolist()\n",
    "  #features.append(feat.numpy())\n",
    "\n",
    "for name, img in loaderTest:\n",
    "  feat = get_vector(model, layer, dim, img.to(device))\n",
    "  namefile = name[0][0]\n",
    "  code, rest = namefile.split('.')\n",
    "  #print(code)\n",
    "  #imshow(img)\n",
    "  features_test[code] = feat.numpy().tolist()\n",
    "#features = np.vstack(features)\n",
    "#print(features.shape)\n",
    "\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "features_train_df = pd.DataFrame.from_dict(features_train, orient='index')\n",
    "features_val_df = pd.DataFrame.from_dict(features_val, orient='index')\n",
    "features_test_df = pd.DataFrame.from_dict(features_test, orient='index')\n",
    "\n",
    "features_train_df.to_json(output_train, orient='index')\n",
    "features_val_df.to_json(output_val, orient='index')\n",
    "features_test_df.to_json(output_test, orient='index')\n",
    "\n",
    "display(features_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25e3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
