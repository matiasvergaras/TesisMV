{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Montando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '../content/gdrive/MyDrive'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NUM_LABELS = 26\n",
    "BATCH_SIZE = 1 #importante para la extraccion\n",
    "\n",
    "TH_TRAIN = 0.5\n",
    "TH_VAL = 0.5\n",
    "TH_TEST = 0.5\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = True\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\0\n",
      "--Labels set encontrado en ../labels\\base\\0\n",
      "--Path al modelo ../models\\alexnet\\base\\0\\26L_weighted_1.pth\n",
      "\n",
      "Las features se guardarán en: ../features\\alexnet_retrained\\base\\K0\n",
      "Fold  1\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\1\n",
      "--Labels set encontrado en ../labels\\base\\1\n",
      "--Path al modelo ../models\\alexnet\\base\\1\\26L_weighted_1.pth\n",
      "\n",
      "Las features se guardarán en: ../features\\alexnet_retrained\\base\\K1\n",
      "Fold  2\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\2\n",
      "--Labels set encontrado en ../labels\\base\\2\n",
      "--Path al modelo ../models\\alexnet\\base\\2\\26L_weighted_1.pth\n",
      "\n",
      "Las features se guardarán en: ../features\\alexnet_retrained\\base\\K2\n",
      "Fold  3\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\3\n",
      "--Labels set encontrado en ../labels\\base\\3\n",
      "--Path al modelo ../models\\alexnet\\base\\3\\26L_weighted_1.pth\n",
      "\n",
      "Las features se guardarán en: ../features\\alexnet_retrained\\base\\K3\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "Kfolds = {}\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    patterns_dir = os.path.join(root_dir, 'patterns', data_flags, str(i))\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "\n",
    "    if not (os.path.isdir(patterns_dir) and os.path.isdir(labels_dir)):\n",
    "        print(patterns_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    #exp_name = f\"{NUM_LABELS}L\"\n",
    "    weights_str = str(pos_weights_factor)\n",
    "    weights_str = weights_str.replace('.','_')\n",
    "    #exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "     \n",
    "    #features_dir = os.path.join(root_dir, \"features\", \"alexnet\", data_flags, exp_name, str(i))\n",
    "    features_dir = os.path.join(root_dir, 'features', 'alexnet_retrained', data_flags, 'K' + str(i))\n",
    "    model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "    Kfolds[i] = {\n",
    "        'patterns_dir': patterns_dir,\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path,\n",
    "        'features_dir': features_dir\n",
    "    }\n",
    "    \n",
    "    print(\"--Pattern set encontrado en {}\".format(patterns_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"--Path al modelo {}\".format(model_path))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(features_dir, exist_ok = True)\n",
    "        print(f\"Las features se guardarán en: {features_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "def get_vector(model,layer, dim_embedding, x):\n",
    "\n",
    "  my_embedding = torch.zeros(dim_embedding)\n",
    "\n",
    "  def copy_data(m,i,o):\n",
    "    my_embedding.copy_(o.data.squeeze())\n",
    "\n",
    "  h = layer.register_forward_hook(copy_data)\n",
    "  model(x)\n",
    "  h.remove()\n",
    "\n",
    "  return my_embedding\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_file(string): path to text file\n",
    "        root_dir(string): directory with all train images\n",
    "    \"\"\"\n",
    "    self.pruner = KunischPruner(len(top_labels))\n",
    "    self.pruner.set_top_labels(top_labels)\n",
    "    labels = pd.read_json(labels_file, orient='index')\n",
    "    self.labels_frame = self.pruner.filter_df(labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "def hamming_score(y_true, y_pred):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set(np.where(y_true[i])[0])\n",
    "        set_pred = set(np.where(y_pred[i])[0])\n",
    "        # print('\\nset_true: {0}'.format(set_true))\n",
    "        # print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred)) / \\\n",
    "                    float(len(set_true.union(set_pred)))\n",
    "        # print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return round(np.mean(acc_list), 4)\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_VAL).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "    \n",
    "      scheduler.step(hs)\n",
    "\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, valid_loss, accuracy, f1_score, hs))\n",
    "      return hs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n",
      "Usando top_labels previamente generados para 26 labels\n",
      "Usando top_labels previamente generados para 26 labels\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    }
   ],
   "source": [
    "pruner = KunischPruner(NUM_LABELS)\n",
    "\n",
    "    \n",
    "for i in range(0, K):\n",
    "    fold = Kfolds[i]\n",
    "    labels_dir = fold['labels_dir']\n",
    "    patterns_dir = fold['patterns_dir']\n",
    "    output_dir = fold['output_dir']\n",
    "    model_path = fold['model_path']\n",
    "    features_dir = fold['features_dir']\n",
    "    # Carga de top labels\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "        print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "        top_labels = pruner.filter_labels(train_labels)\n",
    "        pruner.set_top_labels(top_labels)\n",
    "        \n",
    "        save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "            \n",
    "    else: \n",
    "        print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "\n",
    "    NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "    # Creacion de pesos positivos\n",
    "    if use_pos_weights:\n",
    "        pos_weights = make_positive_weights(top_labels, pos_weights_factor)\n",
    "        pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "\n",
    "    else:\n",
    "        pos_weights = None\n",
    "\n",
    "    # Alexnet requires 227 x 227\n",
    "    # Training\n",
    "    kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'train'),\n",
    "                                     labels_file=os.path.join(labels_dir, 'augmented_train_df.json'),\n",
    "                                     transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize(\n",
    "                                                                       mean=[0.485, 0.456, 0.406],\n",
    "                                                                       std=[0.229, 0.224, 0.225])]),\n",
    "                                     top_labels=top_labels)\n",
    "\n",
    "    kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Validation\n",
    "    kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'val'),\n",
    "                                          labels_file=os.path.join(labels_dir, 'val_df.json'),\n",
    "                                          transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                        transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize(\n",
    "                                                                            mean=[0.485, 0.456, 0.406],\n",
    "                                                                            std=[0.229, 0.224, 0.225])]),\n",
    "                                          top_labels=top_labels)\n",
    "\n",
    "    kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                          num_workers=0)\n",
    "\n",
    "    # Test\n",
    "    kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'test'),\n",
    "                                    labels_file=os.path.join(labels_dir, 'test_df.json'),\n",
    "                                    transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                  transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize(\n",
    "                                                                      mean=[0.485, 0.456, 0.406],\n",
    "                                                                      std=[0.229, 0.224, 0.225])]),\n",
    "                                    top_labels=top_labels)\n",
    "\n",
    "    kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    for param in alex_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "    alex_net.load_state_dict(torch.load(model_path))\n",
    "    alex_net.to(device)\n",
    "    \n",
    "    layer =  alex_net.classifier[4]\n",
    "    dim = alex_net.classifier[4].in_features\n",
    "    \n",
    "    alex_net.eval()\n",
    "\n",
    "    #features = []\n",
    "    features_train = {}\n",
    "    features_val = {}\n",
    "    features_test = {}\n",
    "    \n",
    "    \n",
    "    for data in kunischTrainLoader:\n",
    "      img = data['image']\n",
    "      name = os.path.normpath(data['paths'][0],).split('\\\\')\n",
    "      name = name[len(name)-1]\n",
    "      feat = get_vector(alex_net, layer, dim, img.to(device))\n",
    "      namefile = name\n",
    "      code, rest = namefile.split('.')\n",
    "      features_train[code] = feat.numpy().tolist()\n",
    "      #features.append(feat.numpy())\n",
    "\n",
    "    for data in kunischValidationLoader:\n",
    "      img = data['image']\n",
    "      name = os.path.normpath(data['paths'][0],).split('\\\\')\n",
    "      name = name[len(name)-1]\n",
    "      feat = get_vector(alex_net, layer, dim, img.to(device))\n",
    "      namefile = name\n",
    "      code, rest = namefile.split('.')\n",
    "      features_val[code] = feat.numpy().tolist()\n",
    "\n",
    "    for data in kunischTestLoader:\n",
    "      img = data['image']\n",
    "      name = os.path.normpath(data['paths'][0],).split('\\\\')\n",
    "      name = name[len(name)-1]\n",
    "      feat = get_vector(alex_net, layer, dim, img.to(device))\n",
    "      namefile = name\n",
    "      code, rest = namefile.split('.')\n",
    "      features_test[code] = feat.numpy().tolist()\n",
    "    #features = np.vstack(features)\n",
    "\n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "    features_train_df = pd.DataFrame.from_dict(features_train, orient='index')\n",
    "    features_val_df = pd.DataFrame.from_dict(features_val, orient='index')\n",
    "    features_test_df = pd.DataFrame.from_dict(features_test, orient='index')\n",
    "\n",
    "    output_train = os.path.join(features_dir, 'augmented_train_df.json')\n",
    "    output_val = os.path.join(features_dir, 'val_df.json')\n",
    "    output_test = os.path.join(features_dir, 'test_df.json')\n",
    "    \n",
    "    features_train_df.to_json(output_train, orient='index')\n",
    "    features_val_df.to_json(output_val, orient='index')\n",
    "    features_test_df.to_json(output_test, orient='index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
