{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Montando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '../content/gdrive/MyDrive'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SUBCHAPTERS = False\n",
    "DS_FLAGS = ['blur', 'mtnblur', 'elastic']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 2\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NUM_LABELS = 26\n",
    "use_testval = True\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "TH_TRAIN = 0.5\n",
    "TH_VAL = 0.5\n",
    "TH_TEST = 0.5\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns set encontrado en ../patterns\\blur_mtnblur_elastic1\n",
      "Labels set encontrado en ../labels\\blur_mtnblur_elastic1\n",
      "Nombre del experimento: 26L_testval_weighted_1\n",
      "Los resultados se guardarán en: ../outputs\\alexnet\\blur_mtnblur_elastic1\\26L_testval_weighted_1\n",
      "Los resultados se guardarán en: ../outputs\\alexnet\\blur_mtnblur_elastic1\\26L_testval_weighted_1\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = os.path.join(root_dir, 'patterns', data_flags) \n",
    "labels_path = os.path.join(root_dir, 'labels', data_flags)\n",
    "\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista en {}\".format(\n",
    "        (os.path.isdir(patterns_path), os.path.isdir(labels_path))))\n",
    "print(\"Patterns set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "\n",
    "exp_name = f\"{NUM_LABELS}L\"\n",
    "exp_name += \"_testval\" if use_testval else \"\"\n",
    "weights_str = str(pos_weights_factor)\n",
    "weights_str = weights_str.replace('.','_')\n",
    "exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "print(f\"Nombre del experimento: {exp_name}\")\n",
    "\n",
    "output_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name)\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags)\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "print(f\"Los resultados se guardarán en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x242aa80a800>]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_train = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "labels_val = pd.read_json(os.path.join(labels_path, 'val_df.json'), orient='index')\n",
    "labels_test = pd.read_json(os.path.join(labels_path, 'test_df.json'), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pendent</th>\n",
       "      <th>teardrop</th>\n",
       "      <th>horizontal</th>\n",
       "      <th>panel</th>\n",
       "      <th>group</th>\n",
       "      <th>vertical</th>\n",
       "      <th>bar</th>\n",
       "      <th>floating</th>\n",
       "      <th>enclosing</th>\n",
       "      <th>shorter</th>\n",
       "      <th>...</th>\n",
       "      <th>light</th>\n",
       "      <th>body</th>\n",
       "      <th>bird</th>\n",
       "      <th>striped</th>\n",
       "      <th>worm</th>\n",
       "      <th>angular</th>\n",
       "      <th>raised</th>\n",
       "      <th>head</th>\n",
       "      <th>bird-seed</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40g</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81j</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pendent  teardrop  horizontal  panel  group  vertical  bar  floating  \\\n",
       "53b        0         0           1      1      0         0    0         0   \n",
       "2b         0         0           1      1      0         1    1         0   \n",
       "40g        0         0           0      1      0         1    0         0   \n",
       "70h        0         0           0      0      0         0    0         0   \n",
       "94a        0         0           0      0      0         0    0         0   \n",
       "..       ...       ...         ...    ...    ...       ...  ...       ...   \n",
       "85c        0         0           1      1      0         0    0         0   \n",
       "92b        0         0           0      0      0         0    0         0   \n",
       "71a        0         0           1      1      0         0    0         0   \n",
       "81j        0         0           0      0      0         0    0         0   \n",
       "75h        0         0           1      1      0         0    0         0   \n",
       "\n",
       "     enclosing  shorter  ...  light  body  bird  striped  worm  angular  \\\n",
       "53b          0        0  ...      0     0     0        0     0        0   \n",
       "2b           0        0  ...      0     0     0        0     0        0   \n",
       "40g          0        0  ...      0     0     0        0     0        0   \n",
       "70h          0        0  ...      0     0     0        0     0        0   \n",
       "94a          0        0  ...      0     0     0        0     0        0   \n",
       "..         ...      ...  ...    ...   ...   ...      ...   ...      ...   \n",
       "85c          0        0  ...      0     0     0        0     0        0   \n",
       "92b          0        0  ...      0     0     0        0     0        0   \n",
       "71a          0        0  ...      0     0     0        0     0        0   \n",
       "81j          1        0  ...      0     0     0        0     0        0   \n",
       "75h          0        0  ...      0     0     0        0     0        0   \n",
       "\n",
       "     raised  head  bird-seed  long  \n",
       "53b       0     0          0     0  \n",
       "2b        0     0          0     0  \n",
       "40g       0     0          0     0  \n",
       "70h       0     0          0     0  \n",
       "94a       0     0          0     0  \n",
       "..      ...   ...        ...   ...  \n",
       "85c       0     0          0     0  \n",
       "92b       0     0          0     0  \n",
       "71a       0     0          0     0  \n",
       "81j       0     0          0     0  \n",
       "75h       0     0          0     0  \n",
       "\n",
       "[156 rows x 330 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n",
      "panel            289\n",
      "horizontal       234\n",
      "ornament         123\n",
      "hatched          108\n",
      "vertical         104\n",
      "circle            80\n",
      "metopal           77\n",
      "filling           74\n",
      "lozenge           64\n",
      "enclosing         62\n",
      "double            54\n",
      "cross-hatched     51\n",
      "triangle          49\n",
      "line              42\n",
      "chain             41\n",
      "concentric        40\n",
      "meander           40\n",
      "bar               39\n",
      "dotted            37\n",
      "solid             35\n",
      "dot               34\n",
      "cross             31\n",
      "outline           31\n",
      "single            28\n",
      "hook              27\n",
      "floor             27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "    print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "    top_labels, _ = filter_labels(train_labels, number_labels = NUM_LABELS)\n",
    "    save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "    if save == \"y\":\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "            pickle.dump(top_labels, f)\n",
    "        print(top_labels)\n",
    "    else:\n",
    "        raise Exception(\"No se logró cargar top_labels\")\n",
    "else:\n",
    "    print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "    with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "        top_labels = pickle.load(f)\n",
    "    print(top_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m_jvs\\AppData\\Local\\Temp\\ipykernel_11780\\1986160355.py:32: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  self.labels_frame = pd.DataFrame.append(self.labels_frame, extra_labels_frame)\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "if use_pos_weights:\n",
    "    pos_weights = make_positive_weights(top_labels, pos_weights_factor)\n",
    "    pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "else:\n",
    "    pos_weights = None\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels, extra_labels = None, extra_images_dir = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_file(string): path to text file\n",
    "        root_dir(string): directory with all train images\n",
    "    \"\"\"\n",
    "    self.labels_frame = filter_dfs(pd.read_json(labels_file, orient='index'), top_labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "    self.extra_images_dir = None \n",
    "\n",
    "    # para crear conjunto test-val \n",
    "    if extra_labels:\n",
    "      extra_labels_frame = filter_dfs(pd.read_json(extra_labels, orient='index'), top_labels)\n",
    "      self.labels_frame = pd.DataFrame.append(self.labels_frame, extra_labels_frame)\n",
    "      self.extra_images_dir = extra_images_dir\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    # caso test-val\n",
    "    if (self.extra_images_dir is not None) and (img_name is None):\n",
    "      for chapter in os.listdir(self.extra_images_dir):\n",
    "        if img_id in os.listdir(os.path.join(self.extra_images_dir, chapter)):\n",
    "          img_name = os.path.join(self.extra_images_dir, chapter, img_id)\n",
    "          break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Alexnet requires 227 x 227\n",
    "# Training\n",
    "kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_path, 'train'),\n",
    "                                 labels_file=os.path.join(labels_path, 'augmented_train_df.json'),\n",
    "                                 transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                               transforms.ToTensor(),\n",
    "                                                               transforms.Normalize(\n",
    "                                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                                   std=[0.229, 0.224, 0.225])]),\n",
    "                                 top_labels=top_labels)\n",
    "\n",
    "kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation\n",
    "kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_path, 'val'),\n",
    "                                      labels_file=os.path.join(labels_path, 'val_df.json'),\n",
    "                                      transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(\n",
    "                                                                        mean=[0.485, 0.456, 0.406],\n",
    "                                                                        std=[0.229, 0.224, 0.225])]),\n",
    "                                      top_labels=top_labels)\n",
    "\n",
    "kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                      num_workers=0)\n",
    "\n",
    "# Test\n",
    "kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_path, 'test'),\n",
    "                                labels_file=os.path.join(labels_path, 'test_df.json'),\n",
    "                                transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize(\n",
    "                                                                  mean=[0.485, 0.456, 0.406],\n",
    "                                                                  std=[0.229, 0.224, 0.225])]),\n",
    "                                top_labels=top_labels,\n",
    "                                extra_labels=os.path.join(labels_path, 'val_df.json' if use_testval else None),\n",
    "                                extra_images_dir=os.path.join(patterns_path, 'val' if use_testval else None))\n",
    "\n",
    "kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# Define the function for training, validation, and test\n",
    "def alex_train(epoch, num_epochs, train_losses, learning_rate, w):\n",
    "  alex_net.train()\n",
    "  train_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  for i, sample_batched in enumerate(kunischTrainLoader, 1):\n",
    "      inputs = sample_batched['image'].to(device)\n",
    "      labels = sample_batched['labels'].to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = alex_net(inputs)\n",
    "      loss = criterion(outputs.float(), labels.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      train_loss += loss.item()\n",
    "      pred = (torch.sigmoid(outputs).data > TH_TRAIN).int()\n",
    "      # print(pred)\n",
    "      labels = labels.int()\n",
    "      # print(labels)\n",
    "      preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "      labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "    \n",
    "      TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "      TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "      FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "      FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "      #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP, TN, FP, FN))\n",
    "  \n",
    "\n",
    "  TP = TP.cpu().numpy()\n",
    "  TN = TN.cpu().numpy()\n",
    "  FP = FP.cpu().numpy()\n",
    "  FN = FN.cpu().numpy()\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  train_loss = train_loss / len(kunischTrainLoader.dataset) * BATCH_SIZE\n",
    "  hs = hamming_score(preds_total, labels_total)\n",
    "  train_losses.append([epoch, learning_rate, w, train_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "\n",
    "  # print statistics\n",
    "  print('Train Trial [{}/{}], LR: {:.4g}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "        .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, train_loss, accuracy, f1_score, hs))\n",
    "  return hs\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_VAL).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "    \n",
    "      scheduler.step(hs)\n",
    "\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, valid_loss, accuracy, f1_score, hs))\n",
    "      return hs\n",
    "\n",
    "def alex_test(epoch, num_epochs, pred_array, test_losses, learning_rate, w, show_images=1):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  test_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      for i,sample_batched in enumerate(kunischTestLoader, 1):\n",
    "          print(\"CURRENT BATCH SIZE: \", BATCH_SIZE)\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          paths = sample_batched['paths']\n",
    "          outputs = alex_net(inputs)\n",
    "          \n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          test_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_TEST).int()\n",
    "          # print(pred)\n",
    "          labels = labels.int()\n",
    "          # print(labels)\n",
    "          pred_array.append([paths, test_loss, labels, pred])\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "          \n",
    "          for j in range(0, min(BATCH_SIZE, show_images)): # j itera sobre ejemplos\n",
    "              print(f\"Mostrando imagen {j} del batch {i}\")\n",
    "              img = np.transpose(sample_batched['image'][j]) # imagen j \n",
    "              plt.imshow(img, interpolation='nearest')\n",
    "              plt.show()\n",
    "              labels_correctos = \"\"\n",
    "              labels_predichos = \"\"\n",
    "              for k in range(0, len(pred[j])):\n",
    "                labels_correctos += (kunischTestSet.labels_frame.columns.values[k]+' ') if labels[j].cpu().detach()[k] else \"\"\n",
    "                labels_predichos += (kunischTestSet.labels_frame.columns.values[k]+' ') if pred[j].cpu().detach()[k] else \"\"\n",
    "              print(\"Labels correctos:\")\n",
    "              #print(labels[j].cpu().detach().numpy())\n",
    "              print(labels_correctos)\n",
    "              print(\"Labels predichos:\")\n",
    "              #print(pred[j].cpu().detach().numpy())\n",
    "              print(labels_predichos)\n",
    "              print(\"\\n\")\n",
    "            \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "      test_loss = test_loss / len(kunischTestLoader.dataset) * 1024  # 1024 is the batch size\n",
    "      test_losses.append([epoch, learning_rate, w, test_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Test Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, test_loss, accuracy, f1_score, hs))\n",
    "      return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Trial [0/10], LR: 0.0051, W: 1.4193e-06, Avg Loss: 5.9915, Accuracy: 0.4758, F1 score: 0.2280, HS: 0.1262\n",
      "Valid Trial [1/10], LR: 0.0026, W: 9.3094e-06, Avg Loss: 6.4477, Accuracy: 0.4842, F1 score: 0.2052, HS: 0.1137\n",
      "Valid Trial [2/10], LR: 0.0029, W: 2.0519e-05, Avg Loss: 6.2394, Accuracy: 0.4522, F1 score: 0.2115, HS: 0.1225\n",
      "Valid Trial [3/10], LR: 0.0005, W: 4.7713e-06, Avg Loss: 6.2103, Accuracy: 0.4739, F1 score: 0.1849, HS: 0.1025\n",
      "Valid Trial [4/10], LR: 0.0032, W: 2.202e-06, Avg Loss: 6.1749, Accuracy: 0.5217, F1 score: 0.2350, HS: 0.1328\n",
      "Valid Trial [5/10], LR: 0.0009, W: 4.7752e-06, Avg Loss: 5.8603, Accuracy: 0.4822, F1 score: 0.2268, HS: 0.1273\n",
      "Valid Trial [6/10], LR: 0.0061, W: 3.6701e-06, Avg Loss: 6.6526, Accuracy: 0.5429, F1 score: 0.2084, HS: 0.1162\n",
      "Valid Trial [7/10], LR: 0.0004, W: 4.801e-07, Avg Loss: 6.6046, Accuracy: 0.5897, F1 score: 0.2136, HS: 0.1215\n",
      "Valid Trial [8/10], LR: 0.0002, W: 2.67619e-05, Avg Loss: 5.4956, Accuracy: 0.4670, F1 score: 0.2414, HS: 0.1408\n",
      "Valid Trial [9/10], LR: 0.001, W: 3.8486e-06, Avg Loss: 6.2788, Accuracy: 0.5464, F1 score: 0.2230, HS: 0.1262\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameter Tuning\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "for param in alex_net.parameters():\n",
    "    param.requires_grad = False\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))), 4)  # pull geometrically\n",
    "  w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))), 10)  # pull geometrically\n",
    "\n",
    "  # Reset Model per test\n",
    "  alex_net = models.alexnet(pretrained=True)\n",
    "  alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "  alex_net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "  criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.00005)\n",
    "\n",
    "  train_losses_df = pd.DataFrame(train_losses)\n",
    "  train_losses_df.to_csv(os.path.join(output_dir, 'loss_hypertrain.csv'))\n",
    "\n",
    "  alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  validation_losses_df = pd.DataFrame(validation_losses)\n",
    "  validation_losses_df.to_csv(os.path.join(output_dir, 'loss_hyperval.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 2.5876, Accuracy: 0.2085, F1 score: 0.2354, HS: 0.1331\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 3.2224, Accuracy: 0.2382, F1 score: 0.2576, HS: 0.1477\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 2.0627, Accuracy: 0.2579, F1 score: 0.2534, HS: 0.1465\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 3.2163, Accuracy: 0.3674, F1 score: 0.2900, HS: 0.1724\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.9241, Accuracy: 0.3019, F1 score: 0.2649, HS: 0.1556\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 2.7974, Accuracy: 0.4660, F1 score: 0.3252, HS: 0.2041\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.8479, Accuracy: 0.3517, F1 score: 0.2779, HS: 0.1656\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 2.8686, Accuracy: 0.4995, F1 score: 0.3388, HS: 0.2095\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.7609, Accuracy: 0.3743, F1 score: 0.2850, HS: 0.1713\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 2.9223, Accuracy: 0.4576, F1 score: 0.3243, HS: 0.1976\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.6456, Accuracy: 0.4461, F1 score: 0.3108, HS: 0.1897\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 2.9057, Accuracy: 0.5030, F1 score: 0.3342, HS: 0.2066\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 1.5659, Accuracy: 0.4795, F1 score: 0.3235, HS: 0.2024\n",
      "Valid Trial [6/200], LR: 0.00075, W: 0.01, Avg Loss: 2.8817, Accuracy: 0.5542, F1 score: 0.3534, HS: 0.2186\n",
      "\n",
      "Train Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.4533, Accuracy: 0.5353, F1 score: 0.3497, HS: 0.2233\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 2.9794, Accuracy: 0.6277, F1 score: 0.3994, HS: 0.2527\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 1.4000, Accuracy: 0.5576, F1 score: 0.3616, HS: 0.2350\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 2.8359, Accuracy: 0.6504, F1 score: 0.4165, HS: 0.2762\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 1.3426, Accuracy: 0.5827, F1 score: 0.3746, HS: 0.2487\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 2.7287, Accuracy: 0.6696, F1 score: 0.4303, HS: 0.2803\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 1.3157, Accuracy: 0.5926, F1 score: 0.3803, HS: 0.2534\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 2.4454, Accuracy: 0.6415, F1 score: 0.4142, HS: 0.2700\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 1.2437, Accuracy: 0.6273, F1 score: 0.4015, HS: 0.2770\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 2.6621, Accuracy: 0.6637, F1 score: 0.4240, HS: 0.2766\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 1.2615, Accuracy: 0.6108, F1 score: 0.3925, HS: 0.2667\n",
      "Valid Trial [12/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 2.6780, Accuracy: 0.7263, F1 score: 0.4749, HS: 0.3224\n",
      "\n",
      "Train Trial [13/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.1297, Accuracy: 0.6660, F1 score: 0.4294, HS: 0.3008\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 3.1435, Accuracy: 0.7318, F1 score: 0.4759, HS: 0.3232\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.1376, Accuracy: 0.6629, F1 score: 0.4281, HS: 0.3029\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 2.6809, Accuracy: 0.7071, F1 score: 0.4629, HS: 0.3066\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.0758, Accuracy: 0.6824, F1 score: 0.4429, HS: 0.3217\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 2.5289, Accuracy: 0.7224, F1 score: 0.4704, HS: 0.3140\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.0521, Accuracy: 0.6918, F1 score: 0.4497, HS: 0.3277\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 3.0546, Accuracy: 0.7549, F1 score: 0.4913, HS: 0.3363\n",
      "\n",
      "Train Trial [17/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.0407, Accuracy: 0.6975, F1 score: 0.4543, HS: 0.3284\n",
      "Valid Trial [17/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 2.8929, Accuracy: 0.7219, F1 score: 0.4598, HS: 0.3187\n",
      "\n",
      "Train Trial [18/200], LR: 0.0005625, W: 0.01, Avg Loss: 1.0550, Accuracy: 0.6964, F1 score: 0.4537, HS: 0.3262\n",
      "Valid Trial [18/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 3.0406, Accuracy: 0.7579, F1 score: 0.4995, HS: 0.3446\n",
      "\n",
      "Train Trial [19/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.9172, Accuracy: 0.7369, F1 score: 0.4905, HS: 0.3728\n",
      "Valid Trial [19/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 3.5666, Accuracy: 0.7643, F1 score: 0.4968, HS: 0.3438\n",
      "\n",
      "Train Trial [20/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.9278, Accuracy: 0.7309, F1 score: 0.4850, HS: 0.3659\n",
      "Valid Trial [20/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 2.9847, Accuracy: 0.7865, F1 score: 0.5288, HS: 0.3712\n",
      "\n",
      "Train Trial [21/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.8769, Accuracy: 0.7531, F1 score: 0.5066, HS: 0.3898\n",
      "Valid Trial [21/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 3.2403, Accuracy: 0.7949, F1 score: 0.5347, HS: 0.3782\n",
      "\n",
      "Train Trial [22/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.8651, Accuracy: 0.7583, F1 score: 0.5123, HS: 0.3975\n",
      "Valid Trial [22/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 3.5417, Accuracy: 0.7717, F1 score: 0.5090, HS: 0.3513\n",
      "\n",
      "Train Trial [23/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.9149, Accuracy: 0.7479, F1 score: 0.5006, HS: 0.3858\n",
      "Valid Trial [23/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 3.0924, Accuracy: 0.7964, F1 score: 0.5312, HS: 0.3818\n",
      "\n",
      "Train Trial [24/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.8541, Accuracy: 0.7564, F1 score: 0.5100, HS: 0.4008\n",
      "Valid Trial [24/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 3.7772, Accuracy: 0.7855, F1 score: 0.5215, HS: 0.3635\n",
      "\n",
      "Train Trial [25/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.8141, Accuracy: 0.7673, F1 score: 0.5217, HS: 0.4060\n",
      "Valid Trial [25/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 3.5915, Accuracy: 0.8131, F1 score: 0.5504, HS: 0.3996\n",
      "\n",
      "Train Trial [26/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.7789, Accuracy: 0.7846, F1 score: 0.5400, HS: 0.4302\n",
      "Valid Trial [26/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 3.5913, Accuracy: 0.8240, F1 score: 0.5673, HS: 0.4052\n",
      "\n",
      "Train Trial [27/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.7435, Accuracy: 0.7949, F1 score: 0.5527, HS: 0.4418\n",
      "Valid Trial [27/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 3.7992, Accuracy: 0.8200, F1 score: 0.5639, HS: 0.4060\n",
      "\n",
      "Train Trial [28/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.7368, Accuracy: 0.7997, F1 score: 0.5593, HS: 0.4515\n",
      "Valid Trial [28/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 3.5980, Accuracy: 0.8328, F1 score: 0.5830, HS: 0.4307\n",
      "\n",
      "Train Trial [29/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.6974, Accuracy: 0.8052, F1 score: 0.5663, HS: 0.4640\n",
      "Valid Trial [29/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 4.1311, Accuracy: 0.8333, F1 score: 0.5667, HS: 0.4182\n",
      "\n",
      "Train Trial [30/200], LR: 0.0003164, W: 0.01, Avg Loss: 0.7236, Accuracy: 0.7998, F1 score: 0.5593, HS: 0.4536\n",
      "Valid Trial [30/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 4.8166, Accuracy: 0.8526, F1 score: 0.5987, HS: 0.4465\n",
      "\n",
      "Train Trial [31/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.7045, Accuracy: 0.8070, F1 score: 0.5687, HS: 0.4685\n",
      "Valid Trial [31/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 4.2386, Accuracy: 0.8471, F1 score: 0.5964, HS: 0.4415\n",
      "\n",
      "Train Trial [32/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.6551, Accuracy: 0.8216, F1 score: 0.5883, HS: 0.4908\n",
      "Valid Trial [32/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 4.2098, Accuracy: 0.8486, F1 score: 0.5976, HS: 0.4429\n",
      "\n",
      "Train Trial [33/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.6318, Accuracy: 0.8278, F1 score: 0.5969, HS: 0.5002\n",
      "Valid Trial [33/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 4.7164, Accuracy: 0.8506, F1 score: 0.6018, HS: 0.4529\n",
      "\n",
      "Train Trial [34/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.6341, Accuracy: 0.8284, F1 score: 0.5975, HS: 0.5029\n",
      "Valid Trial [34/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 4.4554, Accuracy: 0.8501, F1 score: 0.5925, HS: 0.4590\n",
      "\n",
      "Train Trial [35/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.6252, Accuracy: 0.8292, F1 score: 0.5993, HS: 0.5030\n",
      "Valid Trial [35/200], LR: 0.00023730468750000005, W: 0.01, Avg Loss: 5.3903, Accuracy: 0.8575, F1 score: 0.6047, HS: 0.4526\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [36/200], LR: 0.0002373, W: 0.01, Avg Loss: 0.6188, Accuracy: 0.8305, F1 score: 0.6003, HS: 0.5040\n",
      "Valid Trial [36/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 5.4438, Accuracy: 0.8565, F1 score: 0.5919, HS: 0.4534\n",
      "\n",
      "Train Trial [37/200], LR: 0.000178, W: 0.01, Avg Loss: 0.6000, Accuracy: 0.8379, F1 score: 0.6113, HS: 0.5136\n",
      "Valid Trial [37/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 4.6605, Accuracy: 0.8659, F1 score: 0.6253, HS: 0.4739\n",
      "\n",
      "Train Trial [38/200], LR: 0.000178, W: 0.01, Avg Loss: 0.5742, Accuracy: 0.8460, F1 score: 0.6239, HS: 0.5216\n",
      "Valid Trial [38/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 4.7551, Accuracy: 0.8609, F1 score: 0.6189, HS: 0.4651\n",
      "\n",
      "Train Trial [39/200], LR: 0.000178, W: 0.01, Avg Loss: 0.5361, Accuracy: 0.8575, F1 score: 0.6422, HS: 0.5545\n",
      "Valid Trial [39/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 5.5214, Accuracy: 0.8743, F1 score: 0.6256, HS: 0.4796\n",
      "\n",
      "Train Trial [40/200], LR: 0.000178, W: 0.01, Avg Loss: 0.5344, Accuracy: 0.8566, F1 score: 0.6409, HS: 0.5522\n",
      "Valid Trial [40/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 5.3361, Accuracy: 0.8679, F1 score: 0.6182, HS: 0.4667\n",
      "\n",
      "Train Trial [41/200], LR: 0.000178, W: 0.01, Avg Loss: 0.5333, Accuracy: 0.8580, F1 score: 0.6425, HS: 0.5511\n",
      "Valid Trial [41/200], LR: 0.00017797851562500002, W: 0.01, Avg Loss: 6.0736, Accuracy: 0.8787, F1 score: 0.6361, HS: 0.4965\n",
      "\n",
      "Train Trial [42/200], LR: 0.000178, W: 0.01, Avg Loss: 0.5150, Accuracy: 0.8641, F1 score: 0.6528, HS: 0.5619\n",
      "Valid Trial [42/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 6.0294, Accuracy: 0.8748, F1 score: 0.6287, HS: 0.4827\n",
      "\n",
      "Train Trial [43/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.5153, Accuracy: 0.8595, F1 score: 0.6459, HS: 0.5604\n",
      "Valid Trial [43/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 5.4546, Accuracy: 0.8708, F1 score: 0.6310, HS: 0.4821\n",
      "\n",
      "Train Trial [44/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.5283, Accuracy: 0.8602, F1 score: 0.6463, HS: 0.5566\n",
      "Valid Trial [44/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 5.3317, Accuracy: 0.8659, F1 score: 0.6201, HS: 0.4784\n",
      "\n",
      "Train Trial [45/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.5133, Accuracy: 0.8665, F1 score: 0.6568, HS: 0.5673\n",
      "Valid Trial [45/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 5.2737, Accuracy: 0.8560, F1 score: 0.6054, HS: 0.4555\n",
      "\n",
      "Train Trial [46/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.4983, Accuracy: 0.8692, F1 score: 0.6616, HS: 0.5709\n",
      "Valid Trial [46/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 5.5135, Accuracy: 0.8713, F1 score: 0.6319, HS: 0.4865\n",
      "\n",
      "Train Trial [47/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.4913, Accuracy: 0.8672, F1 score: 0.6582, HS: 0.5745\n",
      "Valid Trial [47/200], LR: 0.00013348388671875002, W: 0.01, Avg Loss: 5.7204, Accuracy: 0.8787, F1 score: 0.6476, HS: 0.5055\n",
      "\n",
      "Train Trial [48/200], LR: 0.0001335, W: 0.01, Avg Loss: 0.4658, Accuracy: 0.8765, F1 score: 0.6745, HS: 0.5871\n",
      "Valid Trial [48/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 6.4682, Accuracy: 0.8797, F1 score: 0.6401, HS: 0.4973\n",
      "\n",
      "Train Trial [49/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4499, Accuracy: 0.8796, F1 score: 0.6803, HS: 0.5986\n",
      "Valid Trial [49/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 6.0775, Accuracy: 0.8802, F1 score: 0.6442, HS: 0.5037\n",
      "\n",
      "Train Trial [50/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4517, Accuracy: 0.8827, F1 score: 0.6855, HS: 0.6015\n",
      "Valid Trial [50/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 6.9044, Accuracy: 0.8841, F1 score: 0.6498, HS: 0.5051\n",
      "\n",
      "Train Trial [51/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4439, Accuracy: 0.8810, F1 score: 0.6832, HS: 0.6074\n",
      "Valid Trial [51/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 7.0458, Accuracy: 0.8787, F1 score: 0.6361, HS: 0.4955\n",
      "\n",
      "Train Trial [52/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4389, Accuracy: 0.8825, F1 score: 0.6856, HS: 0.6038\n",
      "Valid Trial [52/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 7.2542, Accuracy: 0.8836, F1 score: 0.6446, HS: 0.5085\n",
      "\n",
      "Train Trial [53/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4238, Accuracy: 0.8878, F1 score: 0.6954, HS: 0.6121\n",
      "Valid Trial [53/200], LR: 0.00010011291503906251, W: 0.01, Avg Loss: 6.1935, Accuracy: 0.8787, F1 score: 0.6445, HS: 0.4981\n",
      "\n",
      "Train Trial [54/200], LR: 0.0001001, W: 0.01, Avg Loss: 0.4151, Accuracy: 0.8905, F1 score: 0.7006, HS: 0.6208\n",
      "Valid Trial [54/200], LR: 0.0001, W: 0.01, Avg Loss: 6.4645, Accuracy: 0.8802, F1 score: 0.6389, HS: 0.4979\n",
      "\n",
      "Train Trial [55/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4193, Accuracy: 0.8915, F1 score: 0.7025, HS: 0.6230\n",
      "Valid Trial [55/200], LR: 0.0001, W: 0.01, Avg Loss: 6.6260, Accuracy: 0.8787, F1 score: 0.6393, HS: 0.4960\n",
      "\n",
      "Train Trial [56/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4192, Accuracy: 0.8900, F1 score: 0.6998, HS: 0.6218\n",
      "Valid Trial [56/200], LR: 0.0001, W: 0.01, Avg Loss: 7.0088, Accuracy: 0.8762, F1 score: 0.6336, HS: 0.4812\n",
      "\n",
      "Train Trial [57/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3994, Accuracy: 0.8966, F1 score: 0.7126, HS: 0.6360\n",
      "Valid Trial [57/200], LR: 0.0001, W: 0.01, Avg Loss: 7.0692, Accuracy: 0.8886, F1 score: 0.6576, HS: 0.5244\n",
      "\n",
      "Train Trial [58/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3999, Accuracy: 0.8929, F1 score: 0.7053, HS: 0.6274\n",
      "Valid Trial [58/200], LR: 0.0001, W: 0.01, Avg Loss: 7.1253, Accuracy: 0.8876, F1 score: 0.6566, HS: 0.5155\n",
      "\n",
      "Train Trial [59/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4006, Accuracy: 0.8962, F1 score: 0.7116, HS: 0.6319\n",
      "Valid Trial [59/200], LR: 0.0001, W: 0.01, Avg Loss: 6.8127, Accuracy: 0.8807, F1 score: 0.6377, HS: 0.4944\n",
      "\n",
      "Train Trial [60/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4208, Accuracy: 0.8916, F1 score: 0.7026, HS: 0.6211\n",
      "Valid Trial [60/200], LR: 0.0001, W: 0.01, Avg Loss: 6.3751, Accuracy: 0.8669, F1 score: 0.6132, HS: 0.4652\n",
      "\n",
      "Train Trial [61/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4260, Accuracy: 0.8925, F1 score: 0.7043, HS: 0.6183\n",
      "Valid Trial [61/200], LR: 0.0001, W: 0.01, Avg Loss: 6.4444, Accuracy: 0.8821, F1 score: 0.6384, HS: 0.5004\n",
      "\n",
      "Train Trial [62/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3962, Accuracy: 0.8941, F1 score: 0.7082, HS: 0.6346\n",
      "Valid Trial [62/200], LR: 0.0001, W: 0.01, Avg Loss: 7.2270, Accuracy: 0.8895, F1 score: 0.6554, HS: 0.5167\n",
      "\n",
      "Train Trial [63/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3854, Accuracy: 0.9022, F1 score: 0.7242, HS: 0.6493\n",
      "Valid Trial [63/200], LR: 0.0001, W: 0.01, Avg Loss: 6.9521, Accuracy: 0.8886, F1 score: 0.6565, HS: 0.5177\n",
      "\n",
      "Train Trial [64/200], LR: 0.0001, W: 0.01, Avg Loss: 0.4025, Accuracy: 0.8983, F1 score: 0.7157, HS: 0.6429\n",
      "Valid Trial [64/200], LR: 0.0001, W: 0.01, Avg Loss: 7.6049, Accuracy: 0.8866, F1 score: 0.6440, HS: 0.5096\n",
      "\n",
      "Train Trial [65/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3992, Accuracy: 0.8949, F1 score: 0.7095, HS: 0.6307\n",
      "Valid Trial [65/200], LR: 0.0001, W: 0.01, Avg Loss: 6.7608, Accuracy: 0.8831, F1 score: 0.6540, HS: 0.5139\n",
      "\n",
      "Train Trial [66/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3783, Accuracy: 0.9000, F1 score: 0.7195, HS: 0.6392\n",
      "Valid Trial [66/200], LR: 0.0001, W: 0.01, Avg Loss: 7.7286, Accuracy: 0.8895, F1 score: 0.6500, HS: 0.5116\n",
      "\n",
      "Train Trial [67/200], LR: 0.0001, W: 0.01, Avg Loss: 0.3998, Accuracy: 0.9023, F1 score: 0.7237, HS: 0.6473\n",
      "Valid Trial [67/200], LR: 0.0001, W: 0.01, Avg Loss: 6.3306, Accuracy: 0.8802, F1 score: 0.6368, HS: 0.4965\n",
      "\n",
      "Out of patience!\n",
      "Best epoch: 57\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "w = 0.01\n",
    "\n",
    "# Early Stopping\n",
    "patience = 10\n",
    "bad_epochs = 0\n",
    "best_score = 0.0\n",
    "best_weights = None\n",
    "\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  score_train = alex_train(epoch, num_epochs, train_losses, learning_rate, w)\n",
    "  score_valid = alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  print(\"\")\n",
    "\n",
    "  # Early Stopping\n",
    "  if score_valid > best_score:\n",
    "    bad_epochs = 0\n",
    "    best_epoch = epoch\n",
    "    best_score = score_valid\n",
    "    best_weights = alex_net.state_dict()\n",
    "  else:\n",
    "    bad_epochs += 1\n",
    "  \n",
    "  if bad_epochs == patience:\n",
    "    print(\"Out of patience!\")\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    break\n",
    "\n",
    "torch.save(best_weights, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# alerta sonora para cuando haya terminado de entrenar\n",
    "import winsound\n",
    "import time\n",
    "for i in range(0, 2):\n",
    "    duration = 500\n",
    "    freq = 1500\n",
    "    winsound.Beep(freq, duration)\n",
    "    time.sleep(0.5)\n",
    "    i+=1\n",
    "winsound.Beep(freq, 2000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT BATCH SIZE:  124\n",
      "CURRENT BATCH SIZE:  124\n",
      "Test Trial [1/1], LR: 0.0001, W: 0.001, Avg Loss: 42.1041, Accuracy: 0.8710, F1 score: 0.6116, HS: 0.4730\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_losses = []\n",
    "test_pred = []\n",
    "learning_rate = 0.0001\n",
    "w = 0.001\n",
    "\n",
    "# Reset Model\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.load_state_dict(torch.load(model_path))\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "alex_test(1, 1, test_pred, test_losses, learning_rate, w, show_images = 0)\n",
    "test_pred_df = pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en ../outputs\\alexnet\\blur_mtnblur_elastic1\\26L_testval_weighted_1\\predictions.csv\n",
      "(234, 26)\n",
      "(234, 26)\n",
      "HS final: 0.47361280694614033\n"
     ]
    }
   ],
   "source": [
    "preds = test_pred[0][3].cpu().detach().numpy()\n",
    "for i in range(1, len(test_pred)):\n",
    "  pbi = test_pred[i][3].cpu().detach().numpy()\n",
    "  preds = np.concatenate((preds, pbi), axis=0)\n",
    "save_df = pd.DataFrame(preds)\n",
    "save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "\n",
    "preds = pd.read_csv(os.path.join(output_dir, 'predictions.csv'), index_col=0)\n",
    "preds = preds.values\n",
    "testval = pd.concat([labels_test, labels_val])\n",
    "testval = filter_dfs(testval, top_labels)\n",
    "\n",
    "print(preds.shape)\n",
    "print(testval.shape)\n",
    "hs_final = hamming_score(preds, testval.values)\n",
    "print(f\"HS final: {hs_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BATCH_SIZE': 124, 'optimizer': 'Adam', 'scheduler': 'ReduceLROnPlateau', 'criterion': 'BCEWithLogitsLoss', 'epochs': 200, 'best_epoch': 57, 'data_flags': 'blur_mtnblur_elastic1', 'use_pos_weights': True, 'pos_weights_factor': 1, 'NUM_LABELS': 26, 'use_testval': True, 'TH_TRAIN': 0.5, 'TH_VAL': 0.5, 'TH_TEST': 0.5, 'HS_FINAL': 0.47361280694614033, 'patience': 10}\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'optimizer': (type (optimizer).__name__),\n",
    "    'scheduler': (type (scheduler).__name__),\n",
    "    'criterion': (type (criterion).__name__),\n",
    "    'epochs': num_epochs,\n",
    "    'best_epoch': best_epoch,\n",
    "    'data_flags': data_flags,\n",
    "    'use_pos_weights': use_pos_weights,\n",
    "    'pos_weights_factor': pos_weights_factor,\n",
    "    'NUM_LABELS': NUM_LABELS,\n",
    "    'use_testval': use_testval,\n",
    "    'TH_TRAIN': TH_TRAIN,\n",
    "    'TH_VAL': TH_VAL,\n",
    "    'TH_TEST': TH_TEST,\n",
    "    'HS_FINAL': hs_final,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'metadata.pickle'), 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'metadata.pickle'), 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}