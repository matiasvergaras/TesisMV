{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Montando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '../content/gdrive/MyDrive'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = ['base']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 2\n",
    "RANDOM_TIMES = 2\n",
    "ELASTIC_TIMES = 3\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 2.5\n",
    "NUM_LABELS = 30\n",
    "use_testval = True\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns set encontrado en ../patterns\\base\n",
      "Labels set encontrado en ../labels\\base\n",
      "Nombre del experimento: 30L_testval_weighted_2_5\n",
      "Los resultados se guardarán en: ../outputs\\alexnet\\base\\30L_testval_weighted_2_5\n",
      "Los resultados se guardarán en: ../outputs\\alexnet\\base\\30L_testval_weighted_2_5\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = os.path.join(root_dir, 'patterns', data_flags) \n",
    "labels_path = os.path.join(root_dir, 'labels', data_flags)\n",
    "\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista en {}\".format(\n",
    "        (os.path.isdir(patterns_path), os.path.isdir(labels_path))))\n",
    "print(\"Patterns set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "\n",
    "exp_name = f\"{NUM_LABELS}L\"\n",
    "exp_name += \"_testval\" if use_testval else \"\"\n",
    "weights_str = str(pos_weights_factor)\n",
    "weights_str = weights_str.replace('.','_')\n",
    "exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "print(f\"Nombre del experimento: {exp_name}\")\n",
    "\n",
    "output_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name)\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags)\n",
    "model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "print(f\"Los resultados se guardarán en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_train = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "labels_val = pd.read_json(os.path.join(labels_path, 'val_df.json'), orient='index')\n",
    "labels_test = pd.read_json(os.path.join(labels_path, 'test_df.json'), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84a    1\n",
       "Name: outline, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test.loc[['84a']]['outline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando threshold 22 para trabajar con 30 labels\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "top_labels = filter_labels(labels, number_labels = NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1915"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_labels[0].values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "top_labels, _ = filter_labels(train_labels, number_labels=NUM_LABELS)\n",
    "NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "if use_pos_weights:\n",
    "    pos_weights = make_positive_weights(top_labels, pos_weights_factor)\n",
    "    pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "else:\n",
    "    pos_weights = None\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels, extra_labels = None, extra_images_dir = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_file(string): path to text file\n",
    "        root_dir(string): directory with all train images\n",
    "    \"\"\"\n",
    "    self.labels_frame = filter_dfs(pd.read_json(labels_file, orient='index'), top_labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "    self.extra_images_dir = None \n",
    "\n",
    "    # para crear conjunto test-val \n",
    "    if extra_labels:\n",
    "      extra_labels_frame = filter_dfs(pd.read_json(extra_labels, orient='index'), top_labels)\n",
    "      self.labels_frame = pd.DataFrame.append(self.labels_frame, extra_labels_frame)\n",
    "      self.extra_images_dir = extra_images_dir\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    # caso test-val\n",
    "    if (self.extra_images_dir is not None) and (img_name is None):\n",
    "      for chapter in os.listdir(self.extra_images_dir):\n",
    "        if img_id in os.listdir(os.path.join(self.extra_images_dir, chapter)):\n",
    "          img_name = os.path.join(self.extra_images_dir, chapter, img_id)\n",
    "          break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Alexnet requires 227 x 227\n",
    "# Training\n",
    "kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_path, 'train'),\n",
    "                                 labels_file=os.path.join(labels_path, 'augmented_train_df.json'),\n",
    "                                 transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                               transforms.ToTensor(),\n",
    "                                                               transforms.Normalize(\n",
    "                                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                                   std=[0.229, 0.224, 0.225])]),\n",
    "                                 top_labels=top_labels)\n",
    "\n",
    "kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation\n",
    "kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_path, 'val'),\n",
    "                                      labels_file=os.path.join(labels_path, 'val_df.json'),\n",
    "                                      transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(\n",
    "                                                                        mean=[0.485, 0.456, 0.406],\n",
    "                                                                        std=[0.229, 0.224, 0.225])]),\n",
    "                                      top_labels=top_labels)\n",
    "\n",
    "kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                      num_workers=0)\n",
    "\n",
    "# Test\n",
    "kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_path, 'test'),\n",
    "                                labels_file=os.path.join(labels_path, 'test_df.json'),\n",
    "                                transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize(\n",
    "                                                                  mean=[0.485, 0.456, 0.406],\n",
    "                                                                  std=[0.229, 0.224, 0.225])]),\n",
    "                                top_labels=top_labels,\n",
    "                                extra_labels=os.path.join(labels_path, 'val_df.json' if use_testval else None),\n",
    "                                extra_images_dir=os.path.join(patterns_path, 'val' if use_testval else None))\n",
    "\n",
    "kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# Define the function for training, validation, and test\n",
    "def alex_train(epoch, num_epochs, train_losses, learning_rate, w):\n",
    "  alex_net.train()\n",
    "  train_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  for i, sample_batched in enumerate(kunischTrainLoader, 1):\n",
    "      inputs = sample_batched['image'].to(device)\n",
    "      labels = sample_batched['labels'].to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = alex_net(inputs)\n",
    "      loss = criterion(outputs.float(), labels.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "      # print(pred)\n",
    "      labels = labels.int()\n",
    "      # print(labels)\n",
    "      pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "      labels = labels.int()\n",
    "      preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "      labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "    \n",
    "      TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "      TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "      FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "      FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "      #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP, TN, FP, FN))\n",
    "\n",
    "  TP = TP.cpu().numpy()\n",
    "  TN = TN.cpu().numpy()\n",
    "  FP = FP.cpu().numpy()\n",
    "  FN = FN.cpu().numpy()\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  train_loss = train_loss / len(kunischTrainLoader.dataset) * BATCH_SIZE\n",
    "  hs = hamming_score(preds_total, labels_total)\n",
    "  train_losses.append([epoch, learning_rate, w, train_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "  # print statistics\n",
    "  print('Train Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "        .format(epoch, num_epochs, learning_rate, w, train_loss, accuracy, f1_score, hs))\n",
    "  return hs\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, learning_rate, w, valid_loss, accuracy, f1_score, hs))\n",
    "      return hs\n",
    "\n",
    "def alex_test(epoch, num_epochs, pred_array, test_losses, learning_rate, w, show_images=1):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  test_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      for i,sample_batched in enumerate(kunischTestLoader, 1):\n",
    "          print(\"CURRENT BATCH SIZE: \", BATCH_SIZE)\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          paths = sample_batched['paths']\n",
    "          outputs = alex_net(inputs)\n",
    "          \n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          test_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "          # print(pred)\n",
    "          labels = labels.int()\n",
    "          # print(labels)\n",
    "          pred_array.append([paths, test_loss, labels, pred])\n",
    "          pred = (torch.sigmoid(outputs).data > 0.5).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "          \n",
    "          for j in range(0, min(BATCH_SIZE, show_images)): # j itera sobre ejemplos\n",
    "              print(f\"Mostrando imagen {j} del batch {i}\")\n",
    "              img = np.transpose(sample_batched['image'][j]) # imagen j \n",
    "              plt.imshow(img, interpolation='nearest')\n",
    "              plt.show()\n",
    "              labels_correctos = \"\"\n",
    "              labels_predichos = \"\"\n",
    "              for k in range(0, len(pred[j])):\n",
    "                labels_correctos += (kunischTestSet.labels_frame.columns.values[k]+' ') if labels[j].cpu().detach()[k] else \"\"\n",
    "                labels_predichos += (kunischTestSet.labels_frame.columns.values[k]+' ') if pred[j].cpu().detach()[k] else \"\"\n",
    "              print(\"Labels correctos:\")\n",
    "              #print(labels[j].cpu().detach().numpy())\n",
    "              print(labels_correctos)\n",
    "              print(\"Labels predichos:\")\n",
    "              #print(pred[j].cpu().detach().numpy())\n",
    "              print(labels_predichos)\n",
    "              print(\"\\n\")\n",
    "            \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "      test_loss = test_loss / len(kunischTestLoader.dataset) * 1024  # 1024 is the batch size\n",
    "      test_losses.append([epoch, learning_rate, w, test_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Test Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, learning_rate, w, test_loss, accuracy, f1_score, hs))\n",
    "      return hs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyper Parameter Tuning\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "for param in alex_net.parameters():\n",
    "    param.requires_grad = False\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))), 4)  # pull geometrically\n",
    "  w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))), 10)  # pull geometrically\n",
    "\n",
    "  # Reset Model per test\n",
    "  alex_net = models.alexnet(pretrained=True)\n",
    "  alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "  alex_net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "  criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "  \n",
    "  train_losses_df = pd.DataFrame(train_losses)\n",
    "  train_losses_df.to_csv(os.path.join(output_dir, 'loss_hypertrain.csv'))\n",
    "\n",
    "  alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  validation_losses_df = pd.DataFrame(validation_losses)\n",
    "  validation_losses_df.to_csv(os.path.join(output_dir, 'loss_hyperval.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "train_losses = []\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "w = 0.001\n",
    "\n",
    "# Early Stopping\n",
    "patience = 10\n",
    "bad_epochs = 0\n",
    "best_score = 0.0\n",
    "best_weights = None\n",
    "\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  score_train = alex_train(epoch, num_epochs, train_losses, learning_rate, w)\n",
    "  score_valid = alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  scheduler.step()\n",
    "  print(\"\")\n",
    "  \n",
    "  # Early Stopping\n",
    "  if score_valid > best_score:\n",
    "    bad_epochs = 0\n",
    "    best_epoch = epoch\n",
    "    best_score = score_valid\n",
    "    best_weights = alex_net.state_dict()\n",
    "  else:\n",
    "    bad_epochs += 1\n",
    "  \n",
    "  if bad_epochs == patience:\n",
    "    print(\"Out of patience!\")\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    break\n",
    "\n",
    "torch.save(best_weights, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_losses = []\n",
    "test_pred = []\n",
    "learning_rate = 0.0001\n",
    "w = 0.001\n",
    "\n",
    "# Reset Model\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.load_state_dict(torch.load(model_path))\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "alex_test(1, 1, test_pred, test_losses, learning_rate, w, show_images = 1)\n",
    "test_pred_df = pd.DataFrame(test_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando threshold 22 para trabajar con 30 labels\n",
      "(100, 30)\n",
      "(234, 30)\n",
      "HS final: 0.4994444444444444\n"
     ]
    }
   ],
   "source": [
    "preds = test_pred[0][3].cpu().detach().numpy()\n",
    "for i in range(1, len(test_pred)):\n",
    "  pbi = test_pred[i][3].cpu().detach().numpy()\n",
    "  preds = np.concatenate((preds, pbi), axis=0)\n",
    "save_df = pd.DataFrame(preds)\n",
    "save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "\n",
    "tops, _ = filter_labels(train_labels, number_labels = NUM_LABELS)\n",
    "testval = pd.concat([labels_test, labels_val])\n",
    "testval = filter_dfs(testval, tops)\n",
    "\n",
    "print(preds.shape)\n",
    "print(testval.shape)\n",
    "print(f\"HS final: {hamming_score(preds, testval.values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}