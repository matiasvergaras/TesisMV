{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Montando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '../content/gdrive/MyDrive'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SUBCHAPTERS = False\n",
    "DS_FLAGS = ['blur']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 2\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NUM_LABELS = 26\n",
    "use_testval = False\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "TH_TRAIN = 0.5\n",
    "TH_VAL = 0.5\n",
    "TH_TEST = 0.5\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns set encontrado en ../patterns\\blur\n",
      "Labels set encontrado en ../labels\\blur\n",
      "Nombre del experimento: 26L_weighted_1\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = os.path.join(root_dir, 'patterns', data_flags) \n",
    "labels_path = os.path.join(root_dir, 'labels', data_flags)\n",
    "\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista en {}\".format(\n",
    "        (os.path.isdir(patterns_path), os.path.isdir(labels_path))))\n",
    "print(\"Patterns set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "\n",
    "exp_name = f\"{NUM_LABELS}L\"\n",
    "exp_name += \"_testval\" if use_testval else \"\"\n",
    "weights_str = str(pos_weights_factor)\n",
    "weights_str = weights_str.replace('.','_')\n",
    "exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "print(f\"Nombre del experimento: {exp_name}\")\n",
    "\n",
    "output_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name)\n",
    "\n",
    "\n",
    "model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags)\n",
    "model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "if SAVE:\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    os.makedirs(model_dir, exist_ok = True)\n",
    "    print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "    print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x1f376b89b10>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels_train = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "labels_val = pd.read_json(os.path.join(labels_path, 'val_df.json'), orient='index')\n",
    "labels_test = pd.read_json(os.path.join(labels_path, 'test_df.json'), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pendent</th>\n",
       "      <th>teardrop</th>\n",
       "      <th>horizontal</th>\n",
       "      <th>panel</th>\n",
       "      <th>group</th>\n",
       "      <th>vertical</th>\n",
       "      <th>bar</th>\n",
       "      <th>floating</th>\n",
       "      <th>enclosing</th>\n",
       "      <th>shorter</th>\n",
       "      <th>...</th>\n",
       "      <th>light</th>\n",
       "      <th>body</th>\n",
       "      <th>bird</th>\n",
       "      <th>striped</th>\n",
       "      <th>worm</th>\n",
       "      <th>angular</th>\n",
       "      <th>raised</th>\n",
       "      <th>head</th>\n",
       "      <th>bird-seed</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40g</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92b</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81j</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75h</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pendent  teardrop  horizontal  panel  group  vertical  bar  floating  \\\n",
       "53b        0         0           1      1      0         0    0         0   \n",
       "2b         0         0           1      1      0         1    1         0   \n",
       "40g        0         0           0      1      0         1    0         0   \n",
       "70h        0         0           0      0      0         0    0         0   \n",
       "94a        0         0           0      0      0         0    0         0   \n",
       "..       ...       ...         ...    ...    ...       ...  ...       ...   \n",
       "85c        0         0           1      1      0         0    0         0   \n",
       "92b        0         0           0      0      0         0    0         0   \n",
       "71a        0         0           1      1      0         0    0         0   \n",
       "81j        0         0           0      0      0         0    0         0   \n",
       "75h        0         0           1      1      0         0    0         0   \n",
       "\n",
       "     enclosing  shorter  ...  light  body  bird  striped  worm  angular  \\\n",
       "53b          0        0  ...      0     0     0        0     0        0   \n",
       "2b           0        0  ...      0     0     0        0     0        0   \n",
       "40g          0        0  ...      0     0     0        0     0        0   \n",
       "70h          0        0  ...      0     0     0        0     0        0   \n",
       "94a          0        0  ...      0     0     0        0     0        0   \n",
       "..         ...      ...  ...    ...   ...   ...      ...   ...      ...   \n",
       "85c          0        0  ...      0     0     0        0     0        0   \n",
       "92b          0        0  ...      0     0     0        0     0        0   \n",
       "71a          0        0  ...      0     0     0        0     0        0   \n",
       "81j          1        0  ...      0     0     0        0     0        0   \n",
       "75h          0        0  ...      0     0     0        0     0        0   \n",
       "\n",
       "     raised  head  bird-seed  long  \n",
       "53b       0     0          0     0  \n",
       "2b        0     0          0     0  \n",
       "40g       0     0          0     0  \n",
       "70h       0     0          0     0  \n",
       "94a       0     0          0     0  \n",
       "..      ...   ...        ...   ...  \n",
       "85c       0     0          0     0  \n",
       "92b       0     0          0     0  \n",
       "71a       0     0          0     0  \n",
       "81j       0     0          0     0  \n",
       "75h       0     0          0     0  \n",
       "\n",
       "[156 rows x 330 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n",
      "panel            289\n",
      "horizontal       234\n",
      "ornament         123\n",
      "hatched          108\n",
      "vertical         104\n",
      "circle            80\n",
      "metopal           77\n",
      "filling           74\n",
      "lozenge           64\n",
      "enclosing         62\n",
      "double            54\n",
      "cross-hatched     51\n",
      "triangle          49\n",
      "line              42\n",
      "chain             41\n",
      "concentric        40\n",
      "meander           40\n",
      "bar               39\n",
      "dotted            37\n",
      "solid             35\n",
      "dot               34\n",
      "cross             31\n",
      "outline           31\n",
      "single            28\n",
      "hook              27\n",
      "floor             27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "    print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "    top_labels, _ = filter_labels(train_labels, number_labels = NUM_LABELS)\n",
    "    save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "    if save == \"y\":\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "            pickle.dump(top_labels, f)\n",
    "        print(top_labels)\n",
    "    else:\n",
    "        raise Exception(\"No se logró cargar top_labels\")\n",
    "else:\n",
    "    print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "    with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "        top_labels = pickle.load(f)\n",
    "    print(top_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() argument must be str, bytes, or os.PathLike object, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 91>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m kunischValidationLoader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(kunischValidationSet, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m                                                       num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[0;32m     91\u001b[0m kunischTestSet \u001b[38;5;241m=\u001b[39m KunischDataset(images_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patterns_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     92\u001b[0m                                 labels_file\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labels_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_df.json\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     93\u001b[0m                                 transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m227\u001b[39m, \u001b[38;5;241m227\u001b[39m)),\n\u001b[0;32m     94\u001b[0m                                                               transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     95\u001b[0m                                                               transforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[0;32m     96\u001b[0m                                                                   mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[0;32m     97\u001b[0m                                                                   std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])]),\n\u001b[0;32m     98\u001b[0m                                 top_labels\u001b[38;5;241m=\u001b[39mtop_labels,\n\u001b[1;32m---> 99\u001b[0m                                 extra_labels\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_df.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_testval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[0;32m    100\u001b[0m                                 extra_images_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patterns_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_testval \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    102\u001b[0m kunischTestLoader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(kunischTestSet, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Define the function for training, validation, and test\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\ntpath.py:117\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_drive \u001b[38;5;241m+\u001b[39m result_path\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mBytesWarning\u001b[39;00m):\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mgenericpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_arg_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\genericpath.py:152\u001b[0m, in \u001b[0;36m_check_arg_types\u001b[1;34m(funcname, *args)\u001b[0m\n\u001b[0;32m    150\u001b[0m         hasbytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() argument must be str, bytes, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike object, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hasstr \u001b[38;5;129;01mand\u001b[39;00m hasbytes:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt mix strings and bytes in path components\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_json(os.path.join(labels_path, 'augmented_train_df.json'), orient='index')\n",
    "NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "if use_pos_weights:\n",
    "    pos_weights = make_positive_weights(top_labels, pos_weights_factor)\n",
    "    pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "else:\n",
    "    pos_weights = None\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels, extra_labels = None, extra_images_dir = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_file(string): path to text file\n",
    "        root_dir(string): directory with all train images\n",
    "    \"\"\"\n",
    "    self.labels_frame = filter_dfs(pd.read_json(labels_file, orient='index'), top_labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "    self.extra_images_dir = None \n",
    "\n",
    "    # para crear conjunto test-val \n",
    "    if extra_labels:\n",
    "      extra_labels_frame = filter_dfs(pd.read_json(extra_labels, orient='index'), top_labels)\n",
    "      self.labels_frame = pd.DataFrame.append(self.labels_frame, extra_labels_frame)\n",
    "      self.extra_images_dir = extra_images_dir\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    # caso test-val\n",
    "    if (self.extra_images_dir is not None) and (img_name is None):\n",
    "      for chapter in os.listdir(self.extra_images_dir):\n",
    "        if img_id in os.listdir(os.path.join(self.extra_images_dir, chapter)):\n",
    "          img_name = os.path.join(self.extra_images_dir, chapter, img_id)\n",
    "          break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Alexnet requires 227 x 227\n",
    "# Training\n",
    "kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_path, 'train'),\n",
    "                                 labels_file=os.path.join(labels_path, 'augmented_train_df.json'),\n",
    "                                 transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                               transforms.ToTensor(),\n",
    "                                                               transforms.Normalize(\n",
    "                                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                                   std=[0.229, 0.224, 0.225])]),\n",
    "                                 top_labels=top_labels)\n",
    "\n",
    "kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation\n",
    "kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_path, 'val'),\n",
    "                                      labels_file=os.path.join(labels_path, 'val_df.json'),\n",
    "                                      transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(\n",
    "                                                                        mean=[0.485, 0.456, 0.406],\n",
    "                                                                        std=[0.229, 0.224, 0.225])]),\n",
    "                                      top_labels=top_labels)\n",
    "\n",
    "kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                      num_workers=0)\n",
    "\n",
    "# Test\n",
    "kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_path, 'test'),\n",
    "                                labels_file=os.path.join(labels_path, 'test_df.json'),\n",
    "                                transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize(\n",
    "                                                                  mean=[0.485, 0.456, 0.406],\n",
    "                                                                  std=[0.229, 0.224, 0.225])]),\n",
    "                                top_labels=top_labels,\n",
    "                                extra_labels=os.path.join(labels_path, 'val_df.json' if use_testval else None),\n",
    "                                extra_images_dir=os.path.join(patterns_path, 'val' if use_testval else None))\n",
    "\n",
    "kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# Define the function for training, validation, and test\n",
    "def alex_train(epoch, num_epochs, train_losses, learning_rate, w):\n",
    "  alex_net.train()\n",
    "  train_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  for i, sample_batched in enumerate(kunischTrainLoader, 1):\n",
    "      inputs = sample_batched['image'].to(device)\n",
    "      labels = sample_batched['labels'].to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = alex_net(inputs)\n",
    "      loss = criterion(outputs.float(), labels.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      train_loss += loss.item()\n",
    "      pred = (torch.sigmoid(outputs).data > TH_TRAIN).int()\n",
    "      # print(pred)\n",
    "      labels = labels.int()\n",
    "      # print(labels)\n",
    "      preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "      labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "    \n",
    "      TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "      TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "      FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "      FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "      #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP, TN, FP, FN))\n",
    "  \n",
    "\n",
    "  TP = TP.cpu().numpy()\n",
    "  TN = TN.cpu().numpy()\n",
    "  FP = FP.cpu().numpy()\n",
    "  FN = FN.cpu().numpy()\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  train_loss = train_loss / len(kunischTrainLoader.dataset) * BATCH_SIZE\n",
    "  hs = hamming_score(preds_total, labels_total)\n",
    "  train_losses.append([epoch, learning_rate, w, train_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "\n",
    "  # print statistics\n",
    "  print('Train Trial [{}/{}], LR: {:.4g}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "        .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, train_loss, accuracy, f1_score, hs))\n",
    "  return hs\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_VAL).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "    \n",
    "      scheduler.step(hs)\n",
    "\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, valid_loss, accuracy, f1_score, hs))\n",
    "      return hs\n",
    "\n",
    "def alex_test(epoch, num_epochs, pred_array, test_losses, learning_rate, w, show_images=1):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  test_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      for i,sample_batched in enumerate(kunischTestLoader, 1):\n",
    "          print(\"CURRENT BATCH SIZE: \", BATCH_SIZE)\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          paths = sample_batched['paths']\n",
    "          outputs = alex_net(inputs)\n",
    "          \n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          test_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_TEST).int()\n",
    "          # print(pred)\n",
    "          labels = labels.int()\n",
    "          # print(labels)\n",
    "          pred_array.append([paths, test_loss, labels, pred])\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "          \n",
    "          for j in range(0, min(BATCH_SIZE, show_images)): # j itera sobre ejemplos\n",
    "              print(f\"Mostrando imagen {j} del batch {i}\")\n",
    "              img = np.transpose(sample_batched['image'][j]) # imagen j \n",
    "              plt.imshow(img, interpolation='nearest')\n",
    "              plt.show()\n",
    "              labels_correctos = \"\"\n",
    "              labels_predichos = \"\"\n",
    "              for k in range(0, len(pred[j])):\n",
    "                labels_correctos += (kunischTestSet.labels_frame.columns.values[k]+' ') if labels[j].cpu().detach()[k] else \"\"\n",
    "                labels_predichos += (kunischTestSet.labels_frame.columns.values[k]+' ') if pred[j].cpu().detach()[k] else \"\"\n",
    "              print(\"Labels correctos:\")\n",
    "              #print(labels[j].cpu().detach().numpy())\n",
    "              print(labels_correctos)\n",
    "              print(\"Labels predichos:\")\n",
    "              #print(pred[j].cpu().detach().numpy())\n",
    "              print(labels_predichos)\n",
    "              print(\"\\n\")\n",
    "            \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "      test_loss = test_loss / len(kunischTestLoader.dataset) * 1024  # 1024 is the batch size\n",
    "      test_losses.append([epoch, learning_rate, w, test_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Test Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, test_loss, accuracy, f1_score, hs))\n",
    "      return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq=25, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels \n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered. \n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = labels_df.shape[1]\n",
    "      pivot = 0\n",
    "      while filtered_labels > number_labels:\n",
    "        #print(filtered_labels, number_labels, pivot)\n",
    "        filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "        top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "        filtered_labels = filtered_df.shape[1]\n",
    "        pivot += 1\n",
    "      print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "      return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Trial [0/10], LR: 0.0001, W: 9.6847e-06, Avg Loss: 5.7758, Accuracy: 0.4645, F1 score: 0.2448, HS: 0.1381\n",
      "Valid Trial [1/10], LR: 0.0002, W: 2.10098e-05, Avg Loss: 6.5135, Accuracy: 0.5705, F1 score: 0.1806, HS: 0.1010\n",
      "Valid Trial [2/10], LR: 0.0002, W: 2.37318e-05, Avg Loss: 6.0101, Accuracy: 0.4882, F1 score: 0.2160, HS: 0.1228\n",
      "Valid Trial [3/10], LR: 0.0037, W: 7.043e-07, Avg Loss: 6.3187, Accuracy: 0.5025, F1 score: 0.2024, HS: 0.1143\n",
      "Valid Trial [4/10], LR: 0.0005, W: 9.0946e-06, Avg Loss: 5.9385, Accuracy: 0.4758, F1 score: 0.2466, HS: 0.1426\n",
      "Valid Trial [5/10], LR: 0.0021, W: 2.22703e-05, Avg Loss: 6.4601, Accuracy: 0.5064, F1 score: 0.2087, HS: 0.1175\n",
      "Valid Trial [6/10], LR: 0.0003, W: 4.18e-07, Avg Loss: 5.9566, Accuracy: 0.5399, F1 score: 0.2445, HS: 0.1398\n",
      "Valid Trial [7/10], LR: 0.0004, W: 2.3604e-06, Avg Loss: 6.2250, Accuracy: 0.4832, F1 score: 0.2061, HS: 0.1153\n",
      "Valid Trial [8/10], LR: 0.0034, W: 9.62e-06, Avg Loss: 6.2544, Accuracy: 0.5276, F1 score: 0.2287, HS: 0.1321\n",
      "Valid Trial [9/10], LR: 0.0009, W: 2.73053e-05, Avg Loss: 5.9561, Accuracy: 0.4596, F1 score: 0.2336, HS: 0.1364\n"
     ]
    }
   ],
   "source": [
    "# Hyper Parameter Tuning\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "for param in alex_net.parameters():\n",
    "    param.requires_grad = False\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))), 4)  # pull geometrically\n",
    "  w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))), 10)  # pull geometrically\n",
    "\n",
    "  # Reset Model per test\n",
    "  alex_net = models.alexnet(pretrained=True)\n",
    "  alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "  alex_net.to(device)\n",
    "\n",
    "  optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "  criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.00005)\n",
    "\n",
    "  if SAVE:\n",
    "      train_losses_df = pd.DataFrame(train_losses)\n",
    "      train_losses_df.to_csv(os.path.join(output_dir, 'loss_hypertrain.csv'))\n",
    "\n",
    "  alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  if SAVE:\n",
    "      validation_losses_df = pd.DataFrame(validation_losses)\n",
    "      validation_losses_df.to_csv(os.path.join(output_dir, 'loss_hyperval.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 3.0579, Accuracy: 0.3060, F1 score: 0.2508, HS: 0.1431\n",
      "Valid Trial [0/200], LR: 0.001, W: 0.01, Avg Loss: 3.0502, Accuracy: 0.3767, F1 score: 0.2891, HS: 0.1683\n",
      "\n",
      "Train Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 1.8051, Accuracy: 0.4156, F1 score: 0.2974, HS: 0.1805\n",
      "Valid Trial [1/200], LR: 0.001, W: 0.01, Avg Loss: 2.6575, Accuracy: 0.5192, F1 score: 0.3496, HS: 0.2181\n",
      "\n",
      "Train Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 1.5475, Accuracy: 0.5197, F1 score: 0.3417, HS: 0.2151\n",
      "Valid Trial [2/200], LR: 0.001, W: 0.01, Avg Loss: 2.6891, Accuracy: 0.6425, F1 score: 0.4072, HS: 0.2686\n",
      "\n",
      "Train Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 1.4043, Accuracy: 0.5960, F1 score: 0.3808, HS: 0.2447\n",
      "Valid Trial [3/200], LR: 0.001, W: 0.01, Avg Loss: 2.4738, Accuracy: 0.6154, F1 score: 0.3963, HS: 0.2551\n",
      "\n",
      "Train Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 1.2263, Accuracy: 0.6594, F1 score: 0.4223, HS: 0.2817\n",
      "Valid Trial [4/200], LR: 0.001, W: 0.01, Avg Loss: 2.5219, Accuracy: 0.6548, F1 score: 0.4224, HS: 0.2805\n",
      "\n",
      "Train Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 1.0916, Accuracy: 0.6995, F1 score: 0.4552, HS: 0.3116\n",
      "Valid Trial [5/200], LR: 0.001, W: 0.01, Avg Loss: 2.7649, Accuracy: 0.6637, F1 score: 0.4269, HS: 0.2793\n",
      "\n",
      "Train Trial [6/200], LR: 0.001, W: 0.01, Avg Loss: 1.1160, Accuracy: 0.7212, F1 score: 0.4709, HS: 0.3186\n",
      "Valid Trial [6/200], LR: 0.00075, W: 0.01, Avg Loss: 2.8204, Accuracy: 0.6208, F1 score: 0.3969, HS: 0.2547\n",
      "\n",
      "Train Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 1.0638, Accuracy: 0.6696, F1 score: 0.4336, HS: 0.2875\n",
      "Valid Trial [7/200], LR: 0.00075, W: 0.01, Avg Loss: 3.2404, Accuracy: 0.7500, F1 score: 0.4915, HS: 0.3439\n",
      "\n",
      "Train Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 0.8753, Accuracy: 0.7675, F1 score: 0.5213, HS: 0.3687\n",
      "Valid Trial [8/200], LR: 0.00075, W: 0.01, Avg Loss: 2.9465, Accuracy: 0.7475, F1 score: 0.4900, HS: 0.3422\n",
      "\n",
      "Train Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 0.8059, Accuracy: 0.8011, F1 score: 0.5594, HS: 0.4030\n",
      "Valid Trial [9/200], LR: 0.00075, W: 0.01, Avg Loss: 3.3494, Accuracy: 0.7465, F1 score: 0.4860, HS: 0.3380\n",
      "\n",
      "Train Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 0.7939, Accuracy: 0.7930, F1 score: 0.5511, HS: 0.3946\n",
      "Valid Trial [10/200], LR: 0.00075, W: 0.01, Avg Loss: 3.1879, Accuracy: 0.7983, F1 score: 0.5420, HS: 0.3916\n",
      "\n",
      "Train Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6887, Accuracy: 0.8300, F1 score: 0.5991, HS: 0.4407\n",
      "Valid Trial [11/200], LR: 0.00075, W: 0.01, Avg Loss: 3.5759, Accuracy: 0.7885, F1 score: 0.5185, HS: 0.3745\n",
      "\n",
      "Train Trial [12/200], LR: 0.00075, W: 0.01, Avg Loss: 0.6775, Accuracy: 0.8298, F1 score: 0.5994, HS: 0.4446\n",
      "Valid Trial [12/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 4.2760, Accuracy: 0.8205, F1 score: 0.5635, HS: 0.4197\n",
      "\n",
      "Train Trial [13/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.6238, Accuracy: 0.8548, F1 score: 0.6365, HS: 0.4875\n",
      "Valid Trial [13/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 3.5889, Accuracy: 0.8037, F1 score: 0.5446, HS: 0.3983\n",
      "\n",
      "Train Trial [14/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.5195, Accuracy: 0.8656, F1 score: 0.6559, HS: 0.5107\n",
      "Valid Trial [14/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 4.3730, Accuracy: 0.8437, F1 score: 0.5962, HS: 0.4495\n",
      "\n",
      "Train Trial [15/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4911, Accuracy: 0.8799, F1 score: 0.6803, HS: 0.5370\n",
      "Valid Trial [15/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 3.6519, Accuracy: 0.8471, F1 score: 0.6036, HS: 0.4518\n",
      "\n",
      "Train Trial [16/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4634, Accuracy: 0.8898, F1 score: 0.6993, HS: 0.5542\n",
      "Valid Trial [16/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 4.3749, Accuracy: 0.8437, F1 score: 0.5972, HS: 0.4395\n",
      "\n",
      "Train Trial [17/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4397, Accuracy: 0.8981, F1 score: 0.7151, HS: 0.5728\n",
      "Valid Trial [17/200], LR: 0.0005625000000000001, W: 0.01, Avg Loss: 4.0744, Accuracy: 0.8373, F1 score: 0.5885, HS: 0.4422\n",
      "\n",
      "Train Trial [18/200], LR: 0.0005625, W: 0.01, Avg Loss: 0.4758, Accuracy: 0.8867, F1 score: 0.6927, HS: 0.5448\n",
      "Valid Trial [18/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 5.4671, Accuracy: 0.8422, F1 score: 0.5855, HS: 0.4337\n",
      "\n",
      "Train Trial [19/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.4757, Accuracy: 0.8861, F1 score: 0.6921, HS: 0.5409\n",
      "Valid Trial [19/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 4.4693, Accuracy: 0.8506, F1 score: 0.6060, HS: 0.4640\n",
      "\n",
      "Train Trial [20/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.3939, Accuracy: 0.9085, F1 score: 0.7370, HS: 0.5966\n",
      "Valid Trial [20/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 4.5133, Accuracy: 0.8536, F1 score: 0.6148, HS: 0.4623\n",
      "\n",
      "Train Trial [21/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.3620, Accuracy: 0.9128, F1 score: 0.7468, HS: 0.6176\n",
      "Valid Trial [21/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 6.4284, Accuracy: 0.8649, F1 score: 0.6194, HS: 0.4737\n",
      "\n",
      "Train Trial [22/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.3407, Accuracy: 0.9249, F1 score: 0.7735, HS: 0.6458\n",
      "Valid Trial [22/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 5.6638, Accuracy: 0.8590, F1 score: 0.6104, HS: 0.4740\n",
      "\n",
      "Train Trial [23/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.3276, Accuracy: 0.9233, F1 score: 0.7700, HS: 0.6445\n",
      "Valid Trial [23/200], LR: 0.00042187500000000005, W: 0.01, Avg Loss: 6.7632, Accuracy: 0.8624, F1 score: 0.6098, HS: 0.4626\n",
      "\n",
      "Train Trial [24/200], LR: 0.0004219, W: 0.01, Avg Loss: 0.3430, Accuracy: 0.9198, F1 score: 0.7621, HS: 0.6330\n",
      "Valid Trial [24/200], LR: 0.00031640625000000006, W: 0.01, Avg Loss: 5.4391, Accuracy: 0.8580, F1 score: 0.6087, HS: 0.4668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "w = 0.01\n",
    "\n",
    "# Early Stopping\n",
    "patience = 10\n",
    "bad_epochs = 0\n",
    "best_score = 0.0\n",
    "best_weights = None\n",
    "\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "# scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  score_train = alex_train(epoch, num_epochs, train_losses, learning_rate, w)\n",
    "  score_valid = alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "  print(\"\")\n",
    "\n",
    "  # Early Stopping\n",
    "  if score_valid > best_score:\n",
    "    bad_epochs = 0\n",
    "    best_epoch = epoch\n",
    "    best_score = score_valid\n",
    "    best_weights = alex_net.state_dict()\n",
    "  else:\n",
    "    bad_epochs += 1\n",
    "  \n",
    "  if bad_epochs == patience:\n",
    "    print(\"Out of patience!\")\n",
    "    print(f\"Best epoch: {best_epoch}\")\n",
    "    break\n",
    "\n",
    "if SAVE:\n",
    "    print(f\"Guardando mejor modelo en {model_path}\")\n",
    "    torch.save(best_weights, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# alerta sonora para cuando haya terminado de entrenar\n",
    "import winsound\n",
    "import time\n",
    "for i in range(0, 2):\n",
    "    duration = 500\n",
    "    freq = 1500\n",
    "    winsound.Beep(freq, duration)\n",
    "    time.sleep(0.5)\n",
    "    i+=1\n",
    "winsound.Beep(freq, 2000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_losses = []\n",
    "test_pred = []\n",
    "learning_rate = 0.0001\n",
    "w = 0.001\n",
    "\n",
    "# Reset Model\n",
    "alex_net = models.alexnet(pretrained=True)\n",
    "alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "alex_net.load_state_dict(torch.load(model_path))\n",
    "alex_net.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "alex_test(1, 1, test_pred, test_losses, learning_rate, w, show_images = 0)\n",
    "test_pred_df = pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234, 26)\n",
      "(234, 26)\n",
      "HS final: 0.5364061864061864\n"
     ]
    }
   ],
   "source": [
    "preds = test_pred[0][3].cpu().detach().numpy()\n",
    "for i in range(1, len(test_pred)):\n",
    "  pbi = test_pred[i][3].cpu().detach().numpy()\n",
    "  preds = np.concatenate((preds, pbi), axis=0)\n",
    "if SAVE:\n",
    "    save_df = pd.DataFrame(preds)\n",
    "    save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "    print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "    preds = pd.read_csv(os.path.join(output_dir, 'predictions.csv'), index_col=0)\n",
    "    preds = preds.values\n",
    "\n",
    "testval = pd.concat([labels_test, labels_val])\n",
    "testval = filter_dfs(testval, top_labels)\n",
    "\n",
    "print(preds.shape)\n",
    "print(testval.shape)\n",
    "hs_final = hamming_score(preds, testval.values)\n",
    "#f1_final = f1_score(preds, testval.values, average='macro')\n",
    "\n",
    "print(f\"HS final: {hs_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BATCH_SIZE': 124, 'optimizer': 'Adam', 'scheduler': 'ReduceLROnPlateau', 'criterion': 'BCEWithLogitsLoss', 'epochs': 200, 'best_epoch': 57, 'data_flags': 'blur_mtnblur_elastic1', 'use_pos_weights': True, 'pos_weights_factor': 1, 'NUM_LABELS': 26, 'use_testval': True, 'TH_TRAIN': 0.5, 'TH_VAL': 0.5, 'TH_TEST': 0.5, 'HS_FINAL': 0.47361280694614033, 'patience': 10}\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'optimizer': (type (optimizer).__name__),\n",
    "    'scheduler': (type (scheduler).__name__),\n",
    "    'criterion': (type (criterion).__name__),\n",
    "    'epochs': num_epochs,\n",
    "    'best_epoch': best_epoch,\n",
    "    'data_flags': data_flags,\n",
    "    'use_pos_weights': use_pos_weights,\n",
    "    'pos_weights_factor': pos_weights_factor,\n",
    "    'NUM_LABELS': NUM_LABELS,\n",
    "    'use_testval': use_testval,\n",
    "    'TH_TRAIN': TH_TRAIN,\n",
    "    'TH_VAL': TH_VAL,\n",
    "    'TH_TEST': TH_TEST,\n",
    "    'HS_FINAL': hs_final,\n",
    "    'patience': patience\n",
    "}\n",
    "\n",
    "if SAVE:\n",
    "    with open(os.path.join(output_dir, 'metadata.pickle'), 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'metadata.pickle'), 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "        print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
