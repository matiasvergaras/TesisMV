{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# Export DATA\n",
    "## Seminario de Tesis II, Primavera 2022 \n",
    "### Master of Data Science, Universidad de Chile.\n",
    "#### Supervisor: Prof. Benjamín Bustos, Prof. Iván Sipirán\n",
    "#### Author: Matías Vergara\n",
    "\n",
    "### References:\n",
    "- [SLEEC Homepage](http://manikvarma.org/code/SLEEC/download.html)\n",
    "- [SLEEC Paper: Sparse Local Embeddings for Extreme Multi-label Classification](https://papers.nips.cc/paper/2015/hash/35051070e572e47d2c26c241ab88307f-Abstract.html)\n",
    "- [The Emerging Trends of Multi-Label Learning](https://arxiv.org/abs/2011.11197)\n",
    "- [GitHub: C2AE Multilabel Classification](https://github.com/dhruvramani/C2AE-Multilabel-Classification)\n",
    "- 'Learning Deep Latent Spaces for Multi-Label Classfications' published in AAAI 2017\n",
    "\n",
    "Este notebook tiene por finalidad exportar un conjunto de datos (a seleccionar con las celdas de selección habituales) al formato requerido por implementaciones oficiales de los modelos presentados en el paper de Emerging Trends of Multi-Label Learning. Hasta el momento se ha experimentado con:\n",
    "- SLEEC, sin mayor éxito (requiere una instalación particular de MeX que viene con Matlab 2017b, al cual no logré acceder)\n",
    "- C2AE, con resultados mediocres (no hay aprendizaje).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder_path = 'drive/MyDrive/TesisMV/'\n",
    "except:\n",
    "    folder_path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Data treatment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.io import savemat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import multilabel_confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LABELS_IN_STUDY = 25 # top N labels will be exported to Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "USE_RN50 = False\n",
    "SUBCHAPTERS = False\n",
    "DS_FLAGS = ['base']\n",
    "    # 'ref': [invertX, invertY],\n",
    "    # 'rot': [rotate90, rotate180, rotate270],\n",
    "    # 'crop': [crop] * CROP_TIMES,\n",
    "    # 'blur': [blur],\n",
    "    # 'emboss': [emboss],\n",
    "    # 'randaug': [randaug],\n",
    "    # 'rain': [rain],\n",
    "    # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../features/resnet18_base/\n",
      "../labels/base/\n",
      "Features set encontrado en ../features/resnet18_base/\n",
      "Labels set encontrado en ../labels/base/\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "subchapter_str = 'subchapters/' if SUBCHAPTERS else ''\n",
    "patterns_path = folder_path + 'patterns/' + subchapter_str + data_flags + \"/\"\n",
    "labels_path = folder_path + 'labels/' +  subchapter_str + data_flags + \"/\"\n",
    "data_flags = f'resnet50_{data_flags}' if USE_RN50 else f'resnet18_{data_flags}'\n",
    "features_path = folder_path + \"features/\" + subchapter_str + data_flags + '/'\n",
    "\n",
    "#rn = 18\n",
    "#ep = 65\n",
    "#labels_path = folder_path + 'labels/' +  subchapter_str + data_flags + \"/\"\n",
    "#data_flags = f'resnet50_{data_flags}_e{ep}' if USE_RN50 else f'resnet18_{data_flags}_e{ep}'\n",
    "#features_path = folder_path + f\"features/resnet{rn}_blur_each5/resnet{rn}_blur_e{ep}/\"\n",
    "\n",
    "print(features_path)\n",
    "print(labels_path)\n",
    "if not (os.path.isdir(features_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation {}\".format(\n",
    "        (os.path.isdir(features_path), os.path.isdir(labels_path))))\n",
    "print(\"Features set encontrado en {}\".format(features_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_filename = \"augmented_train_df.json\"\n",
    "val_filename = \"val_df.json\"\n",
    "test_filename = \"test_df.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_train = pd.read_json(features_path + train_filename, orient='index').sort_index()\n",
    "features_val = pd.read_json(features_path + val_filename, orient='index').sort_index()\n",
    "features_test = pd.read_json(features_path + test_filename, orient='index').sort_index()\n",
    "\n",
    "labels_train = pd.read_json(labels_path + train_filename, orient='index').sort_index()\n",
    "labels_val = pd.read_json(labels_path + val_filename, orient='index').sort_index()\n",
    "labels_test = pd.read_json(labels_path + test_filename, orient='index').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m_jvs\\AppData\\Local\\Temp\\ipykernel_6952\\2011289146.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  features_test_val = pd.DataFrame.append(features_test, features_val )\n",
      "C:\\Users\\m_jvs\\AppData\\Local\\Temp\\ipykernel_6952\\2011289146.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  labels_test_val = pd.DataFrame.append(labels_test, labels_val)\n"
     ]
    }
   ],
   "source": [
    "features_test_val = pd.DataFrame.append(features_test, features_val )\n",
    "labels_test_val = pd.DataFrame.append(labels_test, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_labels(labels_df, freq, number_labels = None):\n",
    "  \"\"\"Filters a label dataframe based on labels frequency (number of events)\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels\n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: filtered labels dataframe\n",
    "\n",
    "  \"\"\"\n",
    "  top_labels = None\n",
    "\n",
    "  if not number_labels:\n",
    "    filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > freq]\n",
    "    top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "    return top_labels, 0\n",
    "\n",
    "  if number_labels:\n",
    "      filtered_labels = 0\n",
    "      pivot = freq\n",
    "      while filtered_labels != number_labels:\n",
    "              filtered_df = labels_df.loc[:, labels_df.sum(axis=0) > pivot]\n",
    "              top_labels = filtered_df.sum().sort_values(ascending=False)\n",
    "              print(pivot, len(top_labels.values))\n",
    "              if len(top_labels.values) > number_labels:\n",
    "                  pivot += 1\n",
    "              elif len(top_labels.values) < number_labels:\n",
    "                  pivot -= 1\n",
    "              else:\n",
    "                  print(\"Aplicando threshold {} para trabajar con {} labels\".format(pivot, len(top_labels.values)))\n",
    "                  return top_labels, pivot\n",
    "\n",
    "def filter_dfs(df, top_labels_df):\n",
    "  df = df[df.columns.intersection(top_labels_df.index)]\n",
    "  return df\n",
    "\n",
    "def combine_dfs(labels_df, top_labels_df, features_df):\n",
    "  \"\"\"Combine labels dataframe with features dataframe based on index (patterns names)\n",
    "     keeping only the most frequent labels.\n",
    "\n",
    "    Parameters:\n",
    "    labels_df (DataFrame): dataframe of labels, with patterns name as index\n",
    "    top_labels_df (DataFrame): a 1D dataframe with the name of the most freq. labels, as\n",
    "    the outcome of filter_labels() function\n",
    "    features_df (DataFrame): dataframe of features, with patterns name as index\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: combined labels + features dataframe, merged on index\n",
    "\n",
    "  \"\"\"\n",
    "  assert len(labels_df) == len(features_df)\n",
    "  labels_df = labels_df[labels_df.columns.intersection(top_labels_df.index)]\n",
    "  final_df = pd.merge(labels_df,\n",
    "                      features_df,\n",
    "                      left_index=True, right_index=True)\n",
    "  return final_df\n",
    "\n",
    "def split_data(final_df, top_labels_df, test_size):\n",
    "  \"\"\"Splits the data in train and test.\n",
    "\n",
    "    Parameters:\n",
    "    final_df (DataFrame): outcome of combine_dfs.\n",
    "    top_labels_df (DataFrame): dataframe of most freq. labels. Necessary to\n",
    "    know at which column the labels (Y) ends and the features (X) starts\n",
    "    freq (int): threshold frequency. Labels with a lower value will be filtered.\n",
    "    test_size (float): proportion test/(test+train).\n",
    "\n",
    "    Returns:\n",
    "    (np.array, np.array, np.array, np.array): X train, X test, Y train, Y test\n",
    "\n",
    "  \"\"\"\n",
    "  X = np.array(final_df.iloc[:, len(final_df):], dtype=float)\n",
    "  Y = np.array(final_df.iloc[:, 0:len(final_df)], dtype=float)\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,random_state=42)\n",
    "  return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 31\n",
      "21 31\n",
      "22 29\n",
      "23 28\n",
      "24 28\n",
      "25 25\n",
      "Aplicando threshold 25 para trabajar con 25 labels\n"
     ]
    }
   ],
   "source": [
    "top_labels_df, pivot = filter_labels(labels_train, 20, LABELS_IN_STUDY) # for example\n",
    "X_train = features_train\n",
    "X_test = features_test_val   # labels_test_val since val examples are unrecognized to multilabel algorithms\n",
    "Y_train = filter_dfs(labels_train, top_labels_df) # reduce labels to most freq\n",
    "Y_test = filter_dfs(labels_test_val, top_labels_df) # in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(542, 512)\n",
      "(542, 25)\n",
      "(234, 512)\n",
      "(234, 25)\n"
     ]
    }
   ],
   "source": [
    "X = X_train.to_numpy()\n",
    "Xt = X_test.to_numpy()\n",
    "Y = Y_train.to_numpy()\n",
    "Yt = Y_test.to_numpy()\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Xt.shape)\n",
    "print(Yt.shape)\n",
    "# TO SLEEC\n",
    "#savemat(f\"{data_flags}.mat\", dict(X=X, Xt=Xt, Y=Y, Yt=Yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TO C2AE\n",
    "import pickle\n",
    "with open('kunisch-train-features.pkl', 'wb') as f: pickle.dump(X, f)\n",
    "with open('kunisch-train-labels.pkl', 'wb') as f: pickle.dump(Y, f)\n",
    "with open('kunisch-test-features.pkl', 'wb') as f: pickle.dump(Xt, f)\n",
    "with open('kunisch-test-labels.pkl', 'wb') as f: pickle.dump(Yt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "|    **Modelo**   \t| **Tradicional** \t| **C2AE** \t|\n",
    "|:---------------:\t|:---------------:\t|:--------:\t|\n",
    "| Precision Micro \t|       0.08      \t|   0.14   \t|\n",
    "| Precision Macro \t|       0.08      \t|   0.07   \t|\n",
    "|   Recall Micro  \t|       0.62      \t|   0.65   \t|\n",
    "|   Recall Macro  \t|       0.41      \t|   0.31   \t|\n",
    "|     F1 Micro    \t|       0.15      \t|   0.24   \t|\n",
    "|     F1 Macro    \t|       0.12      \t|   0.11   \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
