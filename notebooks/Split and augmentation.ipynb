{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzn7gj5P3DXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Augmentation over Kunisch Patterns. \n",
    "## Seminario de Tesis I, Primavera 2022 \n",
    "### MDS Program. University of Chile.\n",
    "#### Supervisor: Prof. Benjamín Bustos, Prof. Iván Sipirán\n",
    "#### Author: Matías Vergara\n",
    "\n",
    "Performs data augmentation on patterns through the application of linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woMl7NKb3LyB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bDe9EneU2rIG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import imgaug.augmenters as aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRTWM7ng3lwE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dKnmKrA63npf",
    "outputId": "1c315239-a236-44a4-9349-15ef4ac5db02",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder_path = 'drive/MyDrive/TesisMV/'\n",
    "except:\n",
    "    folder_path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is enough to select ds flags and number of crops (this one will have effect depending on flags) and then run the rest of the cells. This way, two folders will be created: a labels one and a patterns one. Both of them will be named after the selected flags, separed by \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = ['randaug']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 2\n",
    "RANDOM_TIMES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_flags = '_'.join(DS_FLAGS)\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS and len(DS_FLAGS) > 1:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        data_flags = '_'.join(COPY_FLAGS) + '_' + t + str(CROP_TIMES)\n",
    "    elif t in DS_FLAGS and len(DS_FLAGS) == 1:\n",
    "        data_flags = t + str(CROP_TIMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function randaug at 0x00000231BF5EC280>, <function randaug at 0x00000231BF5EC280>]\n",
      "[<function randaug at 0x00000231BF5EC280>, <function randaug at 0x00000231BF5EC280>]\n",
      "[<function randaug at 0x00000231BF5EC280>, <function randaug at 0x00000231BF5EC280>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<function __main__.randaug(path)>, <function __main__.randaug(path)>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate90(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"90\", rotated)\n",
    "    return rotated, \"rot90\"\n",
    "\n",
    "\n",
    "def rotate180(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    # cv2.imshow(\"180\", rotated)\n",
    "    return rotated, \"rot180\"\n",
    "\n",
    "\n",
    "def rotate270(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    rotated = cv2.rotate(rotated, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"270\", rotated)\n",
    "    return rotated, \"rot270\"\n",
    "\n",
    "\n",
    "def invertX(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    # cv2.imshow(\"flipX\", flipped)\n",
    "    return flipped, \"invX\"\n",
    "\n",
    "\n",
    "def invertY(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 0)\n",
    "    # cv2.imshow(\"flipY\", flipped)\n",
    "    return flipped, \"invY\"\n",
    "\n",
    "\n",
    "def crop(path, min_width = 1/2, min_height= 1/2, max_width = 1/1.1,\n",
    "         max_height = 1/1.1):\n",
    "    image = cv2.imread(path)\n",
    "    height, width = image.shape[0], image.shape[1] # Caution: there are images in RGB and GS\n",
    "    min_width = math.ceil(width * min_width)\n",
    "    min_height = math.ceil(height * min_height)\n",
    "    max_width = math.ceil(width * max_width)\n",
    "    max_height = math.ceil(height * max_height)\n",
    "    x1 = random.randint(0, width - min_width)\n",
    "    w = random.randint(min_width, width - x1)\n",
    "    y1 = random.randint(0, height - min_height)\n",
    "    h = random.randint(min_height, height - y1)\n",
    "    crop = image[y1:y1+h, x1:x1+w]\n",
    "    return crop, \"crop\"\n",
    "\n",
    "def blur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.AverageBlur(k=(4, 11))(image=image)\n",
    "    return image_aug, \"blur\"\n",
    "\n",
    "def emboss(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))(image = image)\n",
    "    return image_aug, \"embs\"\n",
    "\n",
    "def elastic(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.PiecewiseAffine(scale=(0.03, 0.075))(image = image)\n",
    "    return image_aug, \"elst\"\n",
    "\n",
    "def randaug(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.RandAugment(m=(0, 9))(image = image)\n",
    "    return image_aug, \"rand\"\n",
    "\n",
    "def snow(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Snowflakes(flake_size=(0.6, 0.5), speed=(0.2, 0.5))(image = image)\n",
    "    return image_aug, \"snow\"\n",
    "\n",
    "\n",
    "def rain(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Rain(speed=(0.1, 0.5))(image = image)\n",
    "    return image_aug, \"rain\"\n",
    "\n",
    "\n",
    "def apply_transformations(pin, pout, transformations):\n",
    "    # ../patterns/originals/84e/84e.png\n",
    "    new_names = []\n",
    "    i = 0\n",
    "    for transformation in transformations:\n",
    "        result, transf_name = transformation(pin)\n",
    "        if transf_name in MULTIPLE_TRANSF: # special treatment for crops and randoms\n",
    "          transf_name += str(i)\n",
    "          i+=1\n",
    "        path_els = pin.split(\"/\")\n",
    "        obj_name = path_els[3] + \"_\" + transf_name\n",
    "        filename = obj_name + \".png\"\n",
    "        os.makedirs(pout, exist_ok = True)\n",
    "        cv2.imwrite(pout + filename, result)\n",
    "        new_names.append(obj_name)\n",
    "    return new_names\n",
    "\n",
    "# Select data augmentation functions based on data flags\n",
    "\n",
    "MAP_FLAGS = {'ref': [invertX, invertY],\n",
    "             'rot': [rotate90, rotate180, rotate270],\n",
    "             'crop': [crop] * CROP_TIMES,\n",
    "             'blur': [blur],\n",
    "             'emboss': [emboss],\n",
    "             'randaug': [randaug],\n",
    "             'rain': [rain],\n",
    "             'elastic': [elastic]\n",
    "             # snow is not working properly\n",
    "             }\n",
    "\n",
    "ALLOWED_TRANSFORMATIONS = []\n",
    "for f in DS_FLAGS:\n",
    "    ALLOWED_TRANSFORMATIONS += MAP_FLAGS[f]\n",
    "HOR_TRANSFORMATIONS = [invertX, rotate180, blur, rain, emboss, elastic] + [crop] * CROP_TIMES + [randaug] * RANDOM_TIMES\n",
    "VER_TRANSFORMATIONS = [invertY, rotate180, blur, rain, emboss, elastic] + [crop] * CROP_TIMES + [randaug] * RANDOM_TIMES\n",
    "COMMON_TRANSFORMATIONS = [invertX, invertY, rotate90, rotate180, rotate270,\n",
    "                          blur, rain, emboss, elastic\n",
    "                         ] + [crop] * CROP_TIMES + [randaug] * RANDOM_TIMES\n",
    "\n",
    "def mergeTransformations(flags, map_flags, trans_list):\n",
    "    for k, v in map_flags.items():\n",
    "        if k not in flags:\n",
    "            for el in v:\n",
    "                if el in trans_list:\n",
    "                    trans_list.remove(el)\n",
    "    print(trans_list)\n",
    "    return trans_list\n",
    "\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, HOR_TRANSFORMATIONS)\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, VER_TRANSFORMATIONS)\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, COMMON_TRANSFORMATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>subchapter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1a</th>\n",
       "      <td>strokes and lines</td>\n",
       "      <td>strokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1b</th>\n",
       "      <td>strokes and lines</td>\n",
       "      <td>strokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1c</th>\n",
       "      <td>strokes and lines</td>\n",
       "      <td>strokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1d</th>\n",
       "      <td>strokes and lines</td>\n",
       "      <td>strokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e</th>\n",
       "      <td>strokes and lines</td>\n",
       "      <td>strokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96e</th>\n",
       "      <td>pictographics</td>\n",
       "      <td>trees and animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96f</th>\n",
       "      <td>pictographics</td>\n",
       "      <td>trees and animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96g</th>\n",
       "      <td>pictographics</td>\n",
       "      <td>trees and animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96h</th>\n",
       "      <td>pictographics</td>\n",
       "      <td>trees and animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96i</th>\n",
       "      <td>pictographics</td>\n",
       "      <td>trees and animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>776 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               chapter         subchapter\n",
       "1a   strokes and lines            strokes\n",
       "1b   strokes and lines            strokes\n",
       "1c   strokes and lines            strokes\n",
       "1d   strokes and lines            strokes\n",
       "1e   strokes and lines            strokes\n",
       "..                 ...                ...\n",
       "96e      pictographics  trees and animals\n",
       "96f      pictographics  trees and animals\n",
       "96g      pictographics  trees and animals\n",
       "96h      pictographics  trees and animals\n",
       "96i      pictographics  trees and animals\n",
       "\n",
       "[776 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patterns for training: 620\n",
      "Patterns for validation: 78\n",
      "Patterns for testing: 78\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(folder_path + \"labels/normalized_df.json\", orient='index', encoding='latin-1')\n",
    "classes = pd.read_csv(folder_path + \"labels/class_labels.csv\", index_col=0)\n",
    "\n",
    "display(classes)\n",
    "\n",
    "index = df.index.values\n",
    "colnames = df.columns\n",
    "\n",
    "valNumber = math.ceil(0.1 * len(index))\n",
    "testNumber = math.ceil(0.1 * len(index))\n",
    "trainNumber = len(index) - valNumber - testNumber\n",
    "\n",
    "print(\"Patterns for training: {}\".format(trainNumber))\n",
    "print(\"Patterns for validation: {}\".format(valNumber))\n",
    "print(\"Patterns for testing: {}\".format(testNumber))\n",
    "\n",
    "\n",
    "random.shuffle(index)\n",
    "\n",
    "elem_train = index[:trainNumber]\n",
    "elem_val = index[trainNumber:trainNumber+valNumber]\n",
    "elem_test = index[trainNumber+valNumber:]\n",
    "\n",
    "assert (valNumber + testNumber + trainNumber) == len(index)\n",
    "\n",
    "# print(elem_train)\n",
    "# print(elem_val)\n",
    "# print(elem_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Augmentation\n",
    "(Only over training set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_entries = {}\n",
    "\n",
    "for pattern in elem_train: # only training set\n",
    "    labels = df.loc[[pattern]]\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/train/' + lbl_class + \"/\"\n",
    "    is_hor = labels['horizontal'].values[0]\n",
    "    is_ver = labels['vertical'].values[0]\n",
    "    if is_hor and is_ver:\n",
    "        pass\n",
    "    if is_hor and not is_ver:\n",
    "        new_names = apply_transformations(path_in, path_out, HOR_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    elif is_ver and not is_hor:\n",
    "        new_names = apply_transformations(path_in, path_out, VER_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    else: #if not is_hor and not is_ver:\n",
    "        new_names = apply_transformations(path_in, path_out, COMMON_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    for name in new_names:\n",
    "        new_entries[name] = labels\n",
    "    # add the base pattern to the folder\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "for pattern in elem_val:\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/val/' + lbl_class + \"/\"\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "for pattern in elem_test:\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/test/' + lbl_class + \"/\"\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "# agregar todas las entradas de elem_train a new_entries, y crear \n",
    "# el dataset \"augmented_train_df.json\"\n",
    "\n",
    "for p in elem_train:\n",
    "  labels = df.loc[p]\n",
    "  new_entries[p] = labels.values\n",
    "\n",
    "labels_output = folder_path + \"labels/\" + data_flags + \"/\"\n",
    "\n",
    "os.makedirs(labels_output, exist_ok = True)\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(new_entries, columns=colnames, orient='index')\n",
    "df_train.to_json(labels_output + \"augmented_train_df.json\", orient='index')\n",
    "\n",
    "# agregar todas las entradas de elem_val a val_entries, y crear \n",
    "# el dataset \"val_df.json\"\n",
    "val_entries = {}\n",
    "for p in elem_val:\n",
    "  labels = df.loc[p]\n",
    "  val_entries[p] = labels.values\n",
    "\n",
    "df_val = pd.DataFrame.from_dict(val_entries, columns=colnames, orient='index')\n",
    "df_val.to_json(labels_output + \"val_df.json\", orient='index')\n",
    "\n",
    "# agregar todas las entradas de elem_test a test_entries, y crear\n",
    "# el dataset \"test_df.json\"\n",
    "test_entries = {}\n",
    "for p in elem_test:\n",
    "  labels = df.loc[p]\n",
    "  test_entries[p] = labels.values\n",
    "\n",
    "df_test = pd.DataFrame.from_dict(test_entries, columns=colnames, orient='index')\n",
    "df_test.to_json(labels_output + \"test_df.json\", orient='index')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "woMl7NKb3LyB",
    "IRTWM7ng3lwE",
    "lVaAM8Qw3Oo-",
    "boCmALkT3gro",
    "6IsXXq4cAlQp"
   ],
   "name": "Split and augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}