{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzn7gj5P3DXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Augmentation over Kunisch Patterns. \n",
    "## Seminario de Tesis I, Primavera 2022 \n",
    "### MDS Program. University of Chile.\n",
    "#### Supervisor: Prof. Benjamín Bustos, Prof. Iván Sipirán\n",
    "#### Author: Matías Vergara\n",
    "\n",
    "Performs data augmentation on patterns through the application of linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woMl7NKb3LyB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "bDe9EneU2rIG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRTWM7ng3lwE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dKnmKrA63npf",
    "outputId": "1c315239-a236-44a4-9349-15ef4ac5db02",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder_path = 'drive/MyDrive/TesisMV/'\n",
    "except:\n",
    "    folder_path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define flags"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is enough to select ds flags and number of crops (this one will have effect depending on flags) and then run the rest of the cells. This way, two folders will be created: a labels one and a patterns one. Both of them will be named after the selected flags, separed by \"_\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DS_FLAGS = ['ref'] # ref, rot, crop, gaus\n",
    "CROP_TIMES = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_flags = '_'.join(DS_FLAGS)\n",
    "\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "if 'crop' in DS_FLAGS and len(DS_FLAGS) > 1:\n",
    "    COPY_FLAGS.remove('crop')\n",
    "    data_flags = '_'.join(COPY_FLAGS) + '_crop' + str(CROP_TIMES)\n",
    "elif 'crop' in DS_FLAGS and len(DS_FLAGS) == 1:\n",
    "    data_flags = 'crop' + str(CROP_TIMES)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rotate90(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"90\", rotated)\n",
    "    return(rotated, \"rot90\")\n",
    "\n",
    "\n",
    "def rotate180(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    # cv2.imshow(\"180\", rotated)\n",
    "    return(rotated, \"rot180\")\n",
    "\n",
    "\n",
    "def rotate270(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.cv2.ROTATE_180)\n",
    "    rotated = cv2.rotate(rotated, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"270\", rotated)\n",
    "    return (rotated, \"rot270\")\n",
    "\n",
    "\n",
    "def invertX(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    # cv2.imshow(\"flipX\", flipped)\n",
    "    return(flipped, \"invX\")\n",
    "\n",
    "\n",
    "def invertY(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 0)\n",
    "    # cv2.imshow(\"flipY\", flipped)\n",
    "    return(flipped, \"invY\")\n",
    "\n",
    "\n",
    "def crop(path, min_width = 1/2, min_height= 1/2, max_width = 1/1.1,\n",
    "         max_height = 1/1.1):\n",
    "    image = cv2.imread(path)\n",
    "    height, width = image.shape[0], image.shape[1] # Caution: there are images in RGB and GS\n",
    "    min_width = math.ceil(width * min_width)\n",
    "    min_height = math.ceil(height * min_height)\n",
    "    max_width = math.ceil(width * max_width)\n",
    "    max_height = math.ceil(height * max_height)\n",
    "    x1 = random.randint(0, width - min_width)\n",
    "    w = random.randint(min_width, width - x1)\n",
    "    y1 = random.randint(0, height - min_height)\n",
    "    h = random.randint(min_height, height - y1)\n",
    "    crop = image[y1:y1+h, x1:x1+w]\n",
    "    return (crop, \"crop\")\n",
    "\n",
    "def apply_transformations(pin, pout, transformations):\n",
    "    # ../patterns/originals/84e/84e.png\n",
    "    new_names = []\n",
    "    i = 0\n",
    "    for transformation in transformations:\n",
    "        result, transf_name = transformation(pin)\n",
    "        if transf_name == \"crop\": # special treatment for crops\n",
    "          transf_name += str(i)\n",
    "          i+=1\n",
    "        path_els = pin.split(\"/\")\n",
    "        obj_name = path_els[3] + \"_\" + transf_name\n",
    "        filename = obj_name + \".png\"\n",
    "        os.makedirs(pout, exist_ok = True)\n",
    "        cv2.imwrite(pout + filename, result)\n",
    "        new_names.append(obj_name)\n",
    "    return new_names\n",
    "\n",
    "# Select data augmentation functions based on data flags\n",
    "\n",
    "MAP_FLAGS = {'ref': [invertX, invertY],\n",
    "             'rot': [rotate90, rotate180, rotate270],\n",
    "             'crop': [crop] * CROP_TIMES,\n",
    "             'gaus': []\n",
    "             }\n",
    "\n",
    "ALLOWED_TRANSFORMATIONS = []\n",
    "for f in DS_FLAGS:\n",
    "    ALLOWED_TRANSFORMATIONS += MAP_FLAGS[f]\n",
    "HOR_TRANSFORMATIONS = [invertX, rotate180] + [crop] * CROP_TIMES\n",
    "VER_TRANSFORMATIONS = [invertY, rotate180] + [crop] * CROP_TIMES\n",
    "COMMON_TRANSFORMATIONS = [invertX, invertY, rotate90, rotate180, rotate270] + [crop] * CROP_TIMES\n",
    "\n",
    "for t in HOR_TRANSFORMATIONS:\n",
    "    if t not in ALLOWED_TRANSFORMATIONS:\n",
    "        HOR_TRANSFORMATIONS.remove(t)\n",
    "print(HOR_TRANSFORMATIONS)\n",
    "\n",
    "for t in VER_TRANSFORMATIONS:\n",
    "    if t not in ALLOWED_TRANSFORMATIONS:\n",
    "        VER_TRANSFORMATIONS.remove(t)\n",
    "print(VER_TRANSFORMATIONS)\n",
    "\n",
    "for t in COMMON_TRANSFORMATIONS:\n",
    "    if t not in ALLOWED_TRANSFORMATIONS:\n",
    "        COMMON_TRANSFORMATIONS.remove(t)\n",
    "print(COMMON_TRANSFORMATIONS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset splitting\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_json(folder_path + \"labels/normalized_df.json\", orient='index', encoding='latin-1')\n",
    "classes = pd.read_csv(folder_path + \"labels/class_labels.csv\", index_col=0)\n",
    "\n",
    "display(classes)\n",
    "\n",
    "index = df.index.values\n",
    "colnames = df.columns\n",
    "\n",
    "valNumber = math.ceil(0.1 * len(index))\n",
    "testNumber = math.ceil(0.1 * len(index))\n",
    "trainNumber = len(index) - valNumber - testNumber\n",
    "\n",
    "print(\"Patterns for training: {}\".format(trainNumber))\n",
    "print(\"Patterns for validation: {}\".format(valNumber))\n",
    "print(\"Patterns for testing: {}\".format(testNumber))\n",
    "\n",
    "\n",
    "random.shuffle(index)\n",
    "\n",
    "elem_train = index[:trainNumber]\n",
    "elem_val = index[trainNumber:trainNumber+valNumber]\n",
    "elem_test = index[trainNumber+valNumber:]\n",
    "\n",
    "assert (valNumber + testNumber + trainNumber) == len(index)\n",
    "\n",
    "# print(elem_train)\n",
    "# print(elem_val)\n",
    "# print(elem_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation\n",
    "(Only over training set)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_entries = {}\n",
    "\n",
    "for pattern in elem_train: # only training set\n",
    "    labels = df.loc[[pattern]]\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/train/' + lbl_class + \"/\"\n",
    "    is_hor = labels['horizontal'].values[0]\n",
    "    is_ver = labels['vertical'].values[0]\n",
    "    if is_hor and is_ver:\n",
    "        pass\n",
    "    if is_hor and not is_ver:\n",
    "        new_names = apply_transformations(path_in, path_out, HOR_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    elif is_ver and not is_hor:\n",
    "        new_names = apply_transformations(path_in, path_out, VER_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    else: #if not is_hor and not is_ver:\n",
    "        new_names = apply_transformations(path_in, path_out, COMMON_TRANSFORMATIONS)\n",
    "        labels = df.loc[[pattern]].values[0]\n",
    "    for name in new_names:\n",
    "        new_entries[name] = labels\n",
    "    # add the base pattern to the folder\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "for pattern in elem_val:\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/val/' + lbl_class + \"/\"\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "for pattern in elem_test:\n",
    "    lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "    path_in = folder_path + \"patterns/originals/\" + pattern + \"/\" + pattern + \".png\"\n",
    "    path_out = folder_path + 'patterns/' + data_flags + '/test/' + lbl_class + \"/\"\n",
    "    os.makedirs(path_out, exist_ok = True)\n",
    "    shutil.copy(path_in, path_out)\n",
    "\n",
    "# agregar todas las entradas de elem_train a new_entries, y crear \n",
    "# el dataset \"augmented_train_df.json\"\n",
    "\n",
    "for p in elem_train:\n",
    "  labels = df.loc[p]\n",
    "  new_entries[p] = labels.values\n",
    "\n",
    "labels_output = folder_path + \"labels/\" + data_flags + \"/\"\n",
    "\n",
    "os.makedirs(labels_output, exist_ok = True)\n",
    "\n",
    "df_train = pd.DataFrame.from_dict(new_entries, columns=colnames, orient='index')\n",
    "df_train.to_json(labels_output + \"augmented_train_df.json\", orient='index')\n",
    "\n",
    "# agregar todas las entradas de elem_val a val_entries, y crear \n",
    "# el dataset \"val_df.json\"\n",
    "val_entries = {}\n",
    "for p in elem_val:\n",
    "  labels = df.loc[p]\n",
    "  val_entries[p] = labels.values\n",
    "\n",
    "df_val = pd.DataFrame.from_dict(val_entries, columns=colnames, orient='index')\n",
    "df_val.to_json(labels_output + \"val_df.json\", orient='index')\n",
    "\n",
    "# agregar todas las entradas de elem_test a test_entries, y crear\n",
    "# el dataset \"test_df.json\"\n",
    "test_entries = {}\n",
    "for p in elem_test:\n",
    "  labels = df.loc[p]\n",
    "  test_entries[p] = labels.values\n",
    "\n",
    "df_test = pd.DataFrame.from_dict(test_entries, columns=colnames, orient='index')\n",
    "df_test.to_json(labels_output + \"test_df.json\", orient='index')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "woMl7NKb3LyB",
    "IRTWM7ng3lwE",
    "lVaAM8Qw3Oo-",
    "boCmALkT3gro",
    "6IsXXq4cAlQp"
   ],
   "name": "Split and augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}