{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDmhP_idRXT4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN Multilabeling through AlexNet\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Montando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '../content/gdrive/MyDrive'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NUM_LABELS = 26\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "TH_TRAIN = 0.5\n",
    "TH_VAL = 0.5\n",
    "TH_TEST = 0.5\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = False\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\0\n",
      "--Labels set encontrado en ../labels\\base\\0\n",
      "\n",
      "Fold  1\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\1\n",
      "--Labels set encontrado en ../labels\\base\\1\n",
      "\n",
      "Fold  2\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\2\n",
      "--Labels set encontrado en ../labels\\base\\2\n",
      "\n",
      "Fold  3\n",
      "Nombre del experimento: 26L_weighted_1\n",
      "--Pattern set encontrado en ../patterns\\base\\3\n",
      "--Labels set encontrado en ../labels\\base\\3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "Kfolds = {}\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    patterns_dir = os.path.join(root_dir, 'patterns', data_flags, str(i))\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "\n",
    "    if not (os.path.isdir(patterns_dir) and os.path.isdir(labels_dir)):\n",
    "        print(patterns_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    weights_str = str(pos_weights_factor)\n",
    "    weights_str = weights_str.replace('.','_')\n",
    "    exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "     \n",
    "    output_dir = os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name, str(i))\n",
    "    model_dir = os.path.join(root_dir, \"models\", \"alexnet\", data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "\n",
    "    Kfolds[i] = {\n",
    "        'patterns_dir': patterns_dir,\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    \n",
    "    print(\"--Pattern set encontrado en {}\".format(patterns_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive_weights(labels, factor=1):                        \n",
    "    total = labels.values.sum()\n",
    "    weights = [0.] * len(labels)\n",
    "    for i, label in enumerate(labels):\n",
    "      weights[i] = total/(factor * labels[i])\n",
    "    return weights\n",
    "\n",
    "# images_dir=os.path.join(root_dir, 'patterns', data_flags, 'train'),\n",
    "# labels_file=os.path.join(root_dir, 'labels', data_flags, 'augmented_train_df.json'),\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, images_dir, labels_file, transform, top_labels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text_file(string): path to text file\n",
    "        root_dir(string): directory with all train images\n",
    "    \"\"\"\n",
    "    self.pruner = KunischPruner(len(top_labels))\n",
    "    self.pruner.set_top_labels(top_labels)\n",
    "    labels = pd.read_json(labels_file, orient='index')\n",
    "    self.labels_frame = self.pruner.filter_df(labels)\n",
    "    self.num_labels = len(top_labels)\n",
    "    self.images_dir = images_dir\n",
    "    self.labels_file = labels_file\n",
    "    self.transform = transform\n",
    "    self.flags = data_flags\n",
    "    self.top_labels = top_labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "    img_name = None\n",
    "    for chapter in os.listdir(self.images_dir):\n",
    "      if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "        img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "        break\n",
    "    if img_name is None:\n",
    "      raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "    image = Image.open(img_name)\n",
    "    image = image.convert('RGB')\n",
    "    image = self.transform(image)\n",
    "    labels = self.labels_frame.iloc[idx].values\n",
    "    labels = np.array(labels)\n",
    "    labels = torch.from_numpy(labels.astype('int'))\n",
    "    #print(img_id, img_name, self.labels_frame.iloc[idx], self.labels_frame.iloc[idx].values, labels)\n",
    "    sample = {'image': image, 'labels': labels, 'paths': img_name}\n",
    "    return sample\n",
    "\n",
    "def hamming_score(y_true, y_pred):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set(np.where(y_true[i])[0])\n",
    "        set_pred = set(np.where(y_pred[i])[0])\n",
    "        # print('\\nset_true: {0}'.format(set_true))\n",
    "        # print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred)) / \\\n",
    "                    float(len(set_true.union(set_pred)))\n",
    "        # print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return round(np.mean(acc_list), 4)\n",
    "\n",
    "\n",
    "# Define the function for training, validation, and test\n",
    "def alex_train(epoch, num_epochs, train_losses, learning_rate, w):\n",
    "  alex_net.train()\n",
    "  train_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  for i, sample_batched in enumerate(kunischTrainLoader, 1):\n",
    "      inputs = sample_batched['image'].to(device)\n",
    "      labels = sample_batched['labels'].to(device)\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = alex_net(inputs)\n",
    "      loss = criterion(outputs.float(), labels.float())\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      train_loss += loss.item()\n",
    "      pred = (torch.sigmoid(outputs).data > TH_TRAIN).int()\n",
    "      # print(pred)\n",
    "      labels = labels.int()\n",
    "      # print(labels)\n",
    "      preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "      labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "    \n",
    "      TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "      TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "      FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "      FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "      #print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP, TN, FP, FN))\n",
    "  \n",
    "\n",
    "  TP = TP.cpu().numpy()\n",
    "  TN = TN.cpu().numpy()\n",
    "  FP = FP.cpu().numpy()\n",
    "  FN = FN.cpu().numpy()\n",
    "\n",
    "  accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "  precision = TP / (TP + FP)\n",
    "  recall = TP / (TP + FN)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "  train_loss = train_loss / len(kunischTrainLoader.dataset) * BATCH_SIZE\n",
    "  hs = hamming_score(preds_total, labels_total)\n",
    "  train_losses.append([epoch, learning_rate, w, train_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "\n",
    "  # print statistics\n",
    "  print('Train Trial [{}/{}], LR: {:.4g}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "        .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, train_loss, accuracy, f1_score, hs))\n",
    "  return hs\n",
    "\n",
    "def alex_valid(epoch, num_epochs, valid_losses, learning_rate, w):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  valid_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader, 1):\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          outputs = alex_net(inputs)\n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          valid_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_VAL).int()\n",
    "          labels = labels.int()\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "        \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "    \n",
    "      scheduler.step(hs)\n",
    "\n",
    "      valid_loss = valid_loss / len(kunischValidationLoader.dataset) * BATCH_SIZE  # 1024 is the batch size\n",
    "      valid_losses.append(\n",
    "          [epoch, learning_rate, w, valid_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Valid Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, valid_loss, accuracy, f1_score, hs))\n",
    "      return hs\n",
    "\n",
    "def alex_test(epoch, num_epochs, pred_array, test_losses, learning_rate, w, show_images=1):\n",
    "  # Have our model in evaluation mode\n",
    "  alex_net.eval()\n",
    "  # Set losses and Correct labels to zero\n",
    "  test_loss = 0\n",
    "  TN = 0\n",
    "  TP = 0\n",
    "  FP = 0\n",
    "  FN = 0\n",
    "  preds_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "  labels_total = np.empty((1, NUM_LABELS), dtype=int)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "      for i,sample_batched in enumerate(kunischTestLoader, 1):\n",
    "          print(\"CURRENT BATCH SIZE: \", BATCH_SIZE)\n",
    "          inputs = sample_batched['image'].to(device)\n",
    "          labels = sample_batched['labels'].to(device)\n",
    "          paths = sample_batched['paths']\n",
    "          outputs = alex_net(inputs)\n",
    "          \n",
    "          loss = criterion(outputs.float(), labels.float())\n",
    "          test_loss += loss.item()\n",
    "          pred = (torch.sigmoid(outputs).data > TH_TEST).int()\n",
    "          # print(pred)\n",
    "          labels = labels.int()\n",
    "          # print(labels)\n",
    "          pred_array.append([paths, test_loss, labels, pred])\n",
    "          preds_total = np.concatenate((preds_total, pred.cpu()), axis=0)\n",
    "          labels_total = np.concatenate((labels_total, labels.cpu()), axis=0)\n",
    "          \n",
    "          for j in range(0, min(BATCH_SIZE, show_images)): # j itera sobre ejemplos\n",
    "              print(f\"Mostrando imagen {j} del batch {i}\")\n",
    "              img = np.transpose(sample_batched['image'][j]) # imagen j \n",
    "              plt.imshow(img, interpolation='nearest')\n",
    "              plt.show()\n",
    "              labels_correctos = \"\"\n",
    "              labels_predichos = \"\"\n",
    "              for k in range(0, len(pred[j])):\n",
    "                labels_correctos += (kunischTestSet.labels_frame.columns.values[k]+' ') if labels[j].cpu().detach()[k] else \"\"\n",
    "                labels_predichos += (kunischTestSet.labels_frame.columns.values[k]+' ') if pred[j].cpu().detach()[k] else \"\"\n",
    "              print(\"Labels correctos:\")\n",
    "              #print(labels[j].cpu().detach().numpy())\n",
    "              print(labels_correctos)\n",
    "              print(\"Labels predichos:\")\n",
    "              #print(pred[j].cpu().detach().numpy())\n",
    "              print(labels_predichos)\n",
    "              print(\"\\n\")\n",
    "            \n",
    "          TP += ((pred == 1) & (labels == 1)).float().sum()  # True Positive Count\n",
    "          TN += ((pred == 0) & (labels == 0)).float().sum()  # True Negative Count\n",
    "          FP += ((pred == 1) & (labels == 0)).float().sum()  # False Positive Count\n",
    "          FN += ((pred == 0) & (labels == 1)).float().sum()  # False Negative Count\n",
    "          # print('TP: {}\\t TN: {}\\t FP: {}\\t FN: {}\\n'.format(TP,TN,FP,FN) )\n",
    "\n",
    "      TP = TP.cpu().numpy()\n",
    "      TN = TN.cpu().numpy()\n",
    "      FP = FP.cpu().numpy()\n",
    "      FN = FN.cpu().numpy()\n",
    "      accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "      precision = TP / (TP + FP)\n",
    "      recall = TP / (TP + FN)\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      hs = hamming_score(preds_total, labels_total)\n",
    "      test_loss = test_loss / len(kunischTestLoader.dataset) * 1024  # 1024 is the batch size\n",
    "      test_losses.append([epoch, learning_rate, w, test_loss, TP, TN, FP, FN, accuracy, precision, recall, f1_score])\n",
    "      # print statistics\n",
    "      print('Test Trial [{}/{}], LR: {}, W: {}, Avg Loss: {:.4f}, Accuracy: {:.4f}, F1 score: {:.4f}, HS: {:.4f}'\n",
    "            .format(epoch, num_epochs, optimizer.param_groups[0]['lr'], w, test_loss, accuracy, f1_score, hs))\n",
    "      return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando top_labels para 26 labels\n",
      "Aplicando threshold 21 para trabajar con 26 labels\n",
      "Se creará un archivo nuevo para 26 labels. Desea continuar? (y/n)y\n",
      "Top labels creado con éxito\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    140\u001b[0m best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m alex_net \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malexnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m alex_net\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4096\u001b[39m, NUM_LABELS)\n\u001b[0;32m    144\u001b[0m alex_net\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torchvision\\models\\alexnet.py:67\u001b[0m, in \u001b[0;36malexnet\u001b[1;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[1;32m---> 67\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_urls\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malexnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\hub.py:595\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n\u001b[1;32m--> 595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m    995\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_UntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m   1001\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1002\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pruner = KunischPruner(NUM_LABELS)\n",
    "\n",
    "sum_f1 = 0\n",
    "sum_recall = 0\n",
    "sum_precision = 0\n",
    "sum_acc = 0\n",
    "sum_hl = 0\n",
    "sum_emr = 0\n",
    "sum_hs = 0\n",
    "sum_mr1 = 0\n",
    "sum_mr2 = 0\n",
    "sum_mr3 = 0\n",
    "sum_mr4 = 0\n",
    "sum_mr5 = 0\n",
    "\n",
    "    \n",
    "for i in range(0, K):\n",
    "    fold = Kfolds[i]\n",
    "    labels_dir = fold['labels_dir']\n",
    "    patterns_dir = fold['patterns_dir']\n",
    "    output_dir = fold['output_dir']\n",
    "    model_path = fold['model_path']\n",
    "    # Carga de top labels\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "        print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "        top_labels = pruner.filter_labels(train_labels)\n",
    "        pruner.set_top_labels(top_labels)\n",
    "        \n",
    "        save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "            \n",
    "    else: \n",
    "        print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "\n",
    "    NUM_LABELS = len(top_labels) # la cantidad final de etiquetas a trabajar\n",
    "\n",
    "    # Creacion de pesos positivos\n",
    "    if use_pos_weights:\n",
    "        pos_weights = make_positive_weights(top_labels, pos_weights_factor)\n",
    "        pos_weights = torch.Tensor(pos_weights).float().to(device)\n",
    "\n",
    "    else:\n",
    "        pos_weights = None\n",
    "\n",
    "    # Alexnet requires 227 x 227\n",
    "    # Training\n",
    "    kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'train'),\n",
    "                                     labels_file=os.path.join(labels_dir, 'augmented_train_df.json'),\n",
    "                                     transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize(\n",
    "                                                                       mean=[0.485, 0.456, 0.406],\n",
    "                                                                       std=[0.229, 0.224, 0.225])]),\n",
    "                                     top_labels=top_labels)\n",
    "\n",
    "    kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Validation\n",
    "    kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'val'),\n",
    "                                          labels_file=os.path.join(labels_dir, 'val_df.json'),\n",
    "                                          transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                        transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize(\n",
    "                                                                            mean=[0.485, 0.456, 0.406],\n",
    "                                                                            std=[0.229, 0.224, 0.225])]),\n",
    "                                          top_labels=top_labels)\n",
    "\n",
    "    kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                          num_workers=0)\n",
    "\n",
    "    # Test\n",
    "    kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'test'),\n",
    "                                    labels_file=os.path.join(labels_dir, 'test_df.json'),\n",
    "                                    transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                  transforms.ToTensor(),\n",
    "                                                                  transforms.Normalize(\n",
    "                                                                      mean=[0.485, 0.456, 0.406],\n",
    "                                                                      std=[0.229, 0.224, 0.225])]),\n",
    "                                    top_labels=top_labels)\n",
    "\n",
    "    kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    hyperval = \"\"\"\n",
    "    # Hyper Parameter Tuning\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    for param in alex_net.parameters():\n",
    "        param.requires_grad = False\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))), 4)  # pull geometrically\n",
    "      w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))), 10)  # pull geometrically\n",
    "\n",
    "      # Reset Model per test\n",
    "      alex_net = models.alexnet(pretrained=True)\n",
    "      alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "      alex_net.to(device)\n",
    "\n",
    "      optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "      criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.00005)\n",
    "\n",
    "      alex_train(epoch, num_epochs, train_losses, learning_rate, w)\n",
    "      if SAVE:\n",
    "          train_losses_df = pd.DataFrame(train_losses)\n",
    "          train_losses_df.to_csv(os.path.join(output_dir, 'loss_hypertrain.csv'))\n",
    "\n",
    "      alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "      if SAVE:\n",
    "          validation_losses_df = pd.DataFrame(validation_losses)\n",
    "          validation_losses_df.to_csv(os.path.join(output_dir, 'loss_hyperval.csv'))\n",
    "     \"\"\"\n",
    "\n",
    "    # Training\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.001\n",
    "    w = 0.01\n",
    "\n",
    "    # Early Stopping\n",
    "    patience = 10\n",
    "    bad_epochs = 0\n",
    "    best_score = 0.0\n",
    "    best_weights = None\n",
    "\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "    alex_net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "    # scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      score_train = alex_train(epoch, num_epochs, train_losses, learning_rate, w)\n",
    "      score_valid = alex_valid(epoch, num_epochs, validation_losses, learning_rate, w)\n",
    "      print(\"\")\n",
    "\n",
    "      # Early Stopping\n",
    "      if score_valid > best_score:\n",
    "        bad_epochs = 0\n",
    "        best_epoch = epoch\n",
    "        best_score = score_valid\n",
    "        best_weights = alex_net.state_dict()\n",
    "      else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "      if bad_epochs == patience:\n",
    "        print(\"Out of patience!\")\n",
    "        print(f\"Best epoch: {best_epoch}\")\n",
    "        break\n",
    "\n",
    "    if SAVE:\n",
    "        print(f\"Guardando mejor modelo en {model_path}\")\n",
    "        torch.save(best_weights, model_path)\n",
    "\n",
    "    # alerta sonora para cuando haya terminado de entrenar\n",
    "    # import winsound\n",
    "    # import time\n",
    "    # winsound.Beep(freq, 2000)\n",
    "    \n",
    "    # Testing\n",
    "    test_losses = []\n",
    "    test_pred = []\n",
    "    learning_rate = 0.0001\n",
    "    w = 0.001\n",
    "\n",
    "    # Reset Model\n",
    "    alex_net = models.alexnet(pretrained=True)\n",
    "    alex_net.classifier._modules['6'] = nn.Linear(4096, NUM_LABELS)\n",
    "    alex_net.load_state_dict(torch.load(model_path))\n",
    "    alex_net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(alex_net.parameters(), lr=learning_rate, weight_decay=w)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    alex_test(1, 1, test_pred, test_losses, learning_rate, w, show_images = 0)\n",
    "    test_pred_df = pd.DataFrame(test_pred)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    preds = test_pred[0][3].cpu().detach().numpy()\n",
    "    for i in range(1, len(test_pred)):\n",
    "      pbi = test_pred[i][3].cpu().detach().numpy()\n",
    "      preds = np.concatenate((preds, pbi), axis=0)\n",
    "\n",
    "    if SAVE:\n",
    "        save_df = pd.DataFrame(preds)\n",
    "        save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "        print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "        preds = pd.read_csv(os.path.join(output_dir, 'predictions.csv'), index_col=0)\n",
    "        preds = preds.values\n",
    "\n",
    "    #pruner = KunischPruner(preds.shape[1])\n",
    "    #pruner.set_top_labels(top_labels)\n",
    "    #test = pruner.filter_df(labels_test)\n",
    "    \n",
    "    metrics = KunischMetrics(test.values, preds)\n",
    "    sum_f1 += metrics.f1()\n",
    "    sum_recall += metrics.recall()\n",
    "    sum_precision += metrics.precision()\n",
    "    sum_acc += metrics.acc()\n",
    "    sum_hl += metrics.hl()\n",
    "    sum_emr += metrics.emr()\n",
    "    sum_hs += metrics.hs()\n",
    "    sum_mr1 += metrics.mr1()\n",
    "    sum_mr2 += metrics.mr2()\n",
    "    sum_mr3 += metrics.mr3()\n",
    "    sum_mr4 += metrics.mr4()\n",
    "    sum_mr5 += metrics.mr5()\n",
    "\n",
    "    print(f\"HS fold {i}: {metrics.hs()}\")\n",
    "\n",
    "avg_f1 = round(sum_f1/K, 4)\n",
    "avg_recall = round(sum_recall/K, 4)\n",
    "avg_precision = round(sum_precision/K, 4)\n",
    "avg_acc = round(sum_acc/K, 4)\n",
    "avg_hl = round(sum_hl/K, 4)\n",
    "avg_emr = round(sum_emr/K, 4)\n",
    "avg_hs = round(sum_hs/K, 4)\n",
    "avg_mr1 = round(sum_mr1/K, 4)\n",
    "avg_mr2 = round(sum_mr2/K, 4)\n",
    "avg_mr3 = round(sum_mr3/K, 4)\n",
    "avg_mr4 = round(sum_mr4/K, 4)\n",
    "avg_mr5 = round(sum_mr5/K, 4)\n",
    "\n",
    "metadata = {\n",
    "'data_flags': data_flags,\n",
    "'use_pos_weights': use_pos_weights,\n",
    "'pos_weights_factor': pos_weights_factor,\n",
    "'patience': patience,\n",
    "'batch_size': BATCH_SIZE,\n",
    "'optimizer': (type (optimizer).__name__),\n",
    "'scheduler': (type (scheduler).__name__),\n",
    "'criterion': (type (criterion).__name__),\n",
    "'epochs': num_epochs,\n",
    "'best_epoch': best_epoch,\n",
    "'num_labels': NUM_LABELS,\n",
    "'TH_TRAIN': TH_TRAIN,\n",
    "'TH_VAL': TH_VAL,\n",
    "'TH_TEST': TH_TEST,\n",
    "'f1': avg_f1,\n",
    "'recall': avg_recall,\n",
    "'precision': avg_precision,\n",
    "'acc': avg_acc,\n",
    "'hl': avg_hl,\n",
    "'emr': avg_emr,\n",
    "'hs': avg_hs,\n",
    "'mr1': avg_mr1,\n",
    "'mr2': avg_mr2,\n",
    "'mr3': avg_mr3,\n",
    "'mr4': avg_mr4,\n",
    "'mr5': avg_mr5\n",
    "}\n",
    "\n",
    "print(\"HS Final: \", avg_hs)\n",
    "print(\"F1 Final: \", avg_f1)\n",
    "print(\"1MR Final: \", avg_mr1)\n",
    "print(\"5MR Final: \", avg_mr5)\n",
    "\n",
    "if SAVE:\n",
    "    metadf = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "    # output_dir pero sin numero de fold\n",
    "    metadf.to_csv(os.path.join(root_dir, \"outputs\", \"alexnet\", data_flags, exp_name, 'metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN Multilabeling through AlexNet",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
