{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1589c7c",
   "metadata": {},
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# CNN-RNN Multilabeling\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "El objetivo de este notebook es realizar predicciones multilabel sobre patrones geométricos mediante CNN-RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74042862",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e1f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import math \n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "%matplotlib inline\n",
    "import sys \n",
    "import cv2 \n",
    "import pickle\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71546f2",
   "metadata": {},
   "source": [
    "## Configuración de dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b29774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46f8c2",
   "metadata": {},
   "source": [
    "## Selección de dataset y experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab5994d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '..'\n",
    "\n",
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "use_pos_weights = True\n",
    "pos_weights_factor = 1\n",
    "NUM_LABELS = 63\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "TH_TRAIN = 0.5\n",
    "TH_VAL = 0.5\n",
    "TH_TEST = 0.5\n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = False\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27505ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 63L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\0\n",
      "--Labels set encontrado en ..\\labels\\base\\0\n",
      "\n",
      "Fold  1\n",
      "Nombre del experimento: 63L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\1\n",
      "--Labels set encontrado en ..\\labels\\base\\1\n",
      "\n",
      "Fold  2\n",
      "Nombre del experimento: 63L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\2\n",
      "--Labels set encontrado en ..\\labels\\base\\2\n",
      "\n",
      "Fold  3\n",
      "Nombre del experimento: 63L_weighted_1\n",
      "--Pattern set encontrado en ..\\patterns\\base\\3\n",
      "--Labels set encontrado en ..\\labels\\base\\3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "Kfolds = {}\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    patterns_dir = os.path.join(root_dir, 'patterns', data_flags, str(i))\n",
    "    labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "\n",
    "    if not (os.path.isdir(patterns_dir) and os.path.isdir(labels_dir)):\n",
    "        print(patterns_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    weights_str = str(pos_weights_factor)\n",
    "    weights_str = weights_str.replace('.','_')\n",
    "    exp_name += f'_weighted_{weights_str}' if use_pos_weights else ''\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "     \n",
    "    output_dir = os.path.join(root_dir, \"outputs\", \"cnn-rnn\", data_flags, exp_name, str(i))\n",
    "    model_dir = os.path.join(root_dir, \"models\", \"cnn-rnn\", data_flags, str(i))\n",
    "    model_path = os.path.join(model_dir, exp_name)\n",
    "    \n",
    "    path_co = os.path.join(patterns_dir, 'train', 'circular ornaments')\n",
    "    path_lz = os.path.join(patterns_dir, 'train', 'lozenge')\n",
    "    path_pc = os.path.join(patterns_dir, 'train', 'pictographics')\n",
    "    path_ro = os.path.join(patterns_dir, 'train', 'rectangular ornaments')\n",
    "    path_sl = os.path.join(patterns_dir, 'train', 'strokes and lines')\n",
    "    path_to = os.path.join(patterns_dir, 'train', 'triangular ornaments')\n",
    "    \n",
    "    ex_co = os.listdir(path_co)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_co = os.path.join(path_co, ex_co)\n",
    "\n",
    "    ex_lz = os.listdir(path_lz)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_lz = os.path.join(path_lz, ex_lz)\n",
    "\n",
    "    ex_pc = os.listdir(path_pc)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_pc = os.path.join(path_pc, ex_pc)\n",
    "    \n",
    "    ex_ro = os.listdir(path_ro)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_ro = os.path.join(path_ro, ex_ro)\n",
    "\n",
    "    ex_sl = os.listdir(path_sl)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_sl = os.path.join(path_sl, ex_sl)\n",
    "\n",
    "    ex_to = os.listdir(path_to)[random.randint(0, 5)].split('.')[0]\n",
    "    ex_to = os.path.join(path_to, ex_to)\n",
    "    \n",
    "    Kfolds[i] = {\n",
    "        'patterns_dir': patterns_dir,\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path,\n",
    "        'ex_photos': [ex_co, ex_lz, ex_pc, ex_ro, ex_sl, ex_to]\n",
    "    }\n",
    "    \n",
    "    print(\"--Pattern set encontrado en {}\".format(patterns_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed397",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395ff724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Data Loader\n",
    "class KunischDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, images_dir, labels_file, transform, top_labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.pruner = KunischPruner(len(top_labels))\n",
    "        self.pruner.set_top_labels(top_labels)\n",
    "        labels = pd.read_json(labels_file, orient='index')\n",
    "        self.labels_frame = self.pruner.filter_df(labels)\n",
    "        self.num_labels = len(top_labels)\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.transform = transform\n",
    "        self.flags = data_flags\n",
    "        self.top_labels = top_labels\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        return len(self.labels_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.labels_frame.iloc[idx].name + '.png'\n",
    "        img_name = None\n",
    "        for chapter in os.listdir(self.images_dir):\n",
    "          if img_id in os.listdir(os.path.join(self.images_dir, chapter)):\n",
    "            img_name = os.path.join(self.images_dir, chapter, img_id)\n",
    "            break\n",
    "        if img_name is None:\n",
    "          raise Exception(f'No se encontró la imagen para {img_id}')\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        labels = self.labels_frame.iloc[idx].values\n",
    "        labels = np.where(labels)[0].tolist()\n",
    "        labels = list(map(int, labels))\n",
    "        labels = [self.num_labels] + labels + [self.num_labels + 1] #start and end\n",
    "        # [26 ... 0 ... ... 27]\n",
    "        labels = [x + 1 for x in labels] # add 1 to all labels so 0 has no meaning\n",
    "        # [27 ... 1 ... ... 28]\n",
    "        length = len(labels) \n",
    "        for i in range(self.num_labels + 2 - length): #Pad the labels (There are num_labels unique labels)\n",
    "            labels = labels + [0]\n",
    "        # [27 ... 1 ... ... 28 0 0 0 ... 0]\n",
    "        target = torch.Tensor(labels).long()\n",
    "        \n",
    "        sample = {'image': image, 'labels': target, 'lengths': length,\n",
    "                  'paths': img_name}\n",
    "        \n",
    "        return sample\n",
    " \n",
    "\n",
    "# Define Beam Search   \n",
    "def beam_search(k, s, predicted, x, y, pred_sequence_list, prob_sequence_list):\n",
    "  #Inputs Definitions:\n",
    "  #k: Top labels to consider\n",
    "  #s: current state\n",
    "  #predicted: result of lstm (softmax)\n",
    "  #x: current path of labels\n",
    "  #y: current paht of probabilities \n",
    "  #prediction_paths: list of all label paths\n",
    "  #probability_paths: list of all probability paths\n",
    "    \n",
    "    #si se predice end, terminar\n",
    "    if predicted == NUM_LABELS + 2:\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        pred_sequence_list.append(x)\n",
    "        prob_sequence_list.append(y)\n",
    "        #print(\"PREDICHO END\")\n",
    "    # caso contrario: calculo un nuevo estado\n",
    "    else:\n",
    "        inputs = decoder.embedding(predicted) \n",
    "        outputs, s = decoder.execute_lstm(inputs, s)\n",
    "        scores = torch.softmax(outputs[0],dim=0)\n",
    "        top_k_scores = scores.topk(k)[1].unsqueeze(0)\n",
    "        top_k_probs = scores.topk(k)[0].unsqueeze(0)\n",
    "        #print(top_k_scores)\n",
    "        #print(top_k_probs)\n",
    "\n",
    "        sequences = x.expand(k,len(x))\n",
    "        prob_sequences = y.expand(k,len(x))\n",
    "        #print(sequences)\n",
    "        #print(top_k_scores[0][0].unsqueeze(0))\n",
    "        #step =1\n",
    "        \n",
    "        # para cada top k, itero (bajo un nivel del arbol)\n",
    "        for i in range(top_k_scores.size(1)):\n",
    "            x = torch.cat((sequences[i], top_k_scores[0][i].unsqueeze(0) ))\n",
    "            y = torch.cat((prob_sequences[i], top_k_probs[0][i].unsqueeze(0) ))\n",
    "            \n",
    "            #el nuevo predicho es el ultimo elemento de x \n",
    "            predicted = x[len(x)-1].unsqueeze(0)\n",
    "            \n",
    "             #si todavia no predigo ningun label aparte de start y end o si hay uno nuevo, itero\n",
    "            if True: #  len(x) <= 7:\n",
    "                if (x[len(x)-2]==NUM_LABELS + 1 and len(pred_sequence_list)<2) or predicted not in x[:-1]:\n",
    "                    #print('This is predicted: ', x[:-1])\n",
    "                    #print('This is x: ',x)\n",
    "                    # itero de nuevo pero con este nuevo estado, este nuevo predicted,\n",
    "                    # x e y, ..\n",
    "                    beam_search(k,s,predicted,x,y,pred_sequence_list,prob_sequence_list)\n",
    "            else:\n",
    "                #print(\"This case\")\n",
    "                beam_search(k, s, NUM_LABELS + 2, x, y, pred_sequence_list, prob_sequence_list )\n",
    "\n",
    "#Generate Sample Images with Captions\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    image = image.resize([227, 227], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "#Define the CNN RNN architecture\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the Alexnet Architecture modifying where appropriate\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        alex_net = models.alexnet(pretrained=True)\n",
    "        alex_net.classifier._modules['6'] = nn.Linear(4096, embed_size)\n",
    "\n",
    "        \n",
    "        self.alex_net = alex_net\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        \n",
    "        features = self.alex_net(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        #print(np.shape(features))\n",
    "        #print(features.size())\n",
    "        features = self.bn(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=11):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def execute_lstm(self, features, states=None):\n",
    "        \"\"\"Generate labels for given image \"\"\"\n",
    "\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(1):#self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "\n",
    "        return outputs, states\n",
    "    \n",
    "    def embedding(self, features, states=None):\n",
    "        \"\"\"Embedd predicted label\"\"\"\n",
    "        for i in range(1):#self.max_seg_length):\n",
    "            inputs = self.embed(features)                       # inputs: (batch_size, embed_size)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee08d7b",
   "metadata": {},
   "source": [
    "## Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb58504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 63 labels\n"
     ]
    }
   ],
   "source": [
    "fold = Kfolds[0]\n",
    "labels_dir = fold['labels_dir']\n",
    "patterns_dir = fold['patterns_dir']\n",
    "output_dir = fold['output_dir']\n",
    "model_path = fold['model_path']\n",
    "# Carga de top labels\n",
    "train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "\n",
    "if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "    print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "    top_labels = pruner.filter_labels(train_labels)\n",
    "    pruner.set_top_labels(top_labels)\n",
    "\n",
    "    save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "    if save == \"y\":\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "            pickle.dump(top_labels, f)\n",
    "        print(\"Top labels creado con éxito\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"No se logró cargar top_labels\")\n",
    "\n",
    "else: \n",
    "    print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "    with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "        top_labels = pickle.load(f)\n",
    "\n",
    "kunischTrainSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'train'),\n",
    "                                 labels_file=os.path.join(labels_dir, 'augmented_train_df.json'),\n",
    "                                 transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                               transforms.ToTensor(),\n",
    "                                                               transforms.Normalize(\n",
    "                                                                   mean=[0.485, 0.456, 0.406],\n",
    "                                                                   std=[0.229, 0.224, 0.225])]),\n",
    "                                 top_labels=top_labels)\n",
    "\n",
    "kunischTrainLoader = torch.utils.data.DataLoader(kunischTrainSet, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Validation\n",
    "kunischValidationSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'val'),\n",
    "                                      labels_file=os.path.join(labels_dir, 'val_df.json'),\n",
    "                                      transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(\n",
    "                                                                        mean=[0.485, 0.456, 0.406],\n",
    "                                                                        std=[0.229, 0.224, 0.225])]),\n",
    "                                      top_labels=top_labels)\n",
    "\n",
    "kunischValidationLoader = torch.utils.data.DataLoader(kunischValidationSet, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                      num_workers=0)\n",
    "\n",
    "# Test\n",
    "kunischTestSet = KunischDataset(images_dir=os.path.join(patterns_dir, 'test'),\n",
    "                                labels_file=os.path.join(labels_dir, 'test_df.json'),\n",
    "                                transform=transforms.Compose([transforms.Resize((227, 227)),\n",
    "                                                              transforms.ToTensor(),\n",
    "                                                              transforms.Normalize(\n",
    "                                                                  mean=[0.485, 0.456, 0.406],\n",
    "                                                                  std=[0.229, 0.224, 0.225])]),\n",
    "                                top_labels=top_labels)\n",
    "\n",
    "kunischTestLoader = torch.utils.data.DataLoader(kunischTestSet, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab21acf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<padding>', 'panel', 'horizontal', 'ornament', 'hatched', 'vertical', 'metopal', 'filling', 'circle', 'enclosing', 'lozenge', 'double', 'cross-hatched', 'triangle', 'concentric', 'outline', 'dotted', 'meander', 'chain', 'solid', 'bar', 'cross', 'line', 'dot', 'hook', 'swastika', 'floor', 'multiple', 'zigzag', 'single', 'star', 'checkerboard', 'turning', 'pendent', 'left', 'tangential', 'right', 'quatrefoil', \"andrew's\", 'battlement', 'chevron', 'reserved', 'bird', 'shoulder', 'stacked', 'triple', 'band', 'rosette', 'simple', 'parallel', 'body', 'scribble', 'leaf', 'rectilinear', 'pattern', 'sigma', 'alternating', 'quartered', 'axe', 'apex', 'net', 'tree', 'small', 'row', '<start>', '<end>']\n",
      "Train Trial [0/64], Embedded Size: 512, Hidden Size: 512, Layers: 2, LR: 0.0017, W: 2.11128e-05, Avg Loss: 39.260305737692214\n",
      "Valid Trial [0/64], Embedded Size: 512, Hidden Size: 512, Layers: 2, LR: 0.0017, W: 2.11128e-05, Avg Loss: 36.952392578125\n",
      "Train Trial [1/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.001, W: 2.8404e-06, Avg Loss: 31.27794925750248\n",
      "Valid Trial [1/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.001, W: 2.8404e-06, Avg Loss: 28.531703851161858\n",
      "Train Trial [2/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 2.2123e-06, Avg Loss: 40.6026117234003\n",
      "Valid Trial [2/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 2.2123e-06, Avg Loss: 43.91466972155449\n",
      "Train Trial [3/64], Embedded Size: 2048, Hidden Size: 256, Layers: 1, LR: 0.0018, W: 2.1221e-06, Avg Loss: 28.823514908079115\n",
      "Valid Trial [3/64], Embedded Size: 2048, Hidden Size: 256, Layers: 1, LR: 0.0018, W: 2.1221e-06, Avg Loss: 26.23130133213141\n",
      "Train Trial [4/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0004, W: 3.474e-07, Avg Loss: 32.81877644856771\n",
      "Valid Trial [4/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0004, W: 3.474e-07, Avg Loss: 31.332056290064102\n",
      "Train Trial [5/64], Embedded Size: 2048, Hidden Size: 256, Layers: 1, LR: 0.0004, W: 2.06843e-05, Avg Loss: 31.38506111266121\n",
      "Valid Trial [5/64], Embedded Size: 2048, Hidden Size: 256, Layers: 1, LR: 0.0004, W: 2.06843e-05, Avg Loss: 31.424335186298077\n",
      "Train Trial [6/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 2, LR: 0.0001, W: 9.384e-07, Avg Loss: 36.706009637741815\n",
      "Valid Trial [6/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 2, LR: 0.0001, W: 9.384e-07, Avg Loss: 38.7867682041266\n",
      "Train Trial [7/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.001, W: 1.7022e-06, Avg Loss: 37.0760987296937\n",
      "Valid Trial [7/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.001, W: 1.7022e-06, Avg Loss: 35.61743477063301\n",
      "Train Trial [8/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0016, W: 2.75397e-05, Avg Loss: 39.64886135525174\n",
      "Valid Trial [8/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0016, W: 2.75397e-05, Avg Loss: 35.19746594551282\n",
      "Train Trial [9/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0001, W: 6.871e-07, Avg Loss: 35.82284594339038\n",
      "Valid Trial [9/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0001, W: 6.871e-07, Avg Loss: 37.634452624198715\n",
      "Train Trial [10/64], Embedded Size: 1024, Hidden Size: 256, Layers: 1, LR: 0.0075, W: 2.2478e-05, Avg Loss: 32.605531722780256\n",
      "Valid Trial [10/64], Embedded Size: 1024, Hidden Size: 256, Layers: 1, LR: 0.0075, W: 2.2478e-05, Avg Loss: 31.047244340945515\n",
      "Train Trial [11/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0004, W: 2.3441e-06, Avg Loss: 28.204547458224827\n",
      "Valid Trial [11/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0004, W: 2.3441e-06, Avg Loss: 27.74057867588141\n",
      "Train Trial [12/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0003, W: 8.3237e-06, Avg Loss: 31.242048475477432\n",
      "Valid Trial [12/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0003, W: 8.3237e-06, Avg Loss: 31.449212489983974\n",
      "Train Trial [13/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.001, W: 6.6835e-06, Avg Loss: 28.565304226345486\n",
      "Valid Trial [13/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.001, W: 6.6835e-06, Avg Loss: 24.919020432692307\n",
      "Train Trial [14/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.006, W: 2.74007e-05, Avg Loss: 41.199102008153524\n",
      "Valid Trial [14/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.006, W: 2.74007e-05, Avg Loss: 44.38772348257211\n",
      "Train Trial [15/64], Embedded Size: 1024, Hidden Size: 512, Layers: 2, LR: 0.0081, W: 4.0262e-06, Avg Loss: 72.3496800982763\n",
      "Valid Trial [15/64], Embedded Size: 1024, Hidden Size: 512, Layers: 2, LR: 0.0081, W: 4.0262e-06, Avg Loss: 87.26214443108974\n",
      "Train Trial [16/64], Embedded Size: 1024, Hidden Size: 512, Layers: 2, LR: 0.0002, W: 2.9429e-06, Avg Loss: 37.257648286365324\n",
      "Valid Trial [16/64], Embedded Size: 1024, Hidden Size: 512, Layers: 2, LR: 0.0002, W: 2.9429e-06, Avg Loss: 41.28962903145032\n",
      "Train Trial [17/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.0041, W: 8.0624e-06, Avg Loss: 33.59428865947421\n",
      "Valid Trial [17/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.0041, W: 8.0624e-06, Avg Loss: 33.82473520132211\n",
      "Train Trial [18/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0024, W: 1.08662e-05, Avg Loss: 58.3667244078621\n",
      "Valid Trial [18/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0024, W: 1.08662e-05, Avg Loss: 46.19108385917468\n",
      "Train Trial [19/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0014, W: 3.6561e-06, Avg Loss: 38.914524138919894\n",
      "Valid Trial [19/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0014, W: 3.6561e-06, Avg Loss: 33.52046086237981\n",
      "Train Trial [20/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0003, W: 2.7325e-06, Avg Loss: 42.82830858987475\n",
      "Valid Trial [20/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0003, W: 2.7325e-06, Avg Loss: 43.58102025741186\n",
      "Train Trial [21/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 2.96074e-05, Avg Loss: 40.619839622860866\n",
      "Valid Trial [21/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 2.96074e-05, Avg Loss: 39.608235677083336\n",
      "Train Trial [22/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0068, W: 2.5615e-06, Avg Loss: 70.73824346633185\n",
      "Valid Trial [22/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0068, W: 2.5615e-06, Avg Loss: 100.77960987580128\n",
      "Train Trial [23/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.008, W: 4.144e-07, Avg Loss: 79.0119861421131\n",
      "Valid Trial [23/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.008, W: 4.144e-07, Avg Loss: 86.89501327123398\n",
      "Train Trial [24/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 3.978e-07, Avg Loss: 40.229526095920136\n",
      "Valid Trial [24/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 3.978e-07, Avg Loss: 41.32806865985577\n",
      "Train Trial [25/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.002, W: 3.6094e-06, Avg Loss: 29.202449495830233\n",
      "Valid Trial [25/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.002, W: 3.6094e-06, Avg Loss: 29.581248747996796\n",
      "Train Trial [26/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0036, W: 7.1815e-06, Avg Loss: 35.425814674014134\n",
      "Valid Trial [26/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0036, W: 7.1815e-06, Avg Loss: 38.5900628505609\n",
      "Train Trial [27/64], Embedded Size: 512, Hidden Size: 2048, Layers: 1, LR: 0.0018, W: 1.5119e-06, Avg Loss: 54.67141820514013\n",
      "Valid Trial [27/64], Embedded Size: 512, Hidden Size: 2048, Layers: 1, LR: 0.0018, W: 1.5119e-06, Avg Loss: 55.289744841746796\n",
      "Train Trial [28/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0009, W: 6.2078e-06, Avg Loss: 36.793499174572176\n",
      "Valid Trial [28/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0009, W: 6.2078e-06, Avg Loss: 37.02632023737981\n",
      "Train Trial [29/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0001, W: 6.442e-07, Avg Loss: 30.808980305989582\n",
      "Valid Trial [29/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0001, W: 6.442e-07, Avg Loss: 29.42209723056891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [30/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0001, W: 1.8927e-06, Avg Loss: 37.8252209860181\n",
      "Valid Trial [30/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0001, W: 1.8927e-06, Avg Loss: 41.45995154747596\n",
      "Train Trial [31/64], Embedded Size: 2048, Hidden Size: 2048, Layers: 1, LR: 0.004, W: 1.7829e-06, Avg Loss: 81.4197019546751\n",
      "Valid Trial [31/64], Embedded Size: 2048, Hidden Size: 2048, Layers: 1, LR: 0.004, W: 1.7829e-06, Avg Loss: 84.81162359775641\n",
      "Train Trial [32/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 2, LR: 0.0081, W: 5.6231e-06, Avg Loss: 129.3019060407366\n",
      "Valid Trial [32/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 2, LR: 0.0081, W: 5.6231e-06, Avg Loss: 91.27429512219551\n",
      "Train Trial [33/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0045, W: 7.985e-07, Avg Loss: 40.58753603980655\n",
      "Valid Trial [33/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0045, W: 7.985e-07, Avg Loss: 41.09590970552885\n",
      "Train Trial [34/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0052, W: 6.7116e-06, Avg Loss: 77.1060800703745\n",
      "Valid Trial [34/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0052, W: 6.7116e-06, Avg Loss: 90.82080704126602\n",
      "Train Trial [35/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 3.665e-07, Avg Loss: 39.58148096478175\n",
      "Valid Trial [35/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0002, W: 3.665e-07, Avg Loss: 39.60587252103365\n",
      "Train Trial [36/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0009, W: 2.85665e-05, Avg Loss: 39.76197015671503\n",
      "Valid Trial [36/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0009, W: 2.85665e-05, Avg Loss: 32.37996732271635\n",
      "Train Trial [37/64], Embedded Size: 512, Hidden Size: 1024, Layers: 1, LR: 0.003, W: 1.7023e-05, Avg Loss: 44.17641969711062\n",
      "Valid Trial [37/64], Embedded Size: 512, Hidden Size: 1024, Layers: 1, LR: 0.003, W: 1.7023e-05, Avg Loss: 39.34190016526442\n",
      "Train Trial [38/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0015, W: 2.93408e-05, Avg Loss: 31.064908466641864\n",
      "Valid Trial [38/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0015, W: 2.93408e-05, Avg Loss: 26.949259440104168\n",
      "Train Trial [39/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0037, W: 1.7594e-05, Avg Loss: 70.98570711650545\n",
      "Valid Trial [39/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0037, W: 1.7594e-05, Avg Loss: 97.3830316005609\n",
      "Train Trial [40/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0007, W: 1.4763e-06, Avg Loss: 29.299918038504465\n",
      "Valid Trial [40/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0007, W: 1.4763e-06, Avg Loss: 26.115523900741184\n",
      "Train Trial [41/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0013, W: 5.173e-07, Avg Loss: 44.40552920386905\n",
      "Valid Trial [41/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 2, LR: 0.0013, W: 5.173e-07, Avg Loss: 43.378837389823715\n",
      "Train Trial [42/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0004, W: 1.0112e-06, Avg Loss: 29.74771699451265\n",
      "Valid Trial [42/64], Embedded Size: 2048, Hidden Size: 1024, Layers: 1, LR: 0.0004, W: 1.0112e-06, Avg Loss: 27.522792718349358\n",
      "Train Trial [43/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0059, W: 1.56641e-05, Avg Loss: 54.00438242109995\n",
      "Valid Trial [43/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0059, W: 1.56641e-05, Avg Loss: 64.1382086338141\n",
      "Train Trial [44/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0018, W: 3.123e-06, Avg Loss: 39.5455331953745\n",
      "Valid Trial [44/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0018, W: 3.123e-06, Avg Loss: 34.09968449519231\n",
      "Train Trial [45/64], Embedded Size: 512, Hidden Size: 1024, Layers: 1, LR: 0.0005, W: 3.6926e-06, Avg Loss: 33.91062757703993\n",
      "Valid Trial [45/64], Embedded Size: 512, Hidden Size: 1024, Layers: 1, LR: 0.0005, W: 3.6926e-06, Avg Loss: 28.57625012520032\n",
      "Train Trial [46/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0026, W: 8.977e-07, Avg Loss: 60.37569415380084\n",
      "Valid Trial [46/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0026, W: 8.977e-07, Avg Loss: 54.90848482572115\n",
      "Train Trial [47/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0002, W: 3.867e-07, Avg Loss: 38.05943467881944\n",
      "Valid Trial [47/64], Embedded Size: 2048, Hidden Size: 256, Layers: 2, LR: 0.0002, W: 3.867e-07, Avg Loss: 44.13548239683494\n",
      "Train Trial [48/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.004, W: 7.883e-07, Avg Loss: 70.02639625186012\n",
      "Valid Trial [48/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.004, W: 7.883e-07, Avg Loss: 105.76402243589743\n",
      "Train Trial [49/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0013, W: 3.809e-06, Avg Loss: 29.687322949606276\n",
      "Valid Trial [49/64], Embedded Size: 1024, Hidden Size: 512, Layers: 1, LR: 0.0013, W: 3.809e-06, Avg Loss: 26.511199168669872\n",
      "Train Trial [50/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0003, W: 1.1225e-06, Avg Loss: 38.90978713262649\n",
      "Valid Trial [50/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0003, W: 1.1225e-06, Avg Loss: 43.79381698217147\n",
      "Train Trial [51/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.0006, W: 1.6709e-06, Avg Loss: 28.551443190801713\n",
      "Valid Trial [51/64], Embedded Size: 2048, Hidden Size: 512, Layers: 1, LR: 0.0006, W: 1.6709e-06, Avg Loss: 27.041306715745193\n",
      "Train Trial [52/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 1.2666e-05, Avg Loss: 41.200236487010166\n",
      "Valid Trial [52/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 1.2666e-05, Avg Loss: 50.86766013120994\n",
      "Train Trial [53/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0005, W: 5.6742e-06, Avg Loss: 47.79517957899306\n",
      "Valid Trial [53/64], Embedded Size: 512, Hidden Size: 1024, Layers: 2, LR: 0.0005, W: 5.6742e-06, Avg Loss: 42.91209685496795\n",
      "Train Trial [54/64], Embedded Size: 512, Hidden Size: 2048, Layers: 1, LR: 0.0001, W: 5.985e-07, Avg Loss: 34.75059921022446\n",
      "Valid Trial [54/64], Embedded Size: 512, Hidden Size: 2048, Layers: 1, LR: 0.0001, W: 5.985e-07, Avg Loss: 33.33264473157051\n",
      "Train Trial [55/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 1.67332e-05, Avg Loss: 40.936331612723215\n",
      "Valid Trial [55/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 1.67332e-05, Avg Loss: 49.01791616586539\n",
      "Train Trial [56/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0004, W: 8.8174e-06, Avg Loss: 38.117494613405256\n",
      "Valid Trial [56/64], Embedded Size: 512, Hidden Size: 256, Layers: 2, LR: 0.0004, W: 8.8174e-06, Avg Loss: 42.4715576171875\n",
      "Train Trial [57/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0002, W: 4.728e-07, Avg Loss: 31.99112277560764\n",
      "Valid Trial [57/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0002, W: 4.728e-07, Avg Loss: 27.847412109375\n",
      "Train Trial [58/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0002, W: 2.54054e-05, Avg Loss: 34.59255884564112\n",
      "Valid Trial [58/64], Embedded Size: 512, Hidden Size: 512, Layers: 1, LR: 0.0002, W: 2.54054e-05, Avg Loss: 34.84176870492789\n",
      "Train Trial [59/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0055, W: 7.947e-07, Avg Loss: 61.46905469137525\n",
      "Valid Trial [59/64], Embedded Size: 1024, Hidden Size: 1024, Layers: 1, LR: 0.0055, W: 7.947e-07, Avg Loss: 60.77629206730769\n",
      "Train Trial [60/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 8.9598e-06, Avg Loss: 41.044478159102184\n",
      "Valid Trial [60/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0001, W: 8.9598e-06, Avg Loss: 48.95596704727564\n",
      "Train Trial [61/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0002, W: 1.3826e-06, Avg Loss: 36.169894748263886\n",
      "Valid Trial [61/64], Embedded Size: 2048, Hidden Size: 512, Layers: 2, LR: 0.0002, W: 1.3826e-06, Avg Loss: 39.465629382011215\n",
      "Train Trial [62/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0013, W: 3.966e-07, Avg Loss: 37.45139470176091\n",
      "Valid Trial [62/64], Embedded Size: 1024, Hidden Size: 256, Layers: 2, LR: 0.0013, W: 3.966e-07, Avg Loss: 33.788389548277245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Trial [63/64], Embedded Size: 512, Hidden Size: 256, Layers: 1, LR: 0.0005, W: 4.1482e-06, Avg Loss: 32.9197988358755\n",
      "Valid Trial [63/64], Embedded Size: 512, Hidden Size: 256, Layers: 1, LR: 0.0005, W: 4.1482e-06, Avg Loss: 31.765290089142628\n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameter Tuning\n",
    "vocab = list(top_labels.index.values)\n",
    "vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "print(vocab)\n",
    "\n",
    "num_epochs = 64\n",
    "total_step = len(kunischTrainLoader)\n",
    "log_step = 10000\n",
    "train_losses = []\n",
    "validation_losses =[]\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    embed_size = random.choice([512,1024,2048])\n",
    "    hidden_size = random.choice([256,512,1024,2048])\n",
    "    num_layers = random.choice([1,2])\n",
    "    \n",
    "    # Build the models\n",
    "    encoder = EncoderCNN(embed_size).to(device)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "    learning_rate = round(np.exp(random.uniform(np.log(.0001), np.log(.01))),4) #pull geometrically\n",
    "    w = round(np.exp(random.uniform(np.log(3.1e-7), np.log(3.1e-5))),10) #pull geometrically\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.alex_net.parameters()) + list(encoder.bn.parameters())\n",
    "    #optimizer = torch.optim.Adam(params, lr=learning_rate, weight_decay = w)\n",
    "    optimizer = torch.optim.RMSprop(params, lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=w)\n",
    "    train_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for i, sample_batched in enumerate(kunischTrainLoader):\n",
    "        \n",
    "        # Set mini-batch dataset\n",
    "        unsorted_images = sample_batched['image']\n",
    "        unsorted_labels = sample_batched['labels']\n",
    "        unsorted_lengths = sample_batched['lengths']\n",
    "        sorted_length_index = sorted(range(len(unsorted_lengths)),key=unsorted_lengths.__getitem__,reverse=True)\n",
    "        inputs =[]\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for j in sorted_length_index:\n",
    "            inputs.append(unsorted_images[j])\n",
    "            labels.append(unsorted_labels[j])\n",
    "            lengths.append(unsorted_lengths[j])\n",
    "            \n",
    "        inputs =torch.stack(inputs).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        lengths = torch.stack(lengths)\n",
    "        targets = pack_padded_sequence(labels, lengths, batch_first=True)[0].to(device)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(targets)\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(inputs)\n",
    "        outputs = decoder(features, labels, lengths)\n",
    "        #print(outputs)\n",
    "        #print(np.shape(outputs))\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # Print log info\n",
    "        #if i % log_step == 0:\n",
    "        #    print('Train Trial [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "        #          .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(kunischTrainLoader.dataset)*1024 #1024 is the batch size\n",
    "    train_losses.append([epoch,  embed_size, hidden_size, num_layers, learning_rate, w, train_loss ])\n",
    "    \n",
    "    print('Train Trial [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "          .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                  num_layers, learning_rate, w, train_loss ))\n",
    "\n",
    "\n",
    "    train_losses_df = pd.DataFrame(train_losses)\n",
    "    train_losses_df.to_csv('hypertrain_cnn_rnn_losses.csv')\n",
    "    \n",
    "    #Begin Validation\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "      for i, sample_batched in enumerate(kunischValidationLoader):\n",
    "          # Set mini-batch dataset\n",
    "          unsorted_images = sample_batched['image']\n",
    "          unsorted_labels = sample_batched['labels']\n",
    "          unsorted_lengths = sample_batched['lengths']\n",
    "          sorted_length_index = sorted(range(len(unsorted_lengths)),\n",
    "                                       key=unsorted_lengths.__getitem__,reverse=True)\n",
    "          inputs =[]\n",
    "          labels = []\n",
    "          lengths = []\n",
    "          for j in sorted_length_index:\n",
    "              inputs.append(unsorted_images[j])\n",
    "              labels.append(unsorted_labels[j])\n",
    "              lengths.append(unsorted_lengths[j])\n",
    "\n",
    "          inputs =torch.stack(inputs).to(device)\n",
    "          labels = torch.stack(labels).to(device)\n",
    "          lengths = torch.stack(lengths)\n",
    "          targets = pack_padded_sequence(labels , lengths, batch_first=True)[0].to(device)\n",
    "          #print(labels)\n",
    "          #print(targets)\n",
    "          # Forward, backward and optimize\n",
    "          features = encoder(inputs)\n",
    "          outputs = decoder(features, labels , lengths)\n",
    "          #print(outputs)\n",
    "          #print(np.shape(outputs))\n",
    "          loss = criterion(outputs, targets)\n",
    "          validation_loss += loss.item()\n",
    "          \n",
    "    validation_loss = validation_loss/len(kunischValidationLoader.dataset)*1024\n",
    "    validation_losses.append([epoch,  embed_size, hidden_size, num_layers, learning_rate, w,  validation_loss ])\n",
    "    \n",
    "    print('Valid Trial [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "          .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                  num_layers, learning_rate, w, validation_loss ))\n",
    "    validation_losses_df = pd.DataFrame(validation_losses)\n",
    "    validation_losses_df.to_csv('hypervalid_cnn_rnn_losses.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf24e80a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<padding>', 'panel', 'horizontal', 'ornament', 'hatched', 'vertical', 'metopal', 'filling', 'circle', 'enclosing', 'lozenge', 'double', 'cross-hatched', 'triangle', 'concentric', 'outline', 'dotted', 'meander', 'chain', 'solid', 'bar', 'cross', 'line', 'dot', 'hook', 'swastika', 'floor', 'multiple', 'zigzag', 'single', 'star', 'checkerboard', 'turning', 'pendent', 'left', 'tangential', 'right', 'quatrefoil', \"andrew's\", 'battlement', 'chevron', 'reserved', 'bird', 'shoulder', 'stacked', 'triple', 'band', 'rosette', 'simple', 'parallel', 'body', 'scribble', 'leaf', 'rectilinear', 'pattern', 'sigma', 'alternating', 'quartered', 'axe', 'apex', 'net', 'tree', 'small', 'row', '<start>', '<end>']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 6.00 GiB total capacity; 3.99 GiB already allocated; 7.62 MiB free; 4.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#print(labels)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#print(targets)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Forward, backward and optimize\u001b[39;00m\n\u001b[0;32m     72\u001b[0m features \u001b[38;5;241m=\u001b[39m encoder(inputs)\n\u001b[1;32m---> 73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m#print(np.shape(outputs))\u001b[39;00m\n\u001b[0;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[1;34m(self, features, captions, lengths)\u001b[0m\n\u001b[0;32m    158\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((features\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), embeddings), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m packed \u001b[38;5;241m=\u001b[39m pack_padded_sequence(embeddings, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m--> 160\u001b[0m hiddens, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(hiddens[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:764\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    761\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    762\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    766\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    767\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 528.00 MiB (GPU 0; 6.00 GiB total capacity; 3.99 GiB already allocated; 7.62 MiB free; 4.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "fold = Kfolds[0]\n",
    "labels_dir = fold['labels_dir']\n",
    "patterns_dir = fold['patterns_dir']\n",
    "output_dir = fold['output_dir']\n",
    "model_path = fold['model_path']\n",
    "photos = fold['ex_photos']\n",
    "\n",
    "os.makedirs(model_path, exist_ok = True)\n",
    "encoder_path = os.path.join(model_path, 'encoder.pth')\n",
    "decoder_path = os.path.join(model_path, 'decoder.pth')\n",
    "\n",
    "\n",
    "# Carga de top labels\n",
    "train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "\n",
    "vocab = list(top_labels.index.values)\n",
    "vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "print(vocab)\n",
    "\n",
    "embed_size = 1024\n",
    "hidden_size = 2048\n",
    "num_layers = 4\n",
    "# Build the models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "w = 2.20e-05\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.alex_net.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.RMSprop(params, lr=learning_rate, alpha=0.99, eps=1e-08, weight_decay=w)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=5, min_lr=0.0001)\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for i, sample_batched in enumerate(kunischTrainLoader):\n",
    "        \n",
    "        # Set mini-batch dataset\n",
    "        unsorted_images = sample_batched['image']\n",
    "        unsorted_labels = sample_batched['labels']\n",
    "        unsorted_lengths = sample_batched['lengths']\n",
    "        sorted_length_index = sorted(range(len(unsorted_lengths)),key=unsorted_lengths.__getitem__,reverse=True)\n",
    "        inputs =[]\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for j in sorted_length_index:\n",
    "            inputs.append(unsorted_images[j])\n",
    "            labels.append(unsorted_labels[j])\n",
    "            lengths.append(unsorted_lengths[j])\n",
    "            \n",
    "        inputs =torch.stack(inputs).to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        lengths = torch.stack(lengths)\n",
    "        targets = pack_padded_sequence(labels , lengths, batch_first=True)[0].to(device)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(targets)\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(inputs)\n",
    "        outputs = decoder(features, labels , lengths)\n",
    "        #print(outputs)\n",
    "        #print(np.shape(outputs))\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss/len(kunischTrainLoader.dataset)*1024\n",
    "    scheduler.step(train_loss)\n",
    "    train_losses.append([epoch, train_loss ])\n",
    "    \n",
    "    print('Epoch [{}/{}], Embedded Size: {}, Hidden Size: {}, Layers: {}, LR: {}, W: {}, Avg Loss: {}'\n",
    "          .format(epoch, num_epochs, embed_size, hidden_size,\n",
    "                  num_layers, learning_rate, w, train_loss ))\n",
    "\n",
    "    torch.save(encoder.state_dict(), encoder_path)\n",
    "    torch.save(decoder.state_dict(), decoder_path)\n",
    "\n",
    "print(\"Cargando modelos\")\n",
    "    \n",
    "#Load Parameters\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "print(\"Modelos cargados\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "for i in photos:\n",
    "    print(\"Working photo \", i)\n",
    "    img_name = i  +'.png'\n",
    "    \n",
    "    image = load_image(img_name, transform).to(device)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    prediction_paths = []\n",
    "    probability_paths = []\n",
    "    # Encode - read the image features\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    output_sample, s = decoder.execute_lstm(encoder_out)\n",
    "    _, predicted = output_sample.max(1)  \n",
    "\n",
    "    sequences = predicted\n",
    "    prob_sequences = torch.sigmoid(output_sample.max())\n",
    "    #display(sequences)\n",
    "    #display(prob_sequences)\n",
    "    \n",
    "    print(\"Calling to beam search\")\n",
    "    beam_search(3, s, predicted, sequences, prob_sequences, prediction_paths, probability_paths)\n",
    "    index_val = 0\n",
    "    \n",
    "    print(\"After beam search\")\n",
    "    for i in range(0, len(prediction_paths)):\n",
    "        if i ==0:\n",
    "            best = np.prod(probability_paths[i].detach().cpu().numpy())\n",
    "        elif best < np.prod(probability_paths[i].detach().cpu().numpy()):\n",
    "            best = np.prod(probability_paths[i].detach().cpu().numpy())\n",
    "            index_val = i\n",
    "\n",
    "    sampled_ids = prediction_paths[index_val].cpu().numpy()\n",
    "    sampled_prob = best\n",
    "    sampled_caption = []\n",
    "\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "\n",
    "    # Print out the image and the generated caption\n",
    "\n",
    "    image = cv2.imread(img_name)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    print (sentence)\n",
    "    print('Labels: {} \\nPrediction Path Probability: {:.4f}'.format( sampled_ids,sampled_prob))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0142125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<padding>', 'panel', 'horizontal', 'ornament', 'hatched', 'vertical', 'metopal', 'filling', 'circle', 'enclosing', 'lozenge', 'double', 'cross-hatched', 'triangle', 'concentric', 'outline', 'dotted', 'meander', 'chain', 'solid', 'bar', 'cross', 'line', 'dot', 'hook', 'swastika', 'floor', 'multiple', 'zigzag', 'single', 'star', 'checkerboard', 'turning', 'pendent', 'left', 'tangential', 'right', 'quatrefoil', \"andrew's\", 'battlement', 'chevron', 'reserved', 'bird', 'shoulder', 'stacked', 'triple', 'band', 'rosette', 'simple', 'parallel', 'body', 'scribble', 'leaf', 'rectilinear', 'pattern', 'sigma', 'alternating', 'quartered', 'axe', 'apex', 'net', 'tree', 'small', 'row', '<start>', '<end>']\n",
      "257 10604 593 768\n",
      "Test: Accuracy: 0.8886\tPrecision: 0.3024\t Recall: 0.2507\t F1 Score: 0.2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run Models on Test data using Beam Search\n",
    "\n",
    "vocab = list(top_labels.index.values)\n",
    "vocab = ['<padding>'] + vocab + ['<start>', '<end>']\n",
    "print(vocab)\n",
    "\n",
    "TP, TN, FP, FN = 0, 0, 0, 0\n",
    "test_pred = []\n",
    "\n",
    "images = []\n",
    "im_labels = []\n",
    "for sample in kunischTestLoader:\n",
    "    images = images + sample['paths']\n",
    "    lbl = sample['labels'].cpu().detach().numpy()\n",
    "    for i in lbl:\n",
    "        im_labels.append(i)\n",
    "        \n",
    "for i, j  in zip(images, im_labels):\n",
    "    img_name = i\n",
    "    labels = j\n",
    "    labels = list(map(int, labels))\n",
    "    try:\n",
    "        labels.remove(NUM_LABELS + 1)\n",
    "        labels.remove(NUM_LABELS + 2)\n",
    "    except:\n",
    "        print(\"Etiquetas no incorporan start, end\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Prepare an image\n",
    "    image = load_image(img_name, transform)\n",
    "    image_tensor = image.to(device)\n",
    "    \n",
    "    # Generate an caption from the image\n",
    "    prediction_paths = []\n",
    "    probability_paths = []\n",
    "    # Encode - read the image features\n",
    "    encoder_out = encoder(image_tensor)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    output_sample, s = decoder.execute_lstm(encoder_out)\n",
    "    _, predicted = output_sample.max(1)  \n",
    "\n",
    "    predicted = torch.tensor([NUM_LABELS + 1]).to(device)\n",
    "    sequences = predicted\n",
    "    prob_sequences = torch.sigmoid(output_sample.max())\n",
    "    prob_sequences\n",
    "\n",
    "    beam_search(2, s, predicted, sequences, prob_sequences,prediction_paths, probability_paths)\n",
    "    for i in range(len(prediction_paths)):\n",
    "        if i ==0:\n",
    "            best = np.prod(probability_paths[i].detach().cpu().numpy())\n",
    "            index_val = i \n",
    "        elif best < np.prod(probability_paths[i].detach().cpu().numpy()):\n",
    "            best = np.prod(probability_paths[i].detach().cpu().numpy())\n",
    "            index_val = i\n",
    "\n",
    "    sampled_ids = prediction_paths[index_val].cpu().numpy()\n",
    "    \n",
    "    pred = sampled_ids\n",
    "    \n",
    "    \n",
    "    end = np.argwhere(pred==NUM_LABELS + 2)[0][0]\n",
    "    start = np.argwhere(pred==NUM_LABELS + 1)[0][0]+1\n",
    "    pred = pred[start:end]\n",
    "    \n",
    "    pred2 = np.zeros(NUM_LABELS)\n",
    "    labels2 = np.zeros(NUM_LABELS)\n",
    "    for i in pred:\n",
    "        pred2[i-1]=1\n",
    "    for i in labels:\n",
    "        labels2[i-1]=1   \n",
    "    test_pred.append([img_name,labels2,pred2])\n",
    "    TP += ((pred2==1)&(labels2==1)).sum()\n",
    "    TN += ((pred2==0)&(labels2==0)).sum()\n",
    "    FP += ((pred2==1)&(labels2==0)).sum()\n",
    "    FN += ((pred2==0)&(labels2==1)).sum()\n",
    "\n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "precision = TP/(TP + FP)\n",
    "recall = TP/(TP + FN)\n",
    "f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "print(TP,TN,FP,FN)\n",
    "print('Test: Accuracy: {:.4f}\\tPrecision: {:.4f}\\t Recall: {:.4f}\\t F1 Score: {:.4f}\\n'.\n",
    "          format(accuracy,  precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94609011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95abb3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
