{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# C2AE Architecture\n",
    "* X:\n",
    "    * (N, d)\n",
    "* Y:\n",
    "    * (N, m)\n",
    "* Z:\n",
    "    * (N, l)\n",
    "\n",
    "## Three main components:\n",
    "* Fx:\n",
    "    * Encodes x into latent space z.\n",
    "* Fe:\n",
    "    * Encodes y into latent space z.\n",
    "* Fd:\n",
    "    * Decodes z into label space. \n",
    "\n",
    "## Loss functions:\n",
    "\n",
    "$$L_1 = ||F_x(X) - F_e(Y)||^2 s.t. F_x(X)Fx(X)^T = F_e(Y)F_e(Y)^T = I$$\n",
    "$$L_2 = \\Gamma(F_e, F_d) = \\Sigma_i^N E_i$$\n",
    "$$E_i = \\frac{1}{|y_i^1||y_i^0|} \\Sigma_{p,q \\in y_i^1\\times y_i^0} e^{F_d(F_e(y_i))^q - F_d(F_e(y_I))^p}$$\n",
    "\n",
    "## Combined Loss:\n",
    "$$L_1 + \\alpha L_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from C2AE import C2AE, save_model, load_model, Fe, Fx, Fd, eval_metrics, get_predictions\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from textwrap import wrap\n",
    "\n",
    "\n",
    "from utils import KunischMetrics\n",
    "from utils import KunischPruner\n",
    "from utils import DataExplorer\n",
    "from utils import KunischPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = []\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 1\n",
    "\n",
    "NUM_LABELS = 26\n",
    "BATCH_SIZE = 100\n",
    "PATIENCE = 100\n",
    "NUM_EPOCHS = 600\n",
    "FEATURES_DIM = 4096\n",
    "\n",
    "PATTERNS_AS_FEATURES = False \n",
    "\n",
    "# 0 es 3090, 1 y 2 son 2080\n",
    "CUDA_ID = 0\n",
    "\n",
    "SAVE = True\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ..\\features\\alexnet_retrained\\base\\K0\n",
      "--Labels set encontrado en ..\\labels\\base\\0\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\0\n",
      "Los modelos se guardarán en: ..\\models\\C2AE_alexnet_retrained\\base\\0\n",
      "Fold  1\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ..\\features\\alexnet_retrained\\base\\K1\n",
      "--Labels set encontrado en ..\\labels\\base\\1\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\1\n",
      "Los modelos se guardarán en: ..\\models\\C2AE_alexnet_retrained\\base\\1\n",
      "Fold  2\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ..\\features\\alexnet_retrained\\base\\K2\n",
      "--Labels set encontrado en ..\\labels\\base\\2\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\2\n",
      "Los modelos se guardarán en: ..\\models\\C2AE_alexnet_retrained\\base\\2\n",
      "Fold  3\n",
      "Nombre del experimento: 26L\n",
      "--Feature set encontrado en ..\\features\\alexnet_retrained\\base\\K3\n",
      "--Labels set encontrado en ..\\labels\\base\\3\n",
      "\n",
      "Los resultados se guardarán en: ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\3\n",
      "Los modelos se guardarán en: ..\\models\\C2AE_alexnet_retrained\\base\\3\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requested data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "Kfolds = {}\n",
    "root_dir = '..'\n",
    "\n",
    "for i in range(0, K):\n",
    "    print(\"Fold \", i)\n",
    "    \n",
    "    exp_name = f\"{NUM_LABELS}L\"\n",
    "    print(f\"Nombre del experimento: {exp_name}\")\n",
    "    \n",
    "    if PATTERNS_AS_FEATURES:\n",
    "        features_dir = os.path.join(root_dir, 'features', 'patterns', data_flags, f'K{str(i)}')\n",
    "        labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "        output_dir = os.path.join(root_dir, \"outputs\", \"C2AE_images\", data_flags, exp_name, str(i))\n",
    "        model_dir = os.path.join(root_dir, 'models', 'C2AE_images', data_flags, str(i))\n",
    "        model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "    \n",
    "    else:\n",
    "        features_dir = os.path.join(root_dir, 'features', 'alexnet_retrained', data_flags, f'K{str(i)}')\n",
    "        labels_dir = os.path.join(root_dir, 'labels', data_flags, str(i))\n",
    "        output_dir = os.path.join(root_dir, \"outputs\", \"C2AE_alexnet_retrained\", data_flags, exp_name, str(i))\n",
    "        model_dir = os.path.join(root_dir, 'models', 'C2AE_alexnet_retrained', data_flags, str(i))\n",
    "        model_path = os.path.join(model_dir, exp_name + '.pth')\n",
    "           \n",
    "\n",
    "    Kfolds[i] = {\n",
    "        'labels_dir': labels_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'model_path': model_path,\n",
    "        'features_dir': features_dir,\n",
    "    }\n",
    "    \n",
    "    if not (os.path.isdir(features_dir) and os.path.isdir(labels_dir)):\n",
    "        print(features_dir)\n",
    "        print(labels_dir)\n",
    "        raise FileNotFoundError(\"\"\"\n",
    "        No existen directorios de datos para el conjunto de flags seleccionado. \n",
    "        Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation.\n",
    "        \"\"\")\n",
    "        \n",
    "    print(\"--Feature set encontrado en {}\".format(features_dir))\n",
    "    print(\"--Labels set encontrado en {}\".format(labels_dir))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    if SAVE:\n",
    "        os.makedirs(output_dir, exist_ok = True)\n",
    "        os.makedirs(model_dir, exist_ok = True)\n",
    "        print(f\"Los resultados se guardarán en: {output_dir}\")\n",
    "        print(f\"Los modelos se guardarán en: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: NVIDIA GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{CUDA_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Epoch: 0, Loss: 20.08154010772705,  L-Loss: 0.9306446313858032, C-Loss: 1.9616217613220215\n",
      "Epoch: 1, Loss: 19.98279857635498,  L-Loss: 0.745134174823761, C-Loss: 1.9610232710838318\n",
      "Epoch: 2, Loss: 19.905162811279297,  L-Loss: 0.6055770218372345, C-Loss: 1.960237443447113\n",
      "Epoch: 3, Loss: 19.844075202941895,  L-Loss: 0.5069712996482849, C-Loss: 1.959058940410614\n",
      "Epoch: 4, Loss: 19.79284954071045,  L-Loss: 0.43739287555217743, C-Loss: 1.9574153423309326\n",
      "Epoch: 5, Loss: 19.764962196350098,  L-Loss: 0.38199570775032043, C-Loss: 1.9573964476585388\n",
      "Epoch: 6, Loss: 19.7346248626709,  L-Loss: 0.33682355284690857, C-Loss: 1.95662122964859\n",
      "Epoch: 7, Loss: 19.697141647338867,  L-Loss: 0.30778107047080994, C-Loss: 1.9543251395225525\n",
      "Epoch: 8, Loss: 19.696250915527344,  L-Loss: 0.28554457426071167, C-Loss: 1.9553478956222534\n",
      "Epoch: 9, Loss: 19.66380500793457,  L-Loss: 0.2661236822605133, C-Loss: 1.9530743956565857\n",
      "Epoch: 10, Loss: 19.640448570251465,  L-Loss: 0.24957982450723648, C-Loss: 1.9515658617019653\n",
      "Epoch: 11, Loss: 19.628241539001465,  L-Loss: 0.23728309571743011, C-Loss: 1.9509600400924683\n",
      "Epoch: 12, Loss: 19.613140106201172,  L-Loss: 0.22593898326158524, C-Loss: 1.9500171542167664\n",
      "Epoch: 13, Loss: 19.59196186065674,  L-Loss: 0.21741391718387604, C-Loss: 1.9483254551887512\n",
      "Epoch: 14, Loss: 19.593673706054688,  L-Loss: 0.21025226265192032, C-Loss: 1.9488548040390015\n",
      "Epoch: 15, Loss: 19.573967933654785,  L-Loss: 0.20549316704273224, C-Loss: 1.9471221566200256\n",
      "Epoch: 16, Loss: 19.56460189819336,  L-Loss: 0.19875981658697128, C-Loss: 1.946522295475006\n",
      "Epoch: 17, Loss: 19.543070793151855,  L-Loss: 0.19439370930194855, C-Loss: 1.9445874094963074\n",
      "Epoch: 18, Loss: 19.536766052246094,  L-Loss: 0.19082646071910858, C-Loss: 1.944135308265686\n",
      "Epoch: 19, Loss: 19.52556037902832,  L-Loss: 0.18707166612148285, C-Loss: 1.943202555179596\n",
      "Epoch: 20, Loss: 19.50447368621826,  L-Loss: 0.18550892174243927, C-Loss: 1.9411718845367432\n",
      "Epoch: 21, Loss: 19.48852252960205,  L-Loss: 0.18721865117549896, C-Loss: 1.939491331577301\n",
      "Epoch: 22, Loss: 19.479707717895508,  L-Loss: 0.18759933859109879, C-Loss: 1.9385908246040344\n",
      "Epoch: 23, Loss: 19.4666109085083,  L-Loss: 0.18948854506015778, C-Loss: 1.9371867179870605\n",
      "Epoch: 24, Loss: 19.446171760559082,  L-Loss: 0.19117532670497894, C-Loss: 1.9350584149360657\n",
      "Epoch: 25, Loss: 19.440402030944824,  L-Loss: 0.19661659002304077, C-Loss: 1.9342093467712402\n",
      "Epoch: 26, Loss: 19.42021369934082,  L-Loss: 0.20387928932905197, C-Loss: 1.9318274855613708\n",
      "Epoch: 27, Loss: 19.411412239074707,  L-Loss: 0.20904870331287384, C-Loss: 1.930688738822937\n",
      "Epoch: 28, Loss: 19.394636154174805,  L-Loss: 0.22002042084932327, C-Loss: 1.928462564945221\n",
      "Epoch: 29, Loss: 19.389957427978516,  L-Loss: 0.2278371900320053, C-Loss: 1.9276039600372314\n",
      "Epoch: 30, Loss: 19.36199378967285,  L-Loss: 0.23629309237003326, C-Loss: 1.9243847131729126\n",
      "Epoch: 31, Loss: 19.345643997192383,  L-Loss: 0.2421894669532776, C-Loss: 1.9224548935890198\n",
      "Epoch: 32, Loss: 19.324735641479492,  L-Loss: 0.2503550946712494, C-Loss: 1.9199557900428772\n",
      "Epoch: 33, Loss: 19.308700561523438,  L-Loss: 0.26546235382556915, C-Loss: 1.9175969958305359\n",
      "Epoch: 34, Loss: 19.30687713623047,  L-Loss: 0.2890271544456482, C-Loss: 1.9162362813949585\n",
      "Epoch: 35, Loss: 19.293343544006348,  L-Loss: 0.31060363352298737, C-Loss: 1.9138041138648987\n",
      "Epoch: 36, Loss: 19.276824951171875,  L-Loss: 0.3285788595676422, C-Loss: 1.911253571510315\n",
      "Epoch: 37, Loss: 19.26106834411621,  L-Loss: 0.3441598564386368, C-Loss: 1.9088988900184631\n",
      "Epoch: 38, Loss: 19.23347568511963,  L-Loss: 0.36179277300834656, C-Loss: 1.905258059501648\n",
      "Epoch: 39, Loss: 19.21342658996582,  L-Loss: 0.3859981894493103, C-Loss: 1.9020427465438843\n",
      "Epoch: 40, Loss: 19.214613914489746,  L-Loss: 0.41930727660655975, C-Loss: 1.9004959464073181\n",
      "Epoch: 41, Loss: 19.18749237060547,  L-Loss: 0.4340588450431824, C-Loss: 1.8970463275909424\n",
      "Epoch: 42, Loss: 19.157203674316406,  L-Loss: 0.4563307911157608, C-Loss: 1.8929038047790527\n",
      "Epoch: 43, Loss: 19.159931182861328,  L-Loss: 0.48523983359336853, C-Loss: 1.8917310237884521\n",
      "Epoch: 44, Loss: 19.12889862060547,  L-Loss: 0.4969966262578964, C-Loss: 1.8880400657653809\n",
      "Epoch: 45, Loss: 19.109865188598633,  L-Loss: 0.5248269438743591, C-Loss: 1.884745180606842\n",
      "Epoch: 46, Loss: 19.090730667114258,  L-Loss: 0.5473022758960724, C-Loss: 1.8817079663276672\n",
      "Epoch: 47, Loss: 19.071213722229004,  L-Loss: 0.5822321176528931, C-Loss: 1.8780097961425781\n",
      "Epoch: 48, Loss: 19.057714462280273,  L-Loss: 0.6002155244350433, C-Loss: 1.8757606744766235\n",
      "Epoch: 49, Loss: 19.009222984313965,  L-Loss: 0.6037532091140747, C-Loss: 1.8707345724105835\n",
      "Epoch: 50, Loss: 18.96114444732666,  L-Loss: 0.6079705655574799, C-Loss: 1.8657159209251404\n",
      "Epoch: 51, Loss: 18.957186698913574,  L-Loss: 0.6255055665969849, C-Loss: 1.8644434213638306\n",
      "Epoch: 52, Loss: 18.95650005340576,  L-Loss: 0.660489946603775, C-Loss: 1.862625539302826\n",
      "Epoch: 53, Loss: 18.96547794342041,  L-Loss: 0.6819154322147369, C-Loss: 1.86245197057724\n",
      "Epoch: 54, Loss: 18.938036918640137,  L-Loss: 0.6771264672279358, C-Loss: 1.8599473237991333\n",
      "Epoch: 55, Loss: 18.880688667297363,  L-Loss: 0.6734746992588043, C-Loss: 1.8543951511383057\n",
      "Epoch: 56, Loss: 18.85260581970215,  L-Loss: 0.6842363178730011, C-Loss: 1.8510487079620361\n",
      "Epoch: 57, Loss: 18.80171012878418,  L-Loss: 0.7096633017063141, C-Loss: 1.8446878790855408\n",
      "Epoch: 58, Loss: 18.796480178833008,  L-Loss: 0.736912190914154, C-Loss: 1.8428024053573608\n",
      "Epoch: 59, Loss: 18.761225700378418,  L-Loss: 0.7633331716060638, C-Loss: 1.837955892086029\n",
      "Epoch: 60, Loss: 18.748802185058594,  L-Loss: 0.8005348443984985, C-Loss: 1.83485347032547\n",
      "Epoch: 61, Loss: 18.714606285095215,  L-Loss: 0.8000479936599731, C-Loss: 1.8314582109451294\n",
      "Epoch: 62, Loss: 18.688215255737305,  L-Loss: 0.7878353297710419, C-Loss: 1.8294298648834229\n",
      "Epoch: 63, Loss: 18.66556453704834,  L-Loss: 0.8193713128566742, C-Loss: 1.8255878686904907\n",
      "Epoch: 64, Loss: 18.678119659423828,  L-Loss: 0.8323946297168732, C-Loss: 1.8261921405792236\n",
      "Epoch: 65, Loss: 18.62638282775879,  L-Loss: 0.8350094854831696, C-Loss: 1.8208877444267273\n",
      "Epoch: 66, Loss: 18.617024421691895,  L-Loss: 0.8273700773715973, C-Loss: 1.8203339576721191\n",
      "Epoch: 67, Loss: 18.536340713500977,  L-Loss: 0.8209182322025299, C-Loss: 1.8125881552696228\n",
      "Epoch: 68, Loss: 18.535316467285156,  L-Loss: 0.8132095634937286, C-Loss: 1.8128710985183716\n",
      "Epoch: 69, Loss: 18.48465061187744,  L-Loss: 0.7929419279098511, C-Loss: 1.808817982673645\n",
      "Epoch: 70, Loss: 18.47494602203369,  L-Loss: 0.7933142781257629, C-Loss: 1.8078287839889526\n",
      "Epoch: 71, Loss: 18.455878257751465,  L-Loss: 0.8156459629535675, C-Loss: 1.8048053979873657\n",
      "Epoch: 72, Loss: 18.433252334594727,  L-Loss: 0.8371238708496094, C-Loss: 1.801469087600708\n",
      "Epoch: 73, Loss: 18.4354305267334,  L-Loss: 0.8486035168170929, C-Loss: 1.8011128306388855\n",
      "Epoch: 74, Loss: 18.40839385986328,  L-Loss: 0.842741072177887, C-Loss: 1.7987022995948792\n",
      "Epoch: 75, Loss: 18.387869834899902,  L-Loss: 0.8332111239433289, C-Loss: 1.7971265316009521\n",
      "Epoch: 76, Loss: 18.367527961730957,  L-Loss: 0.8497885465621948, C-Loss: 1.7942634224891663\n",
      "Epoch: 77, Loss: 18.35962677001953,  L-Loss: 0.8409186601638794, C-Loss: 1.7939167618751526\n",
      "Epoch: 78, Loss: 18.278196334838867,  L-Loss: 0.8207562267780304, C-Loss: 1.7867818474769592\n",
      "Epoch: 79, Loss: 18.318867683410645,  L-Loss: 0.83634814620018, C-Loss: 1.790069341659546\n",
      "Epoch: 80, Loss: 18.27678394317627,  L-Loss: 0.8434648811817169, C-Loss: 1.78550523519516\n",
      "Epoch: 81, Loss: 18.300572395324707,  L-Loss: 0.8593598008155823, C-Loss: 1.7870892882347107\n",
      "Epoch: 82, Loss: 18.29420566558838,  L-Loss: 0.8432705402374268, C-Loss: 1.7872571349143982\n",
      "Epoch: 83, Loss: 18.20677947998047,  L-Loss: 0.8300274312496185, C-Loss: 1.7791765332221985\n",
      "Epoch: 84, Loss: 18.2138090133667,  L-Loss: 0.8219365775585175, C-Loss: 1.7802841663360596\n",
      "Epoch: 85, Loss: 18.166123390197754,  L-Loss: 0.8647581040859222, C-Loss: 1.773374319076538\n",
      "Epoch: 86, Loss: 18.217137336730957,  L-Loss: 0.9009844362735748, C-Loss: 1.7766645550727844\n",
      "Epoch: 87, Loss: 18.193936347961426,  L-Loss: 0.9180673062801361, C-Loss: 1.7734902501106262\n",
      "Epoch: 88, Loss: 18.17966365814209,  L-Loss: 0.9116557836532593, C-Loss: 1.7723835706710815\n",
      "Epoch: 89, Loss: 18.148902893066406,  L-Loss: 0.8829599916934967, C-Loss: 1.7707422375679016\n",
      "Epoch: 90, Loss: 18.10787868499756,  L-Loss: 0.8602173328399658, C-Loss: 1.7677770853042603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Loss: 18.135315895080566,  L-Loss: 0.8720275461673737, C-Loss: 1.769930362701416\n",
      "Epoch: 92, Loss: 18.10778045654297,  L-Loss: 0.8818396925926208, C-Loss: 1.7666860222816467\n",
      "Epoch: 93, Loss: 18.13768482208252,  L-Loss: 0.8991117775440216, C-Loss: 1.7688128352165222\n",
      "Epoch: 94, Loss: 18.121724128723145,  L-Loss: 0.8953824043273926, C-Loss: 1.7674033641815186\n",
      "Epoch: 95, Loss: 18.04963493347168,  L-Loss: 0.8774243593215942, C-Loss: 1.7610922455787659\n",
      "Epoch: 96, Loss: 18.021714210510254,  L-Loss: 0.8655288517475128, C-Loss: 1.7588949799537659\n",
      "Epoch: 97, Loss: 18.006216049194336,  L-Loss: 0.857913613319397, C-Loss: 1.7577259540557861\n",
      "Epoch: 98, Loss: 17.967266082763672,  L-Loss: 0.8576495051383972, C-Loss: 1.753844141960144\n",
      "Epoch: 99, Loss: 17.99769878387451,  L-Loss: 0.8748101592063904, C-Loss: 1.7560294270515442\n",
      "Epoch: 100, Loss: 17.972054481506348,  L-Loss: 0.8724629878997803, C-Loss: 1.7535822987556458\n",
      "Epoch: 101, Loss: 17.959060668945312,  L-Loss: 0.8685941100120544, C-Loss: 1.7524763345718384\n",
      "Epoch: 102, Loss: 17.96781826019287,  L-Loss: 0.8690712451934814, C-Loss: 1.753328263759613\n",
      "Epoch: 103, Loss: 17.958961486816406,  L-Loss: 0.8694223761558533, C-Loss: 1.7524250149726868\n",
      "Epoch: 104, Loss: 17.950074195861816,  L-Loss: 0.8709803223609924, C-Loss: 1.751458466053009\n",
      "Epoch: 105, Loss: 17.95762538909912,  L-Loss: 0.8936851322650909, C-Loss: 1.7510783076286316\n",
      "Epoch: 106, Loss: 18.008278846740723,  L-Loss: 0.9035517871379852, C-Loss: 1.7556503415107727\n",
      "Epoch: 107, Loss: 17.94619369506836,  L-Loss: 0.89096599817276, C-Loss: 1.750071108341217\n",
      "Epoch: 108, Loss: 17.90406036376953,  L-Loss: 0.876262366771698, C-Loss: 1.7465928792953491\n",
      "Epoch: 109, Loss: 17.862585067749023,  L-Loss: 0.8609791696071625, C-Loss: 1.7432096004486084\n",
      "Epoch: 110, Loss: 17.906063079833984,  L-Loss: 0.9096662998199463, C-Loss: 1.745123028755188\n",
      "Epoch: 111, Loss: 17.963210105895996,  L-Loss: 0.9254052639007568, C-Loss: 1.7500509023666382\n",
      "Epoch: 112, Loss: 17.972899436950684,  L-Loss: 0.9127787947654724, C-Loss: 1.7516510486602783\n",
      "Epoch: 113, Loss: 17.88642692565918,  L-Loss: 0.8769088685512543, C-Loss: 1.7447972297668457\n",
      "Epoch: 114, Loss: 17.835835456848145,  L-Loss: 0.8710113167762756, C-Loss: 1.740032970905304\n",
      "Epoch: 115, Loss: 17.87838840484619,  L-Loss: 0.8809720277786255, C-Loss: 1.7437902092933655\n",
      "Epoch: 116, Loss: 17.896449089050293,  L-Loss: 0.8896535336971283, C-Loss: 1.7451621294021606\n",
      "Epoch: 117, Loss: 17.814940452575684,  L-Loss: 0.8758238554000854, C-Loss: 1.7377028465270996\n",
      "Epoch: 118, Loss: 17.815953254699707,  L-Loss: 0.8701084554195404, C-Loss: 1.7380899786949158\n",
      "Epoch: 119, Loss: 17.814764976501465,  L-Loss: 0.875600665807724, C-Loss: 1.7376964688301086\n",
      "Epoch: 120, Loss: 17.81676197052002,  L-Loss: 0.9070202112197876, C-Loss: 1.7363253235816956\n",
      "Epoch: 121, Loss: 17.80013084411621,  L-Loss: 0.9041300117969513, C-Loss: 1.7348065376281738\n",
      "Epoch: 122, Loss: 17.793442726135254,  L-Loss: 0.9120896756649017, C-Loss: 1.7337397933006287\n",
      "Epoch: 123, Loss: 17.84850788116455,  L-Loss: 0.931719571352005, C-Loss: 1.738264799118042\n",
      "Epoch: 124, Loss: 17.824524879455566,  L-Loss: 0.9367460310459137, C-Loss: 1.7356151938438416\n",
      "Epoch: 125, Loss: 17.80511474609375,  L-Loss: 0.9460155665874481, C-Loss: 1.7332106232643127\n",
      "Epoch: 126, Loss: 17.82596492767334,  L-Loss: 0.9341705739498138, C-Loss: 1.7358878254890442\n",
      "Epoch: 127, Loss: 17.797602653503418,  L-Loss: 0.9280202090740204, C-Loss: 1.7333592772483826\n",
      "Epoch: 128, Loss: 17.826682090759277,  L-Loss: 0.9591332972049713, C-Loss: 1.7347115874290466\n",
      "Epoch: 129, Loss: 17.8326358795166,  L-Loss: 0.9480141699314117, C-Loss: 1.7358629703521729\n",
      "Epoch: 130, Loss: 17.77004623413086,  L-Loss: 0.943565845489502, C-Loss: 1.7298263907432556\n",
      "Epoch: 131, Loss: 17.702404975891113,  L-Loss: 0.9347333312034607, C-Loss: 1.7235038876533508\n",
      "Epoch: 132, Loss: 17.73487949371338,  L-Loss: 0.9417217075824738, C-Loss: 1.726401925086975\n",
      "Epoch: 133, Loss: 17.712424278259277,  L-Loss: 0.9520381987094879, C-Loss: 1.7236405611038208\n",
      "Epoch: 134, Loss: 17.713327407836914,  L-Loss: 0.9669356346130371, C-Loss: 1.7229859232902527\n",
      "Epoch: 135, Loss: 17.691353797912598,  L-Loss: 0.9860145151615143, C-Loss: 1.7198346853256226\n",
      "Epoch: 136, Loss: 17.69329261779785,  L-Loss: 0.9796555638313293, C-Loss: 1.7203463912010193\n",
      "Epoch: 137, Loss: 17.718795776367188,  L-Loss: 0.9698653221130371, C-Loss: 1.7233863472938538\n",
      "Epoch: 138, Loss: 17.69862937927246,  L-Loss: 0.9673033058643341, C-Loss: 1.7214977145195007\n",
      "Epoch: 139, Loss: 17.765299797058105,  L-Loss: 0.985987663269043, C-Loss: 1.7272304892539978\n",
      "Epoch: 140, Loss: 17.805858612060547,  L-Loss: 0.9880207180976868, C-Loss: 1.7311848402023315\n",
      "Epoch: 141, Loss: 17.716371536254883,  L-Loss: 0.9958659708499908, C-Loss: 1.7218438386917114\n",
      "Epoch: 142, Loss: 17.730481147766113,  L-Loss: 1.0141016840934753, C-Loss: 1.7223429679870605\n",
      "Epoch: 143, Loss: 17.763705253601074,  L-Loss: 1.0274150371551514, C-Loss: 1.7249998450279236\n",
      "Epoch: 144, Loss: 17.68453598022461,  L-Loss: 1.0331190824508667, C-Loss: 1.7167975902557373\n",
      "Epoch: 145, Loss: 17.716678619384766,  L-Loss: 1.0234642028808594, C-Loss: 1.7204947471618652\n",
      "Epoch: 146, Loss: 17.749576568603516,  L-Loss: 1.0326345562934875, C-Loss: 1.7233259677886963\n",
      "Epoch: 147, Loss: 17.761917114257812,  L-Loss: 1.0419707894325256, C-Loss: 1.72409325838089\n",
      "Epoch: 148, Loss: 17.712790489196777,  L-Loss: 1.0410621762275696, C-Loss: 1.7192260026931763\n",
      "Epoch: 149, Loss: 17.646821975708008,  L-Loss: 1.0448858737945557, C-Loss: 1.7124378681182861\n",
      "Epoch: 150, Loss: 17.70882797241211,  L-Loss: 1.0568845868110657, C-Loss: 1.7180384993553162\n",
      "Epoch: 151, Loss: 17.72499179840088,  L-Loss: 1.0683254599571228, C-Loss: 1.7190828323364258\n",
      "Epoch: 152, Loss: 17.70818042755127,  L-Loss: 1.0723686814308167, C-Loss: 1.7171996235847473\n",
      "Epoch: 153, Loss: 17.77513885498047,  L-Loss: 1.080923616886139, C-Loss: 1.723467767238617\n",
      "Epoch: 154, Loss: 17.710909843444824,  L-Loss: 1.0712050199508667, C-Loss: 1.7175307869911194\n",
      "Epoch: 155, Loss: 17.661243438720703,  L-Loss: 1.0621978640556335, C-Loss: 1.713014543056488\n",
      "Epoch: 156, Loss: 17.67768669128418,  L-Loss: 1.0721485614776611, C-Loss: 1.714161217212677\n",
      "Epoch: 157, Loss: 17.815272331237793,  L-Loss: 1.105525553226471, C-Loss: 1.7262511253356934\n",
      "Epoch: 158, Loss: 17.827957153320312,  L-Loss: 1.1252671480178833, C-Loss: 1.7265323996543884\n",
      "Epoch: 159, Loss: 17.78754425048828,  L-Loss: 1.1002692580223083, C-Loss: 1.7237409949302673\n",
      "Epoch: 160, Loss: 17.686546325683594,  L-Loss: 1.0985507369041443, C-Loss: 1.7137271165847778\n",
      "Epoch: 161, Loss: 17.711840629577637,  L-Loss: 1.1195200383663177, C-Loss: 1.7152080535888672\n",
      "Epoch: 162, Loss: 17.759798049926758,  L-Loss: 1.1218629479408264, C-Loss: 1.7198867201805115\n",
      "Epoch: 163, Loss: 17.748990058898926,  L-Loss: 1.12527996301651, C-Loss: 1.7186350226402283\n",
      "Epoch: 164, Loss: 17.740978240966797,  L-Loss: 1.1142080426216125, C-Loss: 1.7183874249458313\n",
      "Epoch: 165, Loss: 17.697680473327637,  L-Loss: 1.0951746702194214, C-Loss: 1.715009331703186\n",
      "Epoch: 166, Loss: 17.64732837677002,  L-Loss: 1.087428867816925, C-Loss: 1.7103614807128906\n",
      "Epoch: 167, Loss: 17.711509704589844,  L-Loss: 1.0911117196083069, C-Loss: 1.7165954113006592\n",
      "Epoch: 168, Loss: 17.754676818847656,  L-Loss: 1.1004836559295654, C-Loss: 1.7204434871673584\n",
      "Epoch: 169, Loss: 17.66720676422119,  L-Loss: 1.1169037818908691, C-Loss: 1.7108754515647888\n",
      "Epoch: 170, Loss: 17.70669651031494,  L-Loss: 1.1207629442214966, C-Loss: 1.7146314978599548\n",
      "Epoch: 171, Loss: 17.76482391357422,  L-Loss: 1.1220605373382568, C-Loss: 1.72037935256958\n",
      "Epoch: 172, Loss: 17.7359037399292,  L-Loss: 1.117358386516571, C-Loss: 1.717722475528717\n",
      "Epoch: 173, Loss: 17.711983680725098,  L-Loss: 1.1117203831672668, C-Loss: 1.7156123518943787\n",
      "Epoch: 174, Loss: 17.65142059326172,  L-Loss: 1.1134754419326782, C-Loss: 1.7094682455062866\n",
      "Epoch: 175, Loss: 17.701749801635742,  L-Loss: 1.1362348198890686, C-Loss: 1.713363230228424\n",
      "Epoch: 176, Loss: 17.638081550598145,  L-Loss: 1.1496462225914001, C-Loss: 1.7063258290290833\n",
      "Epoch: 177, Loss: 17.652575492858887,  L-Loss: 1.1608365774154663, C-Loss: 1.7072157263755798\n",
      "Epoch: 178, Loss: 17.699334144592285,  L-Loss: 1.1543782353401184, C-Loss: 1.7122145295143127\n",
      "Epoch: 179, Loss: 17.673322677612305,  L-Loss: 1.1361807584762573, C-Loss: 1.710523247718811\n",
      "Epoch: 180, Loss: 17.573650360107422,  L-Loss: 1.116007924079895, C-Loss: 1.701564610004425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181, Loss: 17.584970474243164,  L-Loss: 1.1181877851486206, C-Loss: 1.7025877237319946\n",
      "Epoch: 182, Loss: 17.67875385284424,  L-Loss: 1.1426778435707092, C-Loss: 1.7107415199279785\n",
      "Epoch: 183, Loss: 17.737385749816895,  L-Loss: 1.1806519627571106, C-Loss: 1.7147060632705688\n",
      "Epoch: 184, Loss: 17.736090660095215,  L-Loss: 1.1637861728668213, C-Loss: 1.7154197692871094\n",
      "Epoch: 185, Loss: 17.660823822021484,  L-Loss: 1.154308259487152, C-Loss: 1.7083669304847717\n",
      "Epoch: 186, Loss: 17.63420867919922,  L-Loss: 1.1677266955375671, C-Loss: 1.7050345540046692\n",
      "Epoch: 187, Loss: 17.690606117248535,  L-Loss: 1.1933696269989014, C-Loss: 1.7093921303749084\n",
      "Epoch: 188, Loss: 17.772869110107422,  L-Loss: 1.2239473462104797, C-Loss: 1.7160894870758057\n",
      "Epoch: 189, Loss: 17.720450401306152,  L-Loss: 1.2194958925247192, C-Loss: 1.71107017993927\n",
      "Epoch: 190, Loss: 17.806336402893066,  L-Loss: 1.2291457056999207, C-Loss: 1.7191762924194336\n",
      "Epoch: 191, Loss: 17.789978981018066,  L-Loss: 1.2272510528564453, C-Loss: 1.7176353335380554\n",
      "Epoch: 192, Loss: 17.59059429168701,  L-Loss: 1.1992750763893127, C-Loss: 1.6990957260131836\n",
      "Epoch: 193, Loss: 17.645002365112305,  L-Loss: 1.2085407376289368, C-Loss: 1.7040732502937317\n",
      "Epoch: 194, Loss: 17.780244827270508,  L-Loss: 1.2197548747062683, C-Loss: 1.7170367240905762\n",
      "Epoch: 195, Loss: 17.70277214050293,  L-Loss: 1.2234005331993103, C-Loss: 1.7091072797775269\n",
      "Epoch: 196, Loss: 17.778435707092285,  L-Loss: 1.2436420321464539, C-Loss: 1.7156614661216736\n",
      "Epoch: 197, Loss: 17.74535369873047,  L-Loss: 1.2445250153541565, C-Loss: 1.7123090028762817\n",
      "Epoch: 198, Loss: 17.800134658813477,  L-Loss: 1.2585013508796692, C-Loss: 1.7170882821083069\n",
      "Epoch: 199, Loss: 17.850374221801758,  L-Loss: 1.284202754497528, C-Loss: 1.720827341079712\n",
      "Epoch: 200, Loss: 17.735987663269043,  L-Loss: 1.275290310382843, C-Loss: 1.709834337234497\n",
      "Epoch: 201, Loss: 17.76925563812256,  L-Loss: 1.276697814464569, C-Loss: 1.7130905389785767\n",
      "Epoch: 202, Loss: 17.79507064819336,  L-Loss: 1.2768189311027527, C-Loss: 1.715666115283966\n",
      "Epoch: 203, Loss: 17.687804222106934,  L-Loss: 1.2730299234390259, C-Loss: 1.7051289081573486\n",
      "Epoch: 204, Loss: 17.62568187713623,  L-Loss: 1.2464025020599365, C-Loss: 1.700248122215271\n",
      "Epoch: 205, Loss: 17.65975284576416,  L-Loss: 1.2539337873458862, C-Loss: 1.7032785415649414\n",
      "Epoch: 206, Loss: 17.625934600830078,  L-Loss: 1.2672325372695923, C-Loss: 1.6992318630218506\n",
      "Epoch: 207, Loss: 17.64922332763672,  L-Loss: 1.2709740996360779, C-Loss: 1.7013735175132751\n",
      "Epoch: 208, Loss: 17.693238258361816,  L-Loss: 1.2825440764427185, C-Loss: 1.705196738243103\n",
      "Epoch: 209, Loss: 17.68816089630127,  L-Loss: 1.289498507976532, C-Loss: 1.704341173171997\n",
      "Epoch: 210, Loss: 17.72992515563965,  L-Loss: 1.3079477548599243, C-Loss: 1.7075952291488647\n",
      "Epoch: 211, Loss: 17.72791290283203,  L-Loss: 1.3143091201782227, C-Loss: 1.7070759534835815\n",
      "Epoch: 212, Loss: 17.74828052520752,  L-Loss: 1.343802034854889, C-Loss: 1.7076378464698792\n",
      "Epoch: 213, Loss: 17.750250816345215,  L-Loss: 1.3455345630645752, C-Loss: 1.7077484130859375\n",
      "Epoch: 214, Loss: 17.725714683532715,  L-Loss: 1.3400945663452148, C-Loss: 1.7055667042732239\n",
      "Epoch: 215, Loss: 17.659945487976074,  L-Loss: 1.3266530632972717, C-Loss: 1.699661910533905\n",
      "Epoch: 216, Loss: 17.74523162841797,  L-Loss: 1.3269309997558594, C-Loss: 1.7081766724586487\n",
      "Epoch: 217, Loss: 17.72417163848877,  L-Loss: 1.3342443704605103, C-Loss: 1.7057050466537476\n",
      "Epoch: 218, Loss: 17.74696636199951,  L-Loss: 1.3399099111557007, C-Loss: 1.7077012062072754\n",
      "Epoch: 219, Loss: 17.696073532104492,  L-Loss: 1.3397790789604187, C-Loss: 1.702618420124054\n",
      "Epoch: 220, Loss: 17.703060150146484,  L-Loss: 1.3550969958305359, C-Loss: 1.7025511264801025\n",
      "Epoch: 221, Loss: 17.67865562438965,  L-Loss: 1.3556424379348755, C-Loss: 1.7000834345817566\n",
      "Epoch: 222, Loss: 17.675103187561035,  L-Loss: 1.355194091796875, C-Loss: 1.6997506618499756\n",
      "Epoch: 223, Loss: 17.630993843078613,  L-Loss: 1.347062587738037, C-Loss: 1.69574636220932\n",
      "Epoch: 224, Loss: 17.70487403869629,  L-Loss: 1.3632088899612427, C-Loss: 1.7023269534111023\n",
      "Epoch: 225, Loss: 17.795968055725098,  L-Loss: 1.372549295425415, C-Loss: 1.7109692692756653\n",
      "Epoch: 226, Loss: 17.680163383483887,  L-Loss: 1.370675802230835, C-Loss: 1.6994826793670654\n",
      "Epoch: 227, Loss: 17.749577522277832,  L-Loss: 1.3841551542282104, C-Loss: 1.705750048160553\n",
      "Epoch: 228, Loss: 17.773484230041504,  L-Loss: 1.3887708187103271, C-Loss: 1.707909882068634\n",
      "Epoch: 229, Loss: 17.80238437652588,  L-Loss: 1.3935177326202393, C-Loss: 1.7105624675750732\n",
      "Epoch: 230, Loss: 17.833877563476562,  L-Loss: 1.4192494750022888, C-Loss: 1.7124253511428833\n",
      "Epoch: 231, Loss: 17.715123176574707,  L-Loss: 1.4071040153503418, C-Loss: 1.7011572122573853\n",
      "Epoch: 232, Loss: 17.881308555603027,  L-Loss: 1.4128217697143555, C-Loss: 1.7174896597862244\n",
      "Epoch: 233, Loss: 17.77629566192627,  L-Loss: 1.4186460971832275, C-Loss: 1.7066972851753235\n",
      "Epoch: 234, Loss: 17.819199562072754,  L-Loss: 1.439532220363617, C-Loss: 1.7099432945251465\n",
      "Epoch: 235, Loss: 17.873059272766113,  L-Loss: 1.4467467069625854, C-Loss: 1.7149685621261597\n",
      "Epoch: 236, Loss: 17.803850173950195,  L-Loss: 1.4622634053230286, C-Loss: 1.7072718143463135\n",
      "Epoch: 237, Loss: 17.853187561035156,  L-Loss: 1.4483577609062195, C-Loss: 1.7129008769989014\n",
      "Epoch: 238, Loss: 17.9095458984375,  L-Loss: 1.453868329524994, C-Loss: 1.718261182308197\n",
      "Epoch: 239, Loss: 17.809303283691406,  L-Loss: 1.4529439806938171, C-Loss: 1.7082830667495728\n",
      "Epoch: 240, Loss: 17.835185050964355,  L-Loss: 1.457418441772461, C-Loss: 1.7106476426124573\n",
      "Epoch: 241, Loss: 17.934194564819336,  L-Loss: 1.49308180809021, C-Loss: 1.718765377998352\n",
      "Epoch: 242, Loss: 17.97806167602539,  L-Loss: 1.5155235528945923, C-Loss: 1.722029983997345\n",
      "Epoch: 243, Loss: 17.96619701385498,  L-Loss: 1.5043818354606628, C-Loss: 1.7214006781578064\n",
      "Epoch: 244, Loss: 17.778223991394043,  L-Loss: 1.499242126941681, C-Loss: 1.7028602957725525\n",
      "Epoch: 245, Loss: 17.951330184936523,  L-Loss: 1.4924489259719849, C-Loss: 1.7205105423927307\n",
      "Epoch: 246, Loss: 18.036608695983887,  L-Loss: 1.4964772462844849, C-Loss: 1.728837013244629\n",
      "Epoch: 247, Loss: 17.85815143585205,  L-Loss: 1.4908562898635864, C-Loss: 1.7112723588943481\n",
      "Epoch: 248, Loss: 17.660264015197754,  L-Loss: 1.4838407635688782, C-Loss: 1.6918343901634216\n",
      "Epoch: 249, Loss: 17.75661849975586,  L-Loss: 1.492023229598999, C-Loss: 1.7010606527328491\n",
      "Epoch: 250, Loss: 17.934317588806152,  L-Loss: 1.5134641528129578, C-Loss: 1.7177584767341614\n",
      "Epoch: 251, Loss: 17.938800811767578,  L-Loss: 1.5170460939407349, C-Loss: 1.7180277109146118\n",
      "Epoch: 252, Loss: 17.84733295440674,  L-Loss: 1.5143381357192993, C-Loss: 1.7090163826942444\n",
      "Epoch: 253, Loss: 17.85209560394287,  L-Loss: 1.5086905360221863, C-Loss: 1.7097749710083008\n",
      "Epoch: 254, Loss: 17.736586570739746,  L-Loss: 1.4991838932037354, C-Loss: 1.6986994743347168\n",
      "Epoch: 255, Loss: 17.865468978881836,  L-Loss: 1.502848505973816, C-Loss: 1.7114043831825256\n",
      "Epoch: 256, Loss: 17.886547088623047,  L-Loss: 1.5314651131629944, C-Loss: 1.712081491947174\n",
      "Epoch: 257, Loss: 17.96462345123291,  L-Loss: 1.5435127019882202, C-Loss: 1.7192866802215576\n",
      "Epoch: 258, Loss: 18.18340301513672,  L-Loss: 1.5803333520889282, C-Loss: 1.7393237352371216\n",
      "Epoch: 259, Loss: 18.047619819641113,  L-Loss: 1.5627954602241516, C-Loss: 1.726622223854065\n",
      "Epoch: 260, Loss: 18.086283683776855,  L-Loss: 1.590360939502716, C-Loss: 1.729110300540924\n",
      "Epoch: 261, Loss: 18.14307689666748,  L-Loss: 1.6030307412147522, C-Loss: 1.7341561913490295\n",
      "Epoch: 262, Loss: 18.04011821746826,  L-Loss: 1.5666226148605347, C-Loss: 1.7256807088851929\n",
      "Epoch: 263, Loss: 17.953059196472168,  L-Loss: 1.5307981371879578, C-Loss: 1.7187660336494446\n",
      "Epoch: 264, Loss: 17.87580680847168,  L-Loss: 1.5144977569580078, C-Loss: 1.7118558287620544\n",
      "Epoch: 265, Loss: 17.879626274108887,  L-Loss: 1.5306212902069092, C-Loss: 1.711431622505188\n",
      "Epoch: 266, Loss: 17.89309024810791,  L-Loss: 1.5525338649749756, C-Loss: 1.711682379245758\n",
      "Epoch: 267, Loss: 17.98916721343994,  L-Loss: 1.5580217242240906, C-Loss: 1.7210156321525574\n",
      "Epoch: 268, Loss: 17.93855857849121,  L-Loss: 1.5484333038330078, C-Loss: 1.7164341807365417\n",
      "Epoch: 269, Loss: 18.00715160369873,  L-Loss: 1.5444613695144653, C-Loss: 1.7234919667243958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270, Loss: 17.912824630737305,  L-Loss: 1.53586345911026, C-Loss: 1.7144892811775208\n",
      "Epoch: 271, Loss: 17.94023895263672,  L-Loss: 1.5447232723236084, C-Loss: 1.7167877554893494\n",
      "Epoch: 272, Loss: 18.045594215393066,  L-Loss: 1.5573486685752869, C-Loss: 1.726692020893097\n",
      "Epoch: 273, Loss: 17.975488662719727,  L-Loss: 1.5589065551757812, C-Loss: 1.7196035385131836\n",
      "Epoch: 274, Loss: 18.162985801696777,  L-Loss: 1.5899894833564758, C-Loss: 1.7367991209030151\n",
      "Epoch: 275, Loss: 18.157248497009277,  L-Loss: 1.5645978450775146, C-Loss: 1.7374950051307678\n",
      "Epoch: 276, Loss: 17.998634338378906,  L-Loss: 1.5608835220336914, C-Loss: 1.7218191623687744\n",
      "Epoch: 277, Loss: 17.927412033081055,  L-Loss: 1.5463794469833374, C-Loss: 1.7154222130775452\n",
      "Epoch: 278, Loss: 17.94078826904297,  L-Loss: 1.5746793150901794, C-Loss: 1.7153448462486267\n",
      "Epoch: 279, Loss: 18.27897357940674,  L-Loss: 1.5918827056884766, C-Loss: 1.748303234577179\n",
      "Epoch: 280, Loss: 17.973959922790527,  L-Loss: 1.5718046426773071, C-Loss: 1.718805730342865\n",
      "Epoch: 281, Loss: 17.772194862365723,  L-Loss: 1.5455141067504883, C-Loss: 1.699943721294403\n",
      "Epoch: 282, Loss: 18.17187786102295,  L-Loss: 1.5910893678665161, C-Loss: 1.7376332879066467\n",
      "Epoch: 283, Loss: 18.348267555236816,  L-Loss: 1.6253587007522583, C-Loss: 1.753558874130249\n",
      "Epoch: 284, Loss: 18.252629280090332,  L-Loss: 1.589022696018219, C-Loss: 1.7458118796348572\n",
      "Epoch: 285, Loss: 18.210996627807617,  L-Loss: 1.5781769752502441, C-Loss: 1.7421907186508179\n",
      "Epoch: 286, Loss: 18.324942588806152,  L-Loss: 1.5935620665550232, C-Loss: 1.7528162002563477\n",
      "Epoch: 287, Loss: 18.326810836791992,  L-Loss: 1.6163791418075562, C-Loss: 1.7518622279167175\n",
      "Epoch: 288, Loss: 18.122227668762207,  L-Loss: 1.5959753394126892, C-Loss: 1.7324239015579224\n",
      "Epoch: 289, Loss: 18.31680393218994,  L-Loss: 1.586652934551239, C-Loss: 1.7523477673530579\n",
      "Epoch: 290, Loss: 18.132585525512695,  L-Loss: 1.575384795665741, C-Loss: 1.7344893217086792\n",
      "Epoch: 291, Loss: 18.12660026550293,  L-Loss: 1.5660773515701294, C-Loss: 1.734356164932251\n",
      "Epoch: 292, Loss: 18.037694931030273,  L-Loss: 1.5783257484436035, C-Loss: 1.7248532176017761\n",
      "Epoch: 293, Loss: 18.29144287109375,  L-Loss: 1.5976092219352722, C-Loss: 1.749263882637024\n",
      "Epoch: 294, Loss: 18.185033798217773,  L-Loss: 1.5918535590171814, C-Loss: 1.738910734653473\n",
      "Epoch: 295, Loss: 18.154647827148438,  L-Loss: 1.5998021960258484, C-Loss: 1.7354746460914612\n",
      "Epoch: 296, Loss: 18.29291820526123,  L-Loss: 1.597520887851715, C-Loss: 1.7494157552719116\n",
      "Epoch: 297, Loss: 18.13125514984131,  L-Loss: 1.6223326325416565, C-Loss: 1.732008934020996\n",
      "Epoch: 298, Loss: 18.225825309753418,  L-Loss: 1.605792224407196, C-Loss: 1.742292881011963\n",
      "Epoch: 299, Loss: 18.225050926208496,  L-Loss: 1.5929579138755798, C-Loss: 1.742857277393341\n",
      "Epoch: 300, Loss: 18.16160774230957,  L-Loss: 1.583354890346527, C-Loss: 1.7369930744171143\n",
      "Epoch: 301, Loss: 18.195979118347168,  L-Loss: 1.5865159630775452, C-Loss: 1.7402721047401428\n",
      "Epoch: 302, Loss: 18.232213973999023,  L-Loss: 1.621448814868927, C-Loss: 1.7421488165855408\n",
      "Epoch: 303, Loss: 18.194249153137207,  L-Loss: 1.6156125664710999, C-Loss: 1.7386442422866821\n",
      "Epoch: 304, Loss: 18.345178604125977,  L-Loss: 1.6305123567581177, C-Loss: 1.7529922723770142\n",
      "Epoch: 305, Loss: 18.450298309326172,  L-Loss: 1.6415054202079773, C-Loss: 1.7629545331001282\n",
      "Epoch: 306, Loss: 18.576380729675293,  L-Loss: 1.6474957466125488, C-Loss: 1.7752633094787598\n",
      "Epoch: 307, Loss: 18.258243560791016,  L-Loss: 1.6344678401947021, C-Loss: 1.7441009879112244\n",
      "Epoch: 308, Loss: 18.130525588989258,  L-Loss: 1.6136854887008667, C-Loss: 1.7323682308197021\n",
      "Epoch: 309, Loss: 18.206050872802734,  L-Loss: 1.5875096321105957, C-Loss: 1.74122953414917\n",
      "Epoch: 310, Loss: 18.21766948699951,  L-Loss: 1.5935840606689453, C-Loss: 1.7420877814292908\n",
      "Epoch: 311, Loss: 18.31208324432373,  L-Loss: 1.5820285081863403, C-Loss: 1.7521069049835205\n",
      "Epoch: 312, Loss: 18.24036693572998,  L-Loss: 1.5702446699142456, C-Loss: 1.7455244660377502\n",
      "Epoch: 313, Loss: 18.167768478393555,  L-Loss: 1.5705015063285828, C-Loss: 1.738251805305481\n",
      "Epoch: 314, Loss: 18.381443977355957,  L-Loss: 1.5842737555503845, C-Loss: 1.7589306235313416\n",
      "Epoch: 315, Loss: 18.217681884765625,  L-Loss: 1.5864745378494263, C-Loss: 1.7424444556236267\n",
      "Epoch: 316, Loss: 18.365103721618652,  L-Loss: 1.5871246457099915, C-Loss: 1.7571541666984558\n",
      "Epoch: 317, Loss: 18.338025093078613,  L-Loss: 1.5963671207427979, C-Loss: 1.7539840340614319\n",
      "Epoch: 318, Loss: 18.514514923095703,  L-Loss: 1.6153284907341003, C-Loss: 1.7706850171089172\n",
      "Epoch: 319, Loss: 18.4512357711792,  L-Loss: 1.6237925291061401, C-Loss: 1.7639340162277222\n",
      "Epoch: 320, Loss: 18.365816116333008,  L-Loss: 1.6173235774040222, C-Loss: 1.7557153701782227\n",
      "Epoch: 321, Loss: 18.43785858154297,  L-Loss: 1.6491491794586182, C-Loss: 1.7613284587860107\n",
      "Epoch: 322, Loss: 18.3453950881958,  L-Loss: 1.6328431963920593, C-Loss: 1.7528973817825317\n",
      "Epoch: 323, Loss: 18.45938491821289,  L-Loss: 1.617074966430664, C-Loss: 1.765084683895111\n",
      "Epoch: 324, Loss: 18.44582462310791,  L-Loss: 1.619271695613861, C-Loss: 1.7636188864707947\n",
      "Epoch: 325, Loss: 18.58693027496338,  L-Loss: 1.6424939036369324, C-Loss: 1.7765684127807617\n",
      "Epoch: 326, Loss: 19.131851196289062,  L-Loss: 1.6797270774841309, C-Loss: 1.8291987776756287\n",
      "Epoch: 327, Loss: 18.40355682373047,  L-Loss: 1.6551005840301514, C-Loss: 1.7576006650924683\n",
      "Epoch: 328, Loss: 18.475940704345703,  L-Loss: 1.6453640460968018, C-Loss: 1.765325903892517\n",
      "Epoch: 329, Loss: 18.290812492370605,  L-Loss: 1.6085564494132996, C-Loss: 1.7486533522605896\n",
      "Epoch: 330, Loss: 18.224153518676758,  L-Loss: 1.5937748551368713, C-Loss: 1.7427266240119934\n",
      "Epoch: 331, Loss: 18.297127723693848,  L-Loss: 1.6152909398078918, C-Loss: 1.7489482164382935\n",
      "Epoch: 332, Loss: 18.470487594604492,  L-Loss: 1.6495647430419922, C-Loss: 1.7645705938339233\n",
      "Epoch: 333, Loss: 18.53508758544922,  L-Loss: 1.642281413078308, C-Loss: 1.771394670009613\n",
      "Epoch: 334, Loss: 18.64107608795166,  L-Loss: 1.6313192248344421, C-Loss: 1.7825416326522827\n",
      "Epoch: 335, Loss: 18.553451538085938,  L-Loss: 1.6191882491111755, C-Loss: 1.7743857502937317\n",
      "Epoch: 336, Loss: 18.79020881652832,  L-Loss: 1.6297070980072021, C-Loss: 1.7975355386734009\n",
      "Epoch: 337, Loss: 18.638378143310547,  L-Loss: 1.616866111755371, C-Loss: 1.7829945087432861\n",
      "Epoch: 338, Loss: 18.23301410675049,  L-Loss: 1.62528657913208, C-Loss: 1.742037057876587\n",
      "Epoch: 339, Loss: 18.457249641418457,  L-Loss: 1.606868326663971, C-Loss: 1.7653815150260925\n",
      "Epoch: 340, Loss: 18.35581398010254,  L-Loss: 1.6132280230522156, C-Loss: 1.7549200057983398\n",
      "Epoch: 341, Loss: 18.370878219604492,  L-Loss: 1.6285020112991333, C-Loss: 1.755662739276886\n",
      "Epoch: 342, Loss: 18.92775249481201,  L-Loss: 1.6518464088439941, C-Loss: 1.8101829290390015\n",
      "Epoch: 343, Loss: 18.68576145172119,  L-Loss: 1.6406593918800354, C-Loss: 1.7865431308746338\n",
      "Epoch: 344, Loss: 18.559277534484863,  L-Loss: 1.6272639632225037, C-Loss: 1.774564504623413\n",
      "Epoch: 345, Loss: 18.59725284576416,  L-Loss: 1.6196072697639465, C-Loss: 1.778744876384735\n",
      "Epoch: 346, Loss: 18.16517448425293,  L-Loss: 1.5798085927963257, C-Loss: 1.7375270128250122\n",
      "Epoch: 347, Loss: 18.35033416748047,  L-Loss: 1.5996209383010864, C-Loss: 1.7550523281097412\n",
      "Epoch: 348, Loss: 18.774391174316406,  L-Loss: 1.6310675740242004, C-Loss: 1.7958857417106628\n",
      "Out of patience at epoch 348\n",
      "Guardando mejor modelo en  ..\\models\\C2AE_alexnet_retrained\\base\\0\\26L.pth\n",
      "HS fold 0: 0.2012\n",
      "Predicciones guardadas en ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\0\\predictions.csv\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Epoch: 0, Loss: 20.372907638549805,  L-Loss: 1.0568046569824219, C-Loss: 1.9844505190849304\n",
      "Epoch: 1, Loss: 20.246129035949707,  L-Loss: 0.8338810503482819, C-Loss: 1.9829188585281372\n",
      "Epoch: 2, Loss: 20.165888786315918,  L-Loss: 0.6787595450878143, C-Loss: 1.9826509952545166\n",
      "Epoch: 3, Loss: 20.094799041748047,  L-Loss: 0.5724351704120636, C-Loss: 1.9808582663536072\n",
      "Epoch: 4, Loss: 20.048466682434082,  L-Loss: 0.4969828575849533, C-Loss: 1.9799975156784058\n",
      "Epoch: 5, Loss: 20.027552604675293,  L-Loss: 0.4450117200613022, C-Loss: 1.9805046319961548\n",
      "Epoch: 6, Loss: 19.98805809020996,  L-Loss: 0.3985465168952942, C-Loss: 1.9788784980773926\n",
      "Epoch: 7, Loss: 19.967248916625977,  L-Loss: 0.36318568885326385, C-Loss: 1.9785656332969666\n",
      "Epoch: 8, Loss: 19.934786796569824,  L-Loss: 0.3320375978946686, C-Loss: 1.9768768548965454\n",
      "Epoch: 9, Loss: 19.912019729614258,  L-Loss: 0.3051200807094574, C-Loss: 1.9759460091590881\n",
      "Epoch: 10, Loss: 19.891019821166992,  L-Loss: 0.2852155417203903, C-Loss: 1.9748411774635315\n",
      "Epoch: 11, Loss: 19.876911163330078,  L-Loss: 0.2696543484926224, C-Loss: 1.9742083549499512\n",
      "Epoch: 12, Loss: 19.844780921936035,  L-Loss: 0.25348974019289017, C-Loss: 1.9718036651611328\n",
      "Epoch: 13, Loss: 19.827232360839844,  L-Loss: 0.24270476400852203, C-Loss: 1.9705879092216492\n",
      "Epoch: 14, Loss: 19.823697090148926,  L-Loss: 0.23445135354995728, C-Loss: 1.9706470966339111\n",
      "Epoch: 15, Loss: 19.80221462249756,  L-Loss: 0.22906386107206345, C-Loss: 1.9687681794166565\n",
      "Epoch: 16, Loss: 19.77938747406006,  L-Loss: 0.22285296767950058, C-Loss: 1.966796100139618\n",
      "Epoch: 17, Loss: 19.772150993347168,  L-Loss: 0.21814535558223724, C-Loss: 1.9663077592849731\n",
      "Epoch: 18, Loss: 19.75905418395996,  L-Loss: 0.2143687680363655, C-Loss: 1.9651869535446167\n",
      "Epoch: 19, Loss: 19.743245124816895,  L-Loss: 0.21338660269975662, C-Loss: 1.963655173778534\n",
      "Epoch: 20, Loss: 19.72865104675293,  L-Loss: 0.21238020062446594, C-Loss: 1.9622460007667542\n",
      "Epoch: 21, Loss: 19.714261054992676,  L-Loss: 0.2121364027261734, C-Loss: 1.9608192443847656\n",
      "Epoch: 22, Loss: 19.708581924438477,  L-Loss: 0.2148575335741043, C-Loss: 1.9601153135299683\n",
      "Epoch: 23, Loss: 19.695226669311523,  L-Loss: 0.21874690800905228, C-Loss: 1.958585262298584\n",
      "Epoch: 24, Loss: 19.669001579284668,  L-Loss: 0.22208622097969055, C-Loss: 1.9557957649230957\n",
      "Epoch: 25, Loss: 19.65324306488037,  L-Loss: 0.2262393906712532, C-Loss: 1.9540124535560608\n",
      "Epoch: 26, Loss: 19.635122299194336,  L-Loss: 0.2320256531238556, C-Loss: 1.9519108533859253\n",
      "Epoch: 27, Loss: 19.613658905029297,  L-Loss: 0.23994890600442886, C-Loss: 1.949368417263031\n",
      "Epoch: 28, Loss: 19.606718063354492,  L-Loss: 0.24741719663143158, C-Loss: 1.9483008980751038\n",
      "Epoch: 29, Loss: 19.573413848876953,  L-Loss: 0.2598443925380707, C-Loss: 1.9443491697311401\n",
      "Epoch: 30, Loss: 19.56329345703125,  L-Loss: 0.27476540207862854, C-Loss: 1.9425910115242004\n",
      "Epoch: 31, Loss: 19.54404926300049,  L-Loss: 0.28945058584213257, C-Loss: 1.9399323463439941\n",
      "Epoch: 32, Loss: 19.5277099609375,  L-Loss: 0.3054978549480438, C-Loss: 1.9374962449073792\n",
      "Epoch: 33, Loss: 19.509270668029785,  L-Loss: 0.3233790993690491, C-Loss: 1.9347580671310425\n",
      "Epoch: 34, Loss: 19.485288619995117,  L-Loss: 0.34537939727306366, C-Loss: 1.9312598705291748\n",
      "Epoch: 35, Loss: 19.468852043151855,  L-Loss: 0.37067656219005585, C-Loss: 1.9283514618873596\n",
      "Epoch: 36, Loss: 19.46616554260254,  L-Loss: 0.391522616147995, C-Loss: 1.9270403981208801\n",
      "Epoch: 37, Loss: 19.446269035339355,  L-Loss: 0.40772928297519684, C-Loss: 1.9242404103279114\n",
      "Epoch: 38, Loss: 19.414748191833496,  L-Loss: 0.4296269863843918, C-Loss: 1.9199934601783752\n",
      "Epoch: 39, Loss: 19.407543182373047,  L-Loss: 0.44925880432128906, C-Loss: 1.9182913899421692\n",
      "Epoch: 40, Loss: 19.38936138153076,  L-Loss: 0.46503356099128723, C-Loss: 1.9156845211982727\n",
      "Epoch: 41, Loss: 19.371224403381348,  L-Loss: 0.4852614551782608, C-Loss: 1.9128593802452087\n",
      "Epoch: 42, Loss: 19.360536575317383,  L-Loss: 0.5016293078660965, C-Loss: 1.9109721779823303\n",
      "Epoch: 43, Loss: 19.32277774810791,  L-Loss: 0.5125032961368561, C-Loss: 1.9066525101661682\n",
      "Epoch: 44, Loss: 19.304529190063477,  L-Loss: 0.5375466346740723, C-Loss: 1.903575599193573\n",
      "Epoch: 45, Loss: 19.284337997436523,  L-Loss: 0.560218334197998, C-Loss: 1.9004228711128235\n",
      "Epoch: 46, Loss: 19.230886459350586,  L-Loss: 0.5900816917419434, C-Loss: 1.8935846090316772\n",
      "Epoch: 47, Loss: 19.236248016357422,  L-Loss: 0.623654305934906, C-Loss: 1.892441987991333\n",
      "Epoch: 48, Loss: 19.208370208740234,  L-Loss: 0.6580752730369568, C-Loss: 1.8879331946372986\n",
      "Epoch: 49, Loss: 19.175251007080078,  L-Loss: 0.7004189491271973, C-Loss: 1.8825042247772217\n",
      "Epoch: 50, Loss: 19.160420417785645,  L-Loss: 0.7219873070716858, C-Loss: 1.8799425959587097\n",
      "Epoch: 51, Loss: 19.117993354797363,  L-Loss: 0.7291363179683685, C-Loss: 1.8753425478935242\n",
      "Epoch: 52, Loss: 19.088775634765625,  L-Loss: 0.7504513561725616, C-Loss: 1.8713549375534058\n",
      "Epoch: 53, Loss: 19.09096908569336,  L-Loss: 0.7663583755493164, C-Loss: 1.8707789182662964\n",
      "Epoch: 54, Loss: 19.040865898132324,  L-Loss: 0.7757236957550049, C-Loss: 1.8653004169464111\n",
      "Epoch: 55, Loss: 19.04055404663086,  L-Loss: 0.8017678558826447, C-Loss: 1.8639670014381409\n",
      "Epoch: 56, Loss: 18.98808002471924,  L-Loss: 0.8253517150878906, C-Loss: 1.8575404286384583\n",
      "Epoch: 57, Loss: 19.01674461364746,  L-Loss: 0.8591844439506531, C-Loss: 1.8587152361869812\n",
      "Epoch: 58, Loss: 18.936094284057617,  L-Loss: 0.863884836435318, C-Loss: 1.8504152297973633\n",
      "Epoch: 59, Loss: 18.935898780822754,  L-Loss: 0.8916980028152466, C-Loss: 1.849004864692688\n",
      "Epoch: 60, Loss: 18.912443161010742,  L-Loss: 0.9059662818908691, C-Loss: 1.845945954322815\n",
      "Epoch: 61, Loss: 18.868558883666992,  L-Loss: 0.9179306626319885, C-Loss: 1.8409592509269714\n",
      "Epoch: 62, Loss: 18.860828399658203,  L-Loss: 0.9319148361682892, C-Loss: 1.8394871354103088\n",
      "Epoch: 63, Loss: 18.864723205566406,  L-Loss: 0.9553590416908264, C-Loss: 1.8387044072151184\n",
      "Epoch: 64, Loss: 18.806669235229492,  L-Loss: 0.9644576907157898, C-Loss: 1.8324440121650696\n",
      "Epoch: 65, Loss: 18.774700164794922,  L-Loss: 0.9903964996337891, C-Loss: 1.8279501795768738\n",
      "Epoch: 66, Loss: 18.758930206298828,  L-Loss: 0.9839543104171753, C-Loss: 1.826695203781128\n",
      "Epoch: 67, Loss: 18.74380397796631,  L-Loss: 0.9818156361579895, C-Loss: 1.8252896070480347\n",
      "Epoch: 68, Loss: 18.693618774414062,  L-Loss: 0.9706654846668243, C-Loss: 1.82082861661911\n",
      "Epoch: 69, Loss: 18.667484283447266,  L-Loss: 0.9805880188941956, C-Loss: 1.8177189826965332\n",
      "Epoch: 70, Loss: 18.65669536590576,  L-Loss: 1.0066581070423126, C-Loss: 1.8153365850448608\n",
      "Epoch: 71, Loss: 18.656149864196777,  L-Loss: 1.032831609249115, C-Loss: 1.8139733672142029\n",
      "Epoch: 72, Loss: 18.611692428588867,  L-Loss: 1.0283229649066925, C-Loss: 1.8097531199455261\n",
      "Epoch: 73, Loss: 18.568191528320312,  L-Loss: 1.0292412340641022, C-Loss: 1.805357038974762\n",
      "Epoch: 74, Loss: 18.525818824768066,  L-Loss: 1.0270949006080627, C-Loss: 1.8012270331382751\n",
      "Epoch: 75, Loss: 18.48402690887451,  L-Loss: 1.0261830687522888, C-Loss: 1.7970935702323914\n",
      "Epoch: 76, Loss: 18.49203586578369,  L-Loss: 1.0107016861438751, C-Loss: 1.7986684441566467\n",
      "Epoch: 77, Loss: 18.439455032348633,  L-Loss: 0.9802815616130829, C-Loss: 1.794931411743164\n",
      "Epoch: 78, Loss: 18.414219856262207,  L-Loss: 0.9747713506221771, C-Loss: 1.7926834225654602\n",
      "Epoch: 79, Loss: 18.387826919555664,  L-Loss: 0.9785249531269073, C-Loss: 1.7898564338684082\n",
      "Epoch: 80, Loss: 18.443371772766113,  L-Loss: 0.982109546661377, C-Loss: 1.7952316999435425\n",
      "Epoch: 81, Loss: 18.408040046691895,  L-Loss: 0.9647165238857269, C-Loss: 1.7925682663917542\n",
      "Epoch: 82, Loss: 18.336478233337402,  L-Loss: 0.9710358083248138, C-Loss: 1.785095989704132\n",
      "Epoch: 83, Loss: 18.33846950531006,  L-Loss: 0.9870547652244568, C-Loss: 1.7844942212104797\n",
      "Epoch: 84, Loss: 18.318456649780273,  L-Loss: 0.9918131232261658, C-Loss: 1.7822549939155579\n",
      "Epoch: 85, Loss: 18.298324584960938,  L-Loss: 1.0055290460586548, C-Loss: 1.7795559763908386\n",
      "Epoch: 86, Loss: 18.24422550201416,  L-Loss: 1.0085342526435852, C-Loss: 1.7739957571029663\n",
      "Epoch: 87, Loss: 18.231350898742676,  L-Loss: 1.0228906869888306, C-Loss: 1.7719905972480774\n",
      "Epoch: 88, Loss: 18.2378568649292,  L-Loss: 1.0087782442569733, C-Loss: 1.7733468413352966\n",
      "Epoch: 89, Loss: 18.250191688537598,  L-Loss: 1.0087080895900726, C-Loss: 1.7745838165283203\n",
      "Epoch: 90, Loss: 18.241244316101074,  L-Loss: 1.0019980669021606, C-Loss: 1.7740244269371033\n",
      "Epoch: 91, Loss: 18.23982810974121,  L-Loss: 0.9972842037677765, C-Loss: 1.7741186022758484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss: 18.182626724243164,  L-Loss: 0.9944948256015778, C-Loss: 1.7685378789901733\n",
      "Epoch: 93, Loss: 18.25198745727539,  L-Loss: 0.9831632971763611, C-Loss: 1.776040494441986\n",
      "Epoch: 94, Loss: 18.23471450805664,  L-Loss: 0.9710892140865326, C-Loss: 1.7749168872833252\n",
      "Epoch: 95, Loss: 18.146475791931152,  L-Loss: 0.9680813550949097, C-Loss: 1.7662434577941895\n",
      "Epoch: 96, Loss: 18.133254051208496,  L-Loss: 0.9775029420852661, C-Loss: 1.7644502520561218\n",
      "Epoch: 97, Loss: 18.140713691711426,  L-Loss: 0.9896182417869568, C-Loss: 1.7645905017852783\n",
      "Epoch: 98, Loss: 18.117164611816406,  L-Loss: 0.988309234380722, C-Loss: 1.7623010873794556\n",
      "Epoch: 99, Loss: 18.11227798461914,  L-Loss: 0.9738709628582001, C-Loss: 1.7625342011451721\n",
      "Epoch: 100, Loss: 18.12287998199463,  L-Loss: 0.9746066629886627, C-Loss: 1.7635576725006104\n",
      "Epoch: 101, Loss: 18.093741416931152,  L-Loss: 0.9639358222484589, C-Loss: 1.7611773014068604\n",
      "Epoch: 102, Loss: 18.084320068359375,  L-Loss: 0.9568997621536255, C-Loss: 1.7605870366096497\n",
      "Epoch: 103, Loss: 18.08151912689209,  L-Loss: 0.9717685282230377, C-Loss: 1.7595635652542114\n",
      "Epoch: 104, Loss: 18.09422016143799,  L-Loss: 0.972697913646698, C-Loss: 1.7607871294021606\n",
      "Epoch: 105, Loss: 18.101778984069824,  L-Loss: 0.9651755392551422, C-Loss: 1.7619191408157349\n",
      "Epoch: 106, Loss: 18.0797176361084,  L-Loss: 0.9740673899650574, C-Loss: 1.759268343448639\n",
      "Epoch: 107, Loss: 18.05493450164795,  L-Loss: 0.9772955179214478, C-Loss: 1.756628692150116\n",
      "Epoch: 108, Loss: 18.04404926300049,  L-Loss: 0.9751651585102081, C-Loss: 1.7556466460227966\n",
      "Epoch: 109, Loss: 18.034473419189453,  L-Loss: 0.9725683927536011, C-Loss: 1.7548189163208008\n",
      "Epoch: 110, Loss: 18.00694465637207,  L-Loss: 0.974299281835556, C-Loss: 1.7519795298576355\n",
      "Epoch: 111, Loss: 18.02379035949707,  L-Loss: 0.9879383742809296, C-Loss: 1.7529820203781128\n",
      "Epoch: 112, Loss: 17.965116500854492,  L-Loss: 0.9871453642845154, C-Loss: 1.747154414653778\n",
      "Epoch: 113, Loss: 17.931666374206543,  L-Loss: 0.9862306118011475, C-Loss: 1.743855059146881\n",
      "Epoch: 114, Loss: 17.966768264770508,  L-Loss: 0.9932124316692352, C-Loss: 1.7470161318778992\n",
      "Epoch: 115, Loss: 17.983186721801758,  L-Loss: 1.003219723701477, C-Loss: 1.7481576800346375\n",
      "Epoch: 116, Loss: 17.961676597595215,  L-Loss: 0.9900125861167908, C-Loss: 1.7466670274734497\n",
      "Epoch: 117, Loss: 17.98104476928711,  L-Loss: 0.98192298412323, C-Loss: 1.7490084171295166\n",
      "Epoch: 118, Loss: 17.9608097076416,  L-Loss: 0.9751259386539459, C-Loss: 1.7473247051239014\n",
      "Epoch: 119, Loss: 17.958426475524902,  L-Loss: 0.9786391854286194, C-Loss: 1.7469106316566467\n",
      "Epoch: 120, Loss: 17.93711566925049,  L-Loss: 0.9814717173576355, C-Loss: 1.7446379661560059\n",
      "Epoch: 121, Loss: 17.969197273254395,  L-Loss: 0.9886668920516968, C-Loss: 1.7474863529205322\n",
      "Epoch: 122, Loss: 17.98304843902588,  L-Loss: 0.9992716014385223, C-Loss: 1.7483413219451904\n",
      "Epoch: 123, Loss: 17.93505573272705,  L-Loss: 0.9781433045864105, C-Loss: 1.7445983290672302\n",
      "Epoch: 124, Loss: 17.94132709503174,  L-Loss: 0.9711993932723999, C-Loss: 1.7455727458000183\n",
      "Epoch: 125, Loss: 17.96363639831543,  L-Loss: 0.9837642014026642, C-Loss: 1.747175395488739\n",
      "Epoch: 126, Loss: 17.935370445251465,  L-Loss: 0.9927868545055389, C-Loss: 1.7438976764678955\n",
      "Epoch: 127, Loss: 17.898883819580078,  L-Loss: 1.0020019114017487, C-Loss: 1.7397883534431458\n",
      "Epoch: 128, Loss: 17.893134117126465,  L-Loss: 1.010467916727066, C-Loss: 1.738789975643158\n",
      "Epoch: 129, Loss: 17.877524375915527,  L-Loss: 1.0039474666118622, C-Loss: 1.7375550270080566\n",
      "Epoch: 130, Loss: 17.892822265625,  L-Loss: 1.0008306205272675, C-Loss: 1.7392407059669495\n",
      "Epoch: 131, Loss: 17.877653121948242,  L-Loss: 0.992192804813385, C-Loss: 1.738155722618103\n",
      "Epoch: 132, Loss: 17.870834350585938,  L-Loss: 1.0039876103401184, C-Loss: 1.7368839383125305\n",
      "Epoch: 133, Loss: 17.912851333618164,  L-Loss: 1.0074518918991089, C-Loss: 1.7409125566482544\n",
      "Epoch: 134, Loss: 17.88821792602539,  L-Loss: 1.0132795572280884, C-Loss: 1.7381576895713806\n",
      "Epoch: 135, Loss: 17.854859352111816,  L-Loss: 1.006575733423233, C-Loss: 1.7351572513580322\n",
      "Epoch: 136, Loss: 17.8171443939209,  L-Loss: 1.0072487592697144, C-Loss: 1.7313519716262817\n",
      "Epoch: 137, Loss: 17.84009552001953,  L-Loss: 1.0133129954338074, C-Loss: 1.7333438992500305\n",
      "Epoch: 138, Loss: 17.820475578308105,  L-Loss: 1.0386239290237427, C-Loss: 1.730116367340088\n",
      "Epoch: 139, Loss: 17.80258560180664,  L-Loss: 1.0393741726875305, C-Loss: 1.7282898426055908\n",
      "Epoch: 140, Loss: 17.795970916748047,  L-Loss: 1.0164768695831299, C-Loss: 1.7287732362747192\n",
      "Epoch: 141, Loss: 17.818178176879883,  L-Loss: 1.0144861936569214, C-Loss: 1.7310935258865356\n",
      "Epoch: 142, Loss: 17.816421508789062,  L-Loss: 1.0323840081691742, C-Loss: 1.73002290725708\n",
      "Epoch: 143, Loss: 17.801469802856445,  L-Loss: 1.0382285714149475, C-Loss: 1.7282356023788452\n",
      "Epoch: 144, Loss: 17.813154220581055,  L-Loss: 1.0831648707389832, C-Loss: 1.7271571159362793\n",
      "Epoch: 145, Loss: 17.85763931274414,  L-Loss: 1.0848298072814941, C-Loss: 1.7315224409103394\n",
      "Epoch: 146, Loss: 17.861525535583496,  L-Loss: 1.0698951184749603, C-Loss: 1.7326577305793762\n",
      "Epoch: 147, Loss: 17.84739398956299,  L-Loss: 1.0740670561790466, C-Loss: 1.7310359477996826\n",
      "Epoch: 148, Loss: 17.831645965576172,  L-Loss: 1.0699109733104706, C-Loss: 1.7296690940856934\n",
      "Epoch: 149, Loss: 17.80209732055664,  L-Loss: 1.0598195791244507, C-Loss: 1.727218747138977\n",
      "Epoch: 150, Loss: 17.78593921661377,  L-Loss: 1.0666762590408325, C-Loss: 1.7252600193023682\n",
      "Epoch: 151, Loss: 17.75875473022461,  L-Loss: 1.0729520916938782, C-Loss: 1.7222278714179993\n",
      "Epoch: 152, Loss: 17.84021282196045,  L-Loss: 1.128004550933838, C-Loss: 1.727621078491211\n",
      "Epoch: 153, Loss: 17.812260627746582,  L-Loss: 1.1116482019424438, C-Loss: 1.7256437540054321\n",
      "Epoch: 154, Loss: 17.739849090576172,  L-Loss: 1.0843710899353027, C-Loss: 1.7197662591934204\n",
      "Epoch: 155, Loss: 17.76137638092041,  L-Loss: 1.0864794254302979, C-Loss: 1.7218136191368103\n",
      "Epoch: 156, Loss: 17.836092948913574,  L-Loss: 1.131004273891449, C-Loss: 1.7270590662956238\n",
      "Epoch: 157, Loss: 17.811985969543457,  L-Loss: 1.133921205997467, C-Loss: 1.7245025634765625\n",
      "Epoch: 158, Loss: 17.801400184631348,  L-Loss: 1.1475709676742554, C-Loss: 1.7227613925933838\n",
      "Epoch: 159, Loss: 17.825437545776367,  L-Loss: 1.1403786540031433, C-Loss: 1.7255248427391052\n",
      "Epoch: 160, Loss: 17.850266456604004,  L-Loss: 1.1328436136245728, C-Loss: 1.7283843755722046\n",
      "Epoch: 161, Loss: 17.893024444580078,  L-Loss: 1.152397334575653, C-Loss: 1.731682538986206\n",
      "Epoch: 162, Loss: 17.90237331390381,  L-Loss: 1.1533322930335999, C-Loss: 1.7325707077980042\n",
      "Epoch: 163, Loss: 17.90272045135498,  L-Loss: 1.1462807059288025, C-Loss: 1.7329580187797546\n",
      "Epoch: 164, Loss: 17.977274894714355,  L-Loss: 1.1600401997566223, C-Loss: 1.7397254705429077\n",
      "Epoch: 165, Loss: 17.926715850830078,  L-Loss: 1.1510068774223328, C-Loss: 1.7351211905479431\n",
      "Epoch: 166, Loss: 17.91214084625244,  L-Loss: 1.1627869009971619, C-Loss: 1.7330747246742249\n",
      "Epoch: 167, Loss: 17.9027681350708,  L-Loss: 1.1735152006149292, C-Loss: 1.7316011190414429\n",
      "Epoch: 168, Loss: 17.84969139099121,  L-Loss: 1.1776593923568726, C-Loss: 1.7260862588882446\n",
      "Epoch: 169, Loss: 17.831246376037598,  L-Loss: 1.168450653553009, C-Loss: 1.7247021198272705\n",
      "Epoch: 170, Loss: 17.794086456298828,  L-Loss: 1.1594302654266357, C-Loss: 1.7214370965957642\n",
      "Epoch: 171, Loss: 17.761021614074707,  L-Loss: 1.1696240305900574, C-Loss: 1.7176209092140198\n",
      "Epoch: 172, Loss: 17.794776916503906,  L-Loss: 1.175065279006958, C-Loss: 1.7207244634628296\n",
      "Epoch: 173, Loss: 17.80104351043701,  L-Loss: 1.1875121593475342, C-Loss: 1.7207286953926086\n",
      "Epoch: 174, Loss: 17.85436248779297,  L-Loss: 1.2211719751358032, C-Loss: 1.7243775725364685\n",
      "Epoch: 175, Loss: 17.846572875976562,  L-Loss: 1.2312737703323364, C-Loss: 1.723093569278717\n",
      "Epoch: 176, Loss: 17.822195053100586,  L-Loss: 1.2147520780563354, C-Loss: 1.7214819192886353\n",
      "Epoch: 177, Loss: 17.845389366149902,  L-Loss: 1.2031133770942688, C-Loss: 1.724383294582367\n",
      "Epoch: 178, Loss: 17.878478050231934,  L-Loss: 1.2192643880844116, C-Loss: 1.7268845438957214\n",
      "Epoch: 179, Loss: 17.889158248901367,  L-Loss: 1.2238162159919739, C-Loss: 1.7277249693870544\n",
      "Epoch: 180, Loss: 17.89530086517334,  L-Loss: 1.2276998162269592, C-Loss: 1.7281451225280762\n",
      "Epoch: 181, Loss: 17.948817253112793,  L-Loss: 1.2384303212165833, C-Loss: 1.7329602241516113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182, Loss: 17.884289741516113,  L-Loss: 1.2246532440185547, C-Loss: 1.7271962761878967\n",
      "Epoch: 183, Loss: 17.828475952148438,  L-Loss: 1.2256978750228882, C-Loss: 1.7215626239776611\n",
      "Epoch: 184, Loss: 17.82652187347412,  L-Loss: 1.2159411907196045, C-Loss: 1.7218552231788635\n",
      "Epoch: 185, Loss: 17.854763984680176,  L-Loss: 1.2302326560020447, C-Loss: 1.7239646911621094\n",
      "Epoch: 186, Loss: 17.842509269714355,  L-Loss: 1.25093674659729, C-Loss: 1.7217040657997131\n",
      "Epoch: 187, Loss: 17.926151275634766,  L-Loss: 1.2602208256721497, C-Loss: 1.7296040654182434\n",
      "Epoch: 188, Loss: 17.93083381652832,  L-Loss: 1.2543047070503235, C-Loss: 1.730368196964264\n",
      "Epoch: 189, Loss: 17.92807388305664,  L-Loss: 1.2480714321136475, C-Loss: 1.7304037809371948\n",
      "Epoch: 190, Loss: 17.823533058166504,  L-Loss: 1.219190001487732, C-Loss: 1.7213939428329468\n",
      "Epoch: 191, Loss: 17.83348560333252,  L-Loss: 1.2336003184318542, C-Loss: 1.7216686010360718\n",
      "Epoch: 192, Loss: 17.869040489196777,  L-Loss: 1.237563669681549, C-Loss: 1.7250258326530457\n",
      "Epoch: 193, Loss: 17.88661479949951,  L-Loss: 1.2363996505737305, C-Loss: 1.7268414497375488\n",
      "Epoch: 194, Loss: 17.76633071899414,  L-Loss: 1.2402480840682983, C-Loss: 1.7146205306053162\n",
      "Epoch: 195, Loss: 17.764521598815918,  L-Loss: 1.2304818630218506, C-Loss: 1.7149282097816467\n",
      "Epoch: 196, Loss: 17.827430725097656,  L-Loss: 1.2525039315223694, C-Loss: 1.7201178669929504\n",
      "Epoch: 197, Loss: 17.875581741333008,  L-Loss: 1.3045579195022583, C-Loss: 1.7223303318023682\n",
      "Epoch: 198, Loss: 17.88617515563965,  L-Loss: 1.291280746459961, C-Loss: 1.7240535020828247\n",
      "Epoch: 199, Loss: 17.836359977722168,  L-Loss: 1.235688030719757, C-Loss: 1.721851646900177\n",
      "Epoch: 200, Loss: 17.78508186340332,  L-Loss: 1.2107343673706055, C-Loss: 1.7179715037345886\n",
      "Epoch: 201, Loss: 17.76979351043701,  L-Loss: 1.2105724215507507, C-Loss: 1.7164507508277893\n",
      "Epoch: 202, Loss: 17.86006259918213,  L-Loss: 1.2553170919418335, C-Loss: 1.7232404947280884\n",
      "Epoch: 203, Loss: 17.94563865661621,  L-Loss: 1.292977750301361, C-Loss: 1.7299150228500366\n",
      "Epoch: 204, Loss: 17.844000816345215,  L-Loss: 1.2618552446365356, C-Loss: 1.7213072776794434\n",
      "Epoch: 205, Loss: 17.733657836914062,  L-Loss: 1.2325425744056702, C-Loss: 1.7117385864257812\n",
      "Epoch: 206, Loss: 17.751526832580566,  L-Loss: 1.2277159690856934, C-Loss: 1.713766872882843\n",
      "Epoch: 207, Loss: 17.791601181030273,  L-Loss: 1.2258498072624207, C-Loss: 1.7178677320480347\n",
      "Epoch: 208, Loss: 17.763861656188965,  L-Loss: 1.2418031692504883, C-Loss: 1.7142958641052246\n",
      "Epoch: 209, Loss: 17.85551929473877,  L-Loss: 1.2649532556533813, C-Loss: 1.7223044037818909\n",
      "Epoch: 210, Loss: 17.89739418029785,  L-Loss: 1.2660260796546936, C-Loss: 1.7264381051063538\n",
      "Epoch: 211, Loss: 17.933300018310547,  L-Loss: 1.2883115410804749, C-Loss: 1.728914499282837\n",
      "Epoch: 212, Loss: 17.95088291168213,  L-Loss: 1.3040288090705872, C-Loss: 1.7298868298530579\n",
      "Epoch: 213, Loss: 17.8490629196167,  L-Loss: 1.2843253016471863, C-Loss: 1.720690131187439\n",
      "Epoch: 214, Loss: 17.82343101501465,  L-Loss: 1.2695354223251343, C-Loss: 1.7188663482666016\n",
      "Epoch: 215, Loss: 17.882732391357422,  L-Loss: 1.2793425917625427, C-Loss: 1.7243061065673828\n",
      "Epoch: 216, Loss: 17.805588722229004,  L-Loss: 1.2718148827552795, C-Loss: 1.7169681191444397\n",
      "Epoch: 217, Loss: 17.80564594268799,  L-Loss: 1.2717073559761047, C-Loss: 1.7169792652130127\n",
      "Epoch: 218, Loss: 17.896668434143066,  L-Loss: 1.2772059440612793, C-Loss: 1.7258065938949585\n",
      "Epoch: 219, Loss: 17.893742561340332,  L-Loss: 1.265814185142517, C-Loss: 1.726083517074585\n",
      "Epoch: 220, Loss: 17.856386184692383,  L-Loss: 1.2446345090866089, C-Loss: 1.723406970500946\n",
      "Epoch: 221, Loss: 17.903042793273926,  L-Loss: 1.267332911491394, C-Loss: 1.7269375324249268\n",
      "Epoch: 222, Loss: 17.948230743408203,  L-Loss: 1.2771869897842407, C-Loss: 1.7309637665748596\n",
      "Epoch: 223, Loss: 17.845468521118164,  L-Loss: 1.2635002136230469, C-Loss: 1.721371829509735\n",
      "Epoch: 224, Loss: 17.896349906921387,  L-Loss: 1.273791491985321, C-Loss: 1.7259454131126404\n",
      "Epoch: 225, Loss: 17.85642719268799,  L-Loss: 1.269100844860077, C-Loss: 1.7221877574920654\n",
      "Epoch: 226, Loss: 17.98494815826416,  L-Loss: 1.3009203672409058, C-Loss: 1.7334488034248352\n",
      "Epoch: 227, Loss: 18.016101837158203,  L-Loss: 1.319094955921173, C-Loss: 1.735655426979065\n",
      "Epoch: 228, Loss: 17.989717483520508,  L-Loss: 1.3160113096237183, C-Loss: 1.7331711649894714\n",
      "Epoch: 229, Loss: 17.971253395080566,  L-Loss: 1.3045026659965515, C-Loss: 1.7319002151489258\n",
      "Epoch: 230, Loss: 17.969943046569824,  L-Loss: 1.2951591610908508, C-Loss: 1.7322363257408142\n",
      "Epoch: 231, Loss: 17.862720489501953,  L-Loss: 1.2668542265892029, C-Loss: 1.7229293584823608\n",
      "Epoch: 232, Loss: 17.804691314697266,  L-Loss: 1.2689788341522217, C-Loss: 1.7170202732086182\n",
      "Epoch: 233, Loss: 17.898963928222656,  L-Loss: 1.3073435425758362, C-Loss: 1.7245292663574219\n",
      "Epoch: 234, Loss: 17.93424892425537,  L-Loss: 1.3055968284606934, C-Loss: 1.7281451225280762\n",
      "Epoch: 235, Loss: 18.012532234191895,  L-Loss: 1.3110951781272888, C-Loss: 1.7356984615325928\n",
      "Epoch: 236, Loss: 17.879831314086914,  L-Loss: 1.2961158156394958, C-Loss: 1.7231773138046265\n",
      "Epoch: 237, Loss: 17.828594207763672,  L-Loss: 1.2932661771774292, C-Loss: 1.7181960940361023\n",
      "Epoch: 238, Loss: 17.869739532470703,  L-Loss: 1.3099859356880188, C-Loss: 1.7214746475219727\n",
      "Epoch: 239, Loss: 18.028008460998535,  L-Loss: 1.3277087211608887, C-Loss: 1.7364153861999512\n",
      "Epoch: 240, Loss: 18.011049270629883,  L-Loss: 1.305711269378662, C-Loss: 1.7358194589614868\n",
      "Epoch: 241, Loss: 18.03610134124756,  L-Loss: 1.2944294214248657, C-Loss: 1.7388886213302612\n",
      "Epoch: 242, Loss: 18.048823356628418,  L-Loss: 1.273874282836914, C-Loss: 1.7411885261535645\n",
      "Epoch: 243, Loss: 17.934751510620117,  L-Loss: 1.2739304304122925, C-Loss: 1.7297786474227905\n",
      "Epoch: 244, Loss: 17.843854904174805,  L-Loss: 1.289103925228119, C-Loss: 1.7199302911758423\n",
      "Epoch: 245, Loss: 17.814181327819824,  L-Loss: 1.3097572326660156, C-Loss: 1.7159302830696106\n",
      "Epoch: 246, Loss: 17.980891227722168,  L-Loss: 1.3022626638412476, C-Loss: 1.732975959777832\n",
      "Epoch: 247, Loss: 18.091246604919434,  L-Loss: 1.2975096106529236, C-Loss: 1.7442492246627808\n",
      "Epoch: 248, Loss: 17.98569679260254,  L-Loss: 1.2977564334869385, C-Loss: 1.733681857585907\n",
      "Epoch: 249, Loss: 17.921396255493164,  L-Loss: 1.2952925562858582, C-Loss: 1.7273749709129333\n",
      "Epoch: 250, Loss: 17.90253448486328,  L-Loss: 1.3093156218528748, C-Loss: 1.7247875928878784\n",
      "Epoch: 251, Loss: 17.920318603515625,  L-Loss: 1.2966252565383911, C-Loss: 1.7272005677223206\n",
      "Epoch: 252, Loss: 18.101717948913574,  L-Loss: 1.306618571281433, C-Loss: 1.744840919971466\n",
      "Epoch: 253, Loss: 18.17349624633789,  L-Loss: 1.328044056892395, C-Loss: 1.7509474158287048\n",
      "Epoch: 254, Loss: 18.113472938537598,  L-Loss: 1.3255128264427185, C-Loss: 1.745071530342102\n",
      "Epoch: 255, Loss: 18.18269157409668,  L-Loss: 1.3424610495567322, C-Loss: 1.7511460781097412\n",
      "Epoch: 256, Loss: 18.036173820495605,  L-Loss: 1.3094337582588196, C-Loss: 1.738145649433136\n",
      "Epoch: 257, Loss: 17.9049015045166,  L-Loss: 1.2893040180206299, C-Loss: 1.7260249853134155\n",
      "Epoch: 258, Loss: 18.043320655822754,  L-Loss: 1.3207576274871826, C-Loss: 1.738294243812561\n",
      "Epoch: 259, Loss: 18.1949405670166,  L-Loss: 1.3392176628112793, C-Loss: 1.7525331377983093\n",
      "Epoch: 260, Loss: 18.127476692199707,  L-Loss: 1.3437958359718323, C-Loss: 1.7455579042434692\n",
      "Epoch: 261, Loss: 18.060458183288574,  L-Loss: 1.3416770100593567, C-Loss: 1.7389619946479797\n",
      "Epoch: 262, Loss: 18.1906099319458,  L-Loss: 1.3573513627052307, C-Loss: 1.7511934638023376\n",
      "Epoch: 263, Loss: 18.092238426208496,  L-Loss: 1.3494846820831299, C-Loss: 1.74174964427948\n",
      "Epoch: 264, Loss: 17.992783546447754,  L-Loss: 1.3409291505813599, C-Loss: 1.7322319149971008\n",
      "Epoch: 265, Loss: 18.041898727416992,  L-Loss: 1.3273893594741821, C-Loss: 1.7378203868865967\n",
      "Epoch: 266, Loss: 18.044713020324707,  L-Loss: 1.3197803497314453, C-Loss: 1.7384822964668274\n",
      "Epoch: 267, Loss: 17.98292636871338,  L-Loss: 1.3084442019462585, C-Loss: 1.7328703999519348\n",
      "Epoch: 268, Loss: 18.03024196624756,  L-Loss: 1.3278222680091858, C-Loss: 1.7366331219673157\n",
      "Epoch: 269, Loss: 18.059690475463867,  L-Loss: 1.3434431552886963, C-Loss: 1.738796889781952\n",
      "Epoch: 270, Loss: 18.18274688720703,  L-Loss: 1.3673031330108643, C-Loss: 1.7499095797538757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 271, Loss: 18.18525218963623,  L-Loss: 1.3685196042060852, C-Loss: 1.7500991821289062\n",
      "Epoch: 272, Loss: 18.242398262023926,  L-Loss: 1.4022671580314636, C-Loss: 1.7541265487670898\n",
      "Epoch: 273, Loss: 18.121893882751465,  L-Loss: 1.373842477798462, C-Loss: 1.7434971928596497\n",
      "Epoch: 274, Loss: 18.00966167449951,  L-Loss: 1.3271324038505554, C-Loss: 1.7346095442771912\n",
      "Epoch: 275, Loss: 18.17771339416504,  L-Loss: 1.349454402923584, C-Loss: 1.7502986192703247\n",
      "Epoch: 276, Loss: 18.250718116760254,  L-Loss: 1.3718557953834534, C-Loss: 1.7564790844917297\n",
      "Epoch: 277, Loss: 18.075215339660645,  L-Loss: 1.3493589758872986, C-Loss: 1.7400535941123962\n",
      "Epoch: 278, Loss: 17.96591091156006,  L-Loss: 1.3466896414756775, C-Loss: 1.729256510734558\n",
      "Epoch: 279, Loss: 17.98552417755127,  L-Loss: 1.3572556376457214, C-Loss: 1.7306897044181824\n",
      "Epoch: 280, Loss: 18.17740535736084,  L-Loss: 1.4008929133415222, C-Loss: 1.7476959228515625\n",
      "Epoch: 281, Loss: 18.280946731567383,  L-Loss: 1.414777159690857, C-Loss: 1.7573557496070862\n",
      "Epoch: 282, Loss: 18.206220626831055,  L-Loss: 1.3847302794456482, C-Loss: 1.751385509967804\n",
      "Epoch: 283, Loss: 17.98822784423828,  L-Loss: 1.3642584681510925, C-Loss: 1.7306098341941833\n",
      "Epoch: 284, Loss: 18.004558563232422,  L-Loss: 1.3528149724006653, C-Loss: 1.732815146446228\n",
      "Epoch: 285, Loss: 17.93085289001465,  L-Loss: 1.3255932927131653, C-Loss: 1.7268055081367493\n",
      "Epoch: 286, Loss: 18.03832244873047,  L-Loss: 1.3517544865608215, C-Loss: 1.7362444996833801\n",
      "Epoch: 287, Loss: 18.15400505065918,  L-Loss: 1.3845373392105103, C-Loss: 1.7461736798286438\n",
      "Epoch: 288, Loss: 18.16760540008545,  L-Loss: 1.3989021182060242, C-Loss: 1.7468153238296509\n",
      "Epoch: 289, Loss: 18.123401641845703,  L-Loss: 1.4037523865699768, C-Loss: 1.7421525120735168\n",
      "Epoch: 290, Loss: 18.179675102233887,  L-Loss: 1.391760528087616, C-Loss: 1.7483795285224915\n",
      "Epoch: 291, Loss: 18.04030704498291,  L-Loss: 1.362364113330841, C-Loss: 1.7359124422073364\n",
      "Epoch: 292, Loss: 18.014866828918457,  L-Loss: 1.3701385259628296, C-Loss: 1.7329797148704529\n",
      "Epoch: 293, Loss: 18.074734687805176,  L-Loss: 1.3805487751960754, C-Loss: 1.738446056842804\n",
      "Epoch: 294, Loss: 18.221494674682617,  L-Loss: 1.398419976234436, C-Loss: 1.7522284984588623\n",
      "Epoch: 295, Loss: 18.15876865386963,  L-Loss: 1.3986034393310547, C-Loss: 1.7459467053413391\n",
      "Epoch: 296, Loss: 18.11289882659912,  L-Loss: 1.4008216261863708, C-Loss: 1.7412487864494324\n",
      "Epoch: 297, Loss: 18.21654987335205,  L-Loss: 1.4151239395141602, C-Loss: 1.7508987784385681\n",
      "Epoch: 298, Loss: 18.327250480651855,  L-Loss: 1.4433985352516174, C-Loss: 1.7605551481246948\n",
      "Epoch: 299, Loss: 18.300986289978027,  L-Loss: 1.4301024675369263, C-Loss: 1.758593499660492\n",
      "Epoch: 300, Loss: 18.098444938659668,  L-Loss: 1.3867918252944946, C-Loss: 1.7405049204826355\n",
      "Epoch: 301, Loss: 18.06532382965088,  L-Loss: 1.3750399351119995, C-Loss: 1.7377803325653076\n",
      "Epoch: 302, Loss: 18.134355545043945,  L-Loss: 1.3991169929504395, C-Loss: 1.7434797286987305\n",
      "Epoch: 303, Loss: 18.088024139404297,  L-Loss: 1.3972883820533752, C-Loss: 1.7389379143714905\n",
      "Epoch: 304, Loss: 17.983060836791992,  L-Loss: 1.3742923140525818, C-Loss: 1.7295914888381958\n",
      "Epoch: 305, Loss: 18.061219215393066,  L-Loss: 1.4029394388198853, C-Loss: 1.7359749674797058\n",
      "Out of patience at epoch 305\n",
      "Guardando mejor modelo en  ..\\models\\C2AE_alexnet_retrained\\base\\1\\26L.pth\n",
      "HS fold 1: 0.2619\n",
      "Predicciones guardadas en ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\1\\predictions.csv\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Epoch: 0, Loss: 19.748130798339844,  L-Loss: 0.7745075225830078, C-Loss: 1.936087727546692\n",
      "Epoch: 1, Loss: 19.64682674407959,  L-Loss: 0.6021750867366791, C-Loss: 1.9345738887786865\n",
      "Epoch: 2, Loss: 19.584423065185547,  L-Loss: 0.49095553159713745, C-Loss: 1.9338944554328918\n",
      "Epoch: 3, Loss: 19.53948974609375,  L-Loss: 0.4236435145139694, C-Loss: 1.9327667951583862\n",
      "Epoch: 4, Loss: 19.51170063018799,  L-Loss: 0.376839280128479, C-Loss: 1.9323281049728394\n",
      "Epoch: 5, Loss: 19.474202156066895,  L-Loss: 0.34007465839385986, C-Loss: 1.930416464805603\n",
      "Epoch: 6, Loss: 19.446861267089844,  L-Loss: 0.3084295094013214, C-Loss: 1.9292646050453186\n",
      "Epoch: 7, Loss: 19.423008918762207,  L-Loss: 0.28329238295555115, C-Loss: 1.928136169910431\n",
      "Epoch: 8, Loss: 19.39670181274414,  L-Loss: 0.2657090574502945, C-Loss: 1.9263848066329956\n",
      "Epoch: 9, Loss: 19.38258934020996,  L-Loss: 0.24981383234262466, C-Loss: 1.925768256187439\n",
      "Epoch: 10, Loss: 19.364198684692383,  L-Loss: 0.23575590550899506, C-Loss: 1.9246320724487305\n",
      "Epoch: 11, Loss: 19.34140110015869,  L-Loss: 0.22455888241529465, C-Loss: 1.922912061214447\n",
      "Epoch: 12, Loss: 19.324893951416016,  L-Loss: 0.21446086466312408, C-Loss: 1.9217662811279297\n",
      "Epoch: 13, Loss: 19.317243576049805,  L-Loss: 0.20698702335357666, C-Loss: 1.9213749170303345\n",
      "Epoch: 14, Loss: 19.3035249710083,  L-Loss: 0.20212139934301376, C-Loss: 1.9202464818954468\n",
      "Epoch: 15, Loss: 19.29748249053955,  L-Loss: 0.19764576107263565, C-Loss: 1.9198659658432007\n",
      "Epoch: 16, Loss: 19.276686668395996,  L-Loss: 0.19678520411252975, C-Loss: 1.91782945394516\n",
      "Epoch: 17, Loss: 19.272958755493164,  L-Loss: 0.1968773528933525, C-Loss: 1.9174519777297974\n",
      "Epoch: 18, Loss: 19.249260902404785,  L-Loss: 0.1973646730184555, C-Loss: 1.915057897567749\n",
      "Epoch: 19, Loss: 19.24533748626709,  L-Loss: 0.19490429013967514, C-Loss: 1.914788544178009\n",
      "Epoch: 20, Loss: 19.204435348510742,  L-Loss: 0.1976490542292595, C-Loss: 1.9105610847473145\n",
      "Epoch: 21, Loss: 19.2042875289917,  L-Loss: 0.20395705848932266, C-Loss: 1.9102308750152588\n",
      "Epoch: 22, Loss: 19.183452606201172,  L-Loss: 0.20538794249296188, C-Loss: 1.9080758690834045\n",
      "Epoch: 23, Loss: 19.189620971679688,  L-Loss: 0.2085030972957611, C-Loss: 1.9085369110107422\n",
      "Epoch: 24, Loss: 19.154550552368164,  L-Loss: 0.21330563724040985, C-Loss: 1.9047897458076477\n",
      "Epoch: 25, Loss: 19.159398078918457,  L-Loss: 0.2269711047410965, C-Loss: 1.904591143131256\n",
      "Epoch: 26, Loss: 19.136795043945312,  L-Loss: 0.2312142476439476, C-Loss: 1.9021188616752625\n",
      "Epoch: 27, Loss: 19.134519577026367,  L-Loss: 0.2413628175854683, C-Loss: 1.9013839364051819\n",
      "Epoch: 28, Loss: 19.103294372558594,  L-Loss: 0.24853618443012238, C-Loss: 1.8979025483131409\n",
      "Epoch: 29, Loss: 19.087589263916016,  L-Loss: 0.25452013313770294, C-Loss: 1.896032989025116\n",
      "Epoch: 30, Loss: 19.09091281890869,  L-Loss: 0.2656167596578598, C-Loss: 1.8958103656768799\n",
      "Epoch: 31, Loss: 19.066022872924805,  L-Loss: 0.27576783299446106, C-Loss: 1.8928138613700867\n",
      "Epoch: 32, Loss: 19.04047679901123,  L-Loss: 0.2875984013080597, C-Loss: 1.8896677494049072\n",
      "Epoch: 33, Loss: 19.029111862182617,  L-Loss: 0.296905517578125, C-Loss: 1.8880659341812134\n",
      "Epoch: 34, Loss: 19.048537254333496,  L-Loss: 0.3097210228443146, C-Loss: 1.889367699623108\n",
      "Epoch: 35, Loss: 18.999208450317383,  L-Loss: 0.31429092586040497, C-Loss: 1.8842062950134277\n",
      "Epoch: 36, Loss: 19.00680637359619,  L-Loss: 0.3345176875591278, C-Loss: 1.8839547634124756\n",
      "Epoch: 37, Loss: 18.967872619628906,  L-Loss: 0.34447377920150757, C-Loss: 1.879563570022583\n",
      "Epoch: 38, Loss: 18.94922924041748,  L-Loss: 0.3639913350343704, C-Loss: 1.8767234086990356\n",
      "Epoch: 39, Loss: 18.939361572265625,  L-Loss: 0.38608698546886444, C-Loss: 1.8746317625045776\n",
      "Epoch: 40, Loss: 18.943574905395508,  L-Loss: 0.4130243957042694, C-Loss: 1.8737062215805054\n",
      "Epoch: 41, Loss: 18.93169593811035,  L-Loss: 0.4418705254793167, C-Loss: 1.8710761070251465\n",
      "Epoch: 42, Loss: 18.93155288696289,  L-Loss: 0.46357275545597076, C-Loss: 1.8699766397476196\n",
      "Epoch: 43, Loss: 18.853729248046875,  L-Loss: 0.4644772857427597, C-Loss: 1.8621490597724915\n",
      "Epoch: 44, Loss: 18.885944366455078,  L-Loss: 0.4878147393465042, C-Loss: 1.8642037510871887\n",
      "Epoch: 45, Loss: 18.829840660095215,  L-Loss: 0.5110744833946228, C-Loss: 1.857430338859558\n",
      "Epoch: 46, Loss: 18.857142448425293,  L-Loss: 0.5510949492454529, C-Loss: 1.8581594824790955\n",
      "Epoch: 47, Loss: 18.787781715393066,  L-Loss: 0.5567569434642792, C-Loss: 1.8509403467178345\n",
      "Epoch: 48, Loss: 18.77573299407959,  L-Loss: 0.5706075727939606, C-Loss: 1.84904283285141\n",
      "Epoch: 49, Loss: 18.718728065490723,  L-Loss: 0.589039534330368, C-Loss: 1.8424208760261536\n",
      "Epoch: 50, Loss: 18.702693939208984,  L-Loss: 0.613521009683609, C-Loss: 1.8395932912826538\n",
      "Epoch: 51, Loss: 18.67383575439453,  L-Loss: 0.6494105458259583, C-Loss: 1.8349130749702454\n",
      "Epoch: 52, Loss: 18.660886764526367,  L-Loss: 0.6760879755020142, C-Loss: 1.8322842717170715\n",
      "Epoch: 53, Loss: 18.603660583496094,  L-Loss: 0.7039205729961395, C-Loss: 1.82517009973526\n",
      "Epoch: 54, Loss: 18.59979248046875,  L-Loss: 0.7413668036460876, C-Loss: 1.8229108452796936\n",
      "Epoch: 55, Loss: 18.55241107940674,  L-Loss: 0.7720379531383514, C-Loss: 1.8166391849517822\n",
      "Epoch: 56, Loss: 18.55258083343506,  L-Loss: 0.8052864968776703, C-Loss: 1.8149937391281128\n",
      "Epoch: 57, Loss: 18.52058982849121,  L-Loss: 0.8326260149478912, C-Loss: 1.810427725315094\n",
      "Epoch: 58, Loss: 18.58323574066162,  L-Loss: 0.8656100034713745, C-Loss: 1.815043032169342\n",
      "Epoch: 59, Loss: 18.518927574157715,  L-Loss: 0.8461778163909912, C-Loss: 1.809583842754364\n",
      "Epoch: 60, Loss: 18.498615264892578,  L-Loss: 0.8314895927906036, C-Loss: 1.8082870841026306\n",
      "Epoch: 61, Loss: 18.382591247558594,  L-Loss: 0.822668194770813, C-Loss: 1.7971256971359253\n",
      "Epoch: 62, Loss: 18.410456657409668,  L-Loss: 0.8477577269077301, C-Loss: 1.798657774925232\n",
      "Epoch: 63, Loss: 18.322265625,  L-Loss: 0.8635724484920502, C-Loss: 1.78904789686203\n",
      "Epoch: 64, Loss: 18.344971656799316,  L-Loss: 0.8938915729522705, C-Loss: 1.7898024916648865\n",
      "Epoch: 65, Loss: 18.30958366394043,  L-Loss: 0.9243046343326569, C-Loss: 1.784743070602417\n",
      "Epoch: 66, Loss: 18.32205104827881,  L-Loss: 0.9317357242107391, C-Loss: 1.785618245601654\n",
      "Epoch: 67, Loss: 18.255279541015625,  L-Loss: 0.936526894569397, C-Loss: 1.778701663017273\n",
      "Epoch: 68, Loss: 18.33255386352539,  L-Loss: 0.9735117256641388, C-Loss: 1.7845797538757324\n",
      "Epoch: 69, Loss: 18.23154640197754,  L-Loss: 0.9674248397350311, C-Loss: 1.774783432483673\n",
      "Epoch: 70, Loss: 18.146554946899414,  L-Loss: 0.9471993744373322, C-Loss: 1.7672954201698303\n",
      "Epoch: 71, Loss: 18.158637046813965,  L-Loss: 0.9547290802001953, C-Loss: 1.768127202987671\n",
      "Epoch: 72, Loss: 18.12677764892578,  L-Loss: 0.9456328451633453, C-Loss: 1.7653961181640625\n",
      "Epoch: 73, Loss: 18.016549110412598,  L-Loss: 0.9335966408252716, C-Loss: 1.7549750804901123\n",
      "Epoch: 74, Loss: 18.07548236846924,  L-Loss: 0.9524944722652435, C-Loss: 1.7599234580993652\n",
      "Epoch: 75, Loss: 18.053146362304688,  L-Loss: 0.972373753786087, C-Loss: 1.7566959857940674\n",
      "Epoch: 76, Loss: 17.980636596679688,  L-Loss: 0.976814478635788, C-Loss: 1.749222993850708\n",
      "Epoch: 77, Loss: 18.003196716308594,  L-Loss: 1.004533290863037, C-Loss: 1.7500929832458496\n",
      "Epoch: 78, Loss: 18.000755310058594,  L-Loss: 1.022559493780136, C-Loss: 1.7489475011825562\n",
      "Epoch: 79, Loss: 17.941837310791016,  L-Loss: 1.0132395029067993, C-Loss: 1.7435217499732971\n",
      "Epoch: 80, Loss: 17.935229301452637,  L-Loss: 1.0263329148292542, C-Loss: 1.7422062754631042\n",
      "Epoch: 81, Loss: 17.900307655334473,  L-Loss: 1.0289156138896942, C-Loss: 1.7385849356651306\n",
      "Epoch: 82, Loss: 17.902193069458008,  L-Loss: 1.0377960801124573, C-Loss: 1.7383294701576233\n",
      "Epoch: 83, Loss: 17.89698028564453,  L-Loss: 1.0428181290626526, C-Loss: 1.7375571131706238\n",
      "Epoch: 84, Loss: 17.834486961364746,  L-Loss: 1.0479137301445007, C-Loss: 1.7310529947280884\n",
      "Epoch: 85, Loss: 17.887033462524414,  L-Loss: 1.0570112466812134, C-Loss: 1.7358527183532715\n",
      "Epoch: 86, Loss: 17.819747924804688,  L-Loss: 1.0268681049346924, C-Loss: 1.7306312918663025\n",
      "Epoch: 87, Loss: 17.74821949005127,  L-Loss: 1.016254186630249, C-Loss: 1.7240092158317566\n",
      "Epoch: 88, Loss: 17.81387424468994,  L-Loss: 1.027846336364746, C-Loss: 1.7299952507019043\n",
      "Epoch: 89, Loss: 17.757192611694336,  L-Loss: 1.0289036631584167, C-Loss: 1.7242740988731384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Loss: 17.795567512512207,  L-Loss: 1.045106828212738, C-Loss: 1.7273013591766357\n",
      "Epoch: 91, Loss: 17.79019546508789,  L-Loss: 1.0484685003757477, C-Loss: 1.7265961170196533\n",
      "Epoch: 92, Loss: 17.716459274291992,  L-Loss: 1.0589019358158112, C-Loss: 1.7187007069587708\n",
      "Epoch: 93, Loss: 17.738162994384766,  L-Loss: 1.082171618938446, C-Loss: 1.7197077870368958\n",
      "Epoch: 94, Loss: 17.681937217712402,  L-Loss: 1.1100444197654724, C-Loss: 1.7126914858818054\n",
      "Epoch: 95, Loss: 17.729647636413574,  L-Loss: 1.1315499544143677, C-Loss: 1.7163872122764587\n",
      "Epoch: 96, Loss: 17.680566787719727,  L-Loss: 1.132757306098938, C-Loss: 1.711418867111206\n",
      "Epoch: 97, Loss: 17.689322471618652,  L-Loss: 1.118510901927948, C-Loss: 1.7130067944526672\n",
      "Epoch: 98, Loss: 17.682854652404785,  L-Loss: 1.1027554869651794, C-Loss: 1.7131478190422058\n",
      "Epoch: 99, Loss: 17.635066032409668,  L-Loss: 1.0896620750427246, C-Loss: 1.7090235352516174\n",
      "Epoch: 100, Loss: 17.629115104675293,  L-Loss: 1.0911478400230408, C-Loss: 1.708354115486145\n",
      "Epoch: 101, Loss: 17.667757987976074,  L-Loss: 1.118738204240799, C-Loss: 1.7108389139175415\n",
      "Epoch: 102, Loss: 17.689845085144043,  L-Loss: 1.1240469217300415, C-Loss: 1.7127821445465088\n",
      "Epoch: 103, Loss: 17.676841735839844,  L-Loss: 1.1365014910697937, C-Loss: 1.7108591794967651\n",
      "Epoch: 104, Loss: 17.686100959777832,  L-Loss: 1.1572951674461365, C-Loss: 1.7107453346252441\n",
      "Epoch: 105, Loss: 17.693397521972656,  L-Loss: 1.1493027210235596, C-Loss: 1.7118746042251587\n",
      "Epoch: 106, Loss: 17.584872245788574,  L-Loss: 1.1278480291366577, C-Loss: 1.7020947933197021\n",
      "Epoch: 107, Loss: 17.5233154296875,  L-Loss: 1.1329487562179565, C-Loss: 1.6956841349601746\n",
      "Epoch: 108, Loss: 17.59478187561035,  L-Loss: 1.165627121925354, C-Loss: 1.7011967897415161\n",
      "Epoch: 109, Loss: 17.58473777770996,  L-Loss: 1.165955364704132, C-Loss: 1.7001760005950928\n",
      "Epoch: 110, Loss: 17.620352745056152,  L-Loss: 1.1548572778701782, C-Loss: 1.7042924761772156\n",
      "Epoch: 111, Loss: 17.553404808044434,  L-Loss: 1.1531994342803955, C-Loss: 1.6976804733276367\n",
      "Epoch: 112, Loss: 17.606611251831055,  L-Loss: 1.1714840531349182, C-Loss: 1.7020869255065918\n",
      "Epoch: 113, Loss: 17.59129810333252,  L-Loss: 1.17892187833786, C-Loss: 1.700183629989624\n",
      "Epoch: 114, Loss: 17.548961639404297,  L-Loss: 1.1863426566123962, C-Loss: 1.6955790519714355\n",
      "Epoch: 115, Loss: 17.65104389190674,  L-Loss: 1.2102272510528564, C-Loss: 1.7045931220054626\n",
      "Epoch: 116, Loss: 17.578715324401855,  L-Loss: 1.1911484003067017, C-Loss: 1.698314130306244\n",
      "Epoch: 117, Loss: 17.519898414611816,  L-Loss: 1.1812968850135803, C-Loss: 1.6929250955581665\n",
      "Epoch: 118, Loss: 17.56204891204834,  L-Loss: 1.199806809425354, C-Loss: 1.6962145566940308\n",
      "Epoch: 119, Loss: 17.54037380218506,  L-Loss: 1.223442554473877, C-Loss: 1.6928651928901672\n",
      "Epoch: 120, Loss: 17.55069637298584,  L-Loss: 1.2482784986495972, C-Loss: 1.6926557421684265\n",
      "Epoch: 121, Loss: 17.620375633239746,  L-Loss: 1.2650910019874573, C-Loss: 1.6987830996513367\n",
      "Epoch: 122, Loss: 17.619027137756348,  L-Loss: 1.2705501914024353, C-Loss: 1.698375165462494\n",
      "Epoch: 123, Loss: 17.660804748535156,  L-Loss: 1.2759556770324707, C-Loss: 1.702282726764679\n",
      "Epoch: 124, Loss: 17.58786392211914,  L-Loss: 1.2888460755348206, C-Loss: 1.6943440437316895\n",
      "Epoch: 125, Loss: 17.601881980895996,  L-Loss: 1.2908767461776733, C-Loss: 1.6956444382667542\n",
      "Epoch: 126, Loss: 17.544987678527832,  L-Loss: 1.3031535148620605, C-Loss: 1.6893410682678223\n",
      "Epoch: 127, Loss: 17.57407569885254,  L-Loss: 1.3064560890197754, C-Loss: 1.6920848488807678\n",
      "Epoch: 128, Loss: 17.614151000976562,  L-Loss: 1.3309221863746643, C-Loss: 1.694869041442871\n",
      "Epoch: 129, Loss: 17.604395866394043,  L-Loss: 1.3449034690856934, C-Loss: 1.6931945085525513\n",
      "Epoch: 130, Loss: 17.715438842773438,  L-Loss: 1.3535519242286682, C-Loss: 1.7038663029670715\n",
      "Epoch: 131, Loss: 17.712509155273438,  L-Loss: 1.3545388579368591, C-Loss: 1.7035239934921265\n",
      "Epoch: 132, Loss: 17.789165496826172,  L-Loss: 1.3724204897880554, C-Loss: 1.710295557975769\n",
      "Epoch: 133, Loss: 17.747629165649414,  L-Loss: 1.3881439566612244, C-Loss: 1.705355703830719\n",
      "Epoch: 134, Loss: 17.555994987487793,  L-Loss: 1.3767545819282532, C-Loss: 1.686761736869812\n",
      "Epoch: 135, Loss: 17.595995903015137,  L-Loss: 1.3835636377334595, C-Loss: 1.690421462059021\n",
      "Epoch: 136, Loss: 17.462448120117188,  L-Loss: 1.3720475435256958, C-Loss: 1.6776424050331116\n",
      "Epoch: 137, Loss: 17.58700942993164,  L-Loss: 1.3713958263397217, C-Loss: 1.6901311874389648\n",
      "Epoch: 138, Loss: 17.634408950805664,  L-Loss: 1.3832663297653198, C-Loss: 1.694277584552765\n",
      "Epoch: 139, Loss: 17.549044609069824,  L-Loss: 1.385375440120697, C-Loss: 1.6856356263160706\n",
      "Epoch: 140, Loss: 17.68175983428955,  L-Loss: 1.4294080138206482, C-Loss: 1.6967055201530457\n",
      "Epoch: 141, Loss: 17.631522178649902,  L-Loss: 1.4350838661193848, C-Loss: 1.6913981437683105\n",
      "Epoch: 142, Loss: 17.637700080871582,  L-Loss: 1.4188835620880127, C-Loss: 1.692825734615326\n",
      "Epoch: 143, Loss: 17.549824714660645,  L-Loss: 1.3887988328933716, C-Loss: 1.6855424642562866\n",
      "Epoch: 144, Loss: 17.506936073303223,  L-Loss: 1.3900827765464783, C-Loss: 1.6811894178390503\n",
      "Epoch: 145, Loss: 17.52565288543701,  L-Loss: 1.3814815878868103, C-Loss: 1.6834912300109863\n",
      "Epoch: 146, Loss: 17.596107482910156,  L-Loss: 1.395002007484436, C-Loss: 1.6898606419563293\n",
      "Epoch: 147, Loss: 17.53981304168701,  L-Loss: 1.4026843905448914, C-Loss: 1.6838470101356506\n",
      "Epoch: 148, Loss: 17.563410758972168,  L-Loss: 1.4265266060829163, C-Loss: 1.6850147247314453\n",
      "Epoch: 149, Loss: 17.67234230041504,  L-Loss: 1.4388580918312073, C-Loss: 1.6952913999557495\n",
      "Epoch: 150, Loss: 17.611677169799805,  L-Loss: 1.4477089643478394, C-Loss: 1.6887821555137634\n",
      "Epoch: 151, Loss: 17.588680267333984,  L-Loss: 1.459288775920868, C-Loss: 1.685903549194336\n",
      "Epoch: 152, Loss: 17.6126651763916,  L-Loss: 1.4818159341812134, C-Loss: 1.687175691127777\n",
      "Epoch: 153, Loss: 17.544184684753418,  L-Loss: 1.4612590074539185, C-Loss: 1.681355595588684\n",
      "Epoch: 154, Loss: 17.656831741333008,  L-Loss: 1.4697588682174683, C-Loss: 1.692195177078247\n",
      "Epoch: 155, Loss: 17.556610107421875,  L-Loss: 1.4557169079780579, C-Loss: 1.6828750967979431\n",
      "Epoch: 156, Loss: 17.62939739227295,  L-Loss: 1.4665427207946777, C-Loss: 1.689612627029419\n",
      "Epoch: 157, Loss: 17.697412490844727,  L-Loss: 1.4767834544181824, C-Loss: 1.6959020495414734\n",
      "Epoch: 158, Loss: 17.548956871032715,  L-Loss: 1.4969515800476074, C-Loss: 1.6800482273101807\n",
      "Epoch: 159, Loss: 17.715139389038086,  L-Loss: 1.5223589539527893, C-Loss: 1.6953959465026855\n",
      "Epoch: 160, Loss: 17.61401081085205,  L-Loss: 1.5034968852996826, C-Loss: 1.6862261295318604\n",
      "Epoch: 161, Loss: 17.581979751586914,  L-Loss: 1.4876079559326172, C-Loss: 1.6838176250457764\n",
      "Epoch: 162, Loss: 17.56223487854004,  L-Loss: 1.4792828559875488, C-Loss: 1.6822593212127686\n",
      "Epoch: 163, Loss: 17.48130989074707,  L-Loss: 1.4970365762710571, C-Loss: 1.6732791662216187\n",
      "Epoch: 164, Loss: 17.827302932739258,  L-Loss: 1.53993821144104, C-Loss: 1.7057334184646606\n",
      "Epoch: 165, Loss: 17.768977165222168,  L-Loss: 1.5316764116287231, C-Loss: 1.700313925743103\n",
      "Epoch: 166, Loss: 17.65769672393799,  L-Loss: 1.5109508037567139, C-Loss: 1.690222144126892\n",
      "Epoch: 167, Loss: 17.650212287902832,  L-Loss: 1.5125678181648254, C-Loss: 1.6893927454948425\n",
      "Epoch: 168, Loss: 17.521028518676758,  L-Loss: 1.5139432549476624, C-Loss: 1.676405668258667\n",
      "Epoch: 169, Loss: 17.471813201904297,  L-Loss: 1.5089784860610962, C-Loss: 1.671732485294342\n",
      "Epoch: 170, Loss: 17.529691696166992,  L-Loss: 1.5221454501152039, C-Loss: 1.676861822605133\n",
      "Epoch: 171, Loss: 17.580357551574707,  L-Loss: 1.5089170336723328, C-Loss: 1.6825899481773376\n",
      "Epoch: 172, Loss: 17.56995391845703,  L-Loss: 1.5086326003074646, C-Loss: 1.681563675403595\n",
      "Epoch: 173, Loss: 17.617700576782227,  L-Loss: 1.5057129859924316, C-Loss: 1.6864843368530273\n",
      "Epoch: 174, Loss: 17.412601470947266,  L-Loss: 1.4805169105529785, C-Loss: 1.6672342419624329\n",
      "Epoch: 175, Loss: 17.406826972961426,  L-Loss: 1.481137990951538, C-Loss: 1.6666257977485657\n",
      "Epoch: 176, Loss: 17.39775276184082,  L-Loss: 1.4915013313293457, C-Loss: 1.6652001738548279\n",
      "Epoch: 177, Loss: 17.51311683654785,  L-Loss: 1.5117858052253723, C-Loss: 1.6757224202156067\n",
      "Epoch: 178, Loss: 17.471599578857422,  L-Loss: 1.5271390676498413, C-Loss: 1.6708030104637146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179, Loss: 17.484509468078613,  L-Loss: 1.5235263109207153, C-Loss: 1.6722745895385742\n",
      "Epoch: 180, Loss: 17.675360679626465,  L-Loss: 1.5392889380455017, C-Loss: 1.6905716061592102\n",
      "Epoch: 181, Loss: 17.681659698486328,  L-Loss: 1.5453444123268127, C-Loss: 1.6908987760543823\n",
      "Epoch: 182, Loss: 17.522305488586426,  L-Loss: 1.5377460718154907, C-Loss: 1.6753432154655457\n",
      "Epoch: 183, Loss: 17.760194778442383,  L-Loss: 1.568844199180603, C-Loss: 1.6975772380828857\n",
      "Epoch: 184, Loss: 17.754382133483887,  L-Loss: 1.5737931728363037, C-Loss: 1.6967486143112183\n",
      "Epoch: 185, Loss: 17.702939987182617,  L-Loss: 1.584610104560852, C-Loss: 1.6910635232925415\n",
      "Epoch: 186, Loss: 17.90804672241211,  L-Loss: 1.6016762852668762, C-Loss: 1.7107208371162415\n",
      "Epoch: 187, Loss: 17.520949363708496,  L-Loss: 1.5739617347717285, C-Loss: 1.6733967661857605\n",
      "Epoch: 188, Loss: 17.620418548583984,  L-Loss: 1.573731541633606, C-Loss: 1.6833552718162537\n",
      "Epoch: 189, Loss: 17.668755531311035,  L-Loss: 1.55025976896286, C-Loss: 1.6893625259399414\n",
      "Epoch: 190, Loss: 17.61310863494873,  L-Loss: 1.542655348777771, C-Loss: 1.6841780543327332\n",
      "Epoch: 191, Loss: 17.658406257629395,  L-Loss: 1.549817979335785, C-Loss: 1.6883497834205627\n",
      "Epoch: 192, Loss: 17.675167083740234,  L-Loss: 1.5615030527114868, C-Loss: 1.6894415020942688\n",
      "Epoch: 193, Loss: 17.50263786315918,  L-Loss: 1.565080463886261, C-Loss: 1.672009825706482\n",
      "Epoch: 194, Loss: 17.625374794006348,  L-Loss: 1.5603982210159302, C-Loss: 1.6845175623893738\n",
      "Epoch: 195, Loss: 17.732277870178223,  L-Loss: 1.5837947726249695, C-Loss: 1.694037914276123\n",
      "Epoch: 196, Loss: 17.543254852294922,  L-Loss: 1.5756184458732605, C-Loss: 1.6755446195602417\n",
      "Epoch: 197, Loss: 17.546512603759766,  L-Loss: 1.562504768371582, C-Loss: 1.6765260100364685\n",
      "Epoch: 198, Loss: 17.817097663879395,  L-Loss: 1.5964888334274292, C-Loss: 1.7018853425979614\n",
      "Epoch: 199, Loss: 17.624244689941406,  L-Loss: 1.5754318833351135, C-Loss: 1.6836528778076172\n",
      "Epoch: 200, Loss: 17.70709991455078,  L-Loss: 1.579126238822937, C-Loss: 1.691753625869751\n",
      "Epoch: 201, Loss: 17.886425971984863,  L-Loss: 1.593746304512024, C-Loss: 1.7089552879333496\n",
      "Epoch: 202, Loss: 17.93610954284668,  L-Loss: 1.608196496963501, C-Loss: 1.7132011651992798\n",
      "Epoch: 203, Loss: 17.775907516479492,  L-Loss: 1.6155792474746704, C-Loss: 1.6968119144439697\n",
      "Epoch: 204, Loss: 17.683527946472168,  L-Loss: 1.603761374950409, C-Loss: 1.6881646513938904\n",
      "Epoch: 205, Loss: 17.757722854614258,  L-Loss: 1.6015211939811707, C-Loss: 1.6956961154937744\n",
      "Epoch: 206, Loss: 17.636852264404297,  L-Loss: 1.5753280520439148, C-Loss: 1.6849187016487122\n",
      "Epoch: 207, Loss: 17.711715698242188,  L-Loss: 1.5711307525634766, C-Loss: 1.692615032196045\n",
      "Epoch: 208, Loss: 17.669547080993652,  L-Loss: 1.5736538171768188, C-Loss: 1.688271939754486\n",
      "Epoch: 209, Loss: 17.715112686157227,  L-Loss: 1.60013347864151, C-Loss: 1.691504716873169\n",
      "Epoch: 210, Loss: 18.02260398864746,  L-Loss: 1.6235920190811157, C-Loss: 1.721080720424652\n",
      "Epoch: 211, Loss: 17.884193420410156,  L-Loss: 1.6180166006088257, C-Loss: 1.7075185179710388\n",
      "Epoch: 212, Loss: 17.99968910217285,  L-Loss: 1.640921711921692, C-Loss: 1.717922866344452\n",
      "Epoch: 213, Loss: 18.09484577178955,  L-Loss: 1.6327157020568848, C-Loss: 1.727848768234253\n",
      "Epoch: 214, Loss: 17.984865188598633,  L-Loss: 1.6104747653007507, C-Loss: 1.7179628014564514\n",
      "Epoch: 215, Loss: 17.829890251159668,  L-Loss: 1.615896224975586, C-Loss: 1.7021941542625427\n",
      "Epoch: 216, Loss: 17.742332458496094,  L-Loss: 1.59768807888031, C-Loss: 1.6943488121032715\n",
      "Epoch: 217, Loss: 17.74553871154785,  L-Loss: 1.591657817363739, C-Loss: 1.6949708461761475\n",
      "Epoch: 218, Loss: 17.584195137023926,  L-Loss: 1.5878254771232605, C-Loss: 1.6790282726287842\n",
      "Epoch: 219, Loss: 17.542902946472168,  L-Loss: 1.580310881137848, C-Loss: 1.6752747297286987\n",
      "Epoch: 220, Loss: 17.78885841369629,  L-Loss: 1.5945902466773987, C-Loss: 1.6991563439369202\n",
      "Epoch: 221, Loss: 17.874975204467773,  L-Loss: 1.5904918909072876, C-Loss: 1.7079729437828064\n",
      "Epoch: 222, Loss: 17.780086517333984,  L-Loss: 1.5816916823387146, C-Loss: 1.6989241242408752\n",
      "Epoch: 223, Loss: 17.725489616394043,  L-Loss: 1.6059816479682922, C-Loss: 1.6922498941421509\n",
      "Epoch: 224, Loss: 17.93385124206543,  L-Loss: 1.6314776539802551, C-Loss: 1.7118112444877625\n",
      "Epoch: 225, Loss: 17.866856575012207,  L-Loss: 1.634913980960846, C-Loss: 1.7049399018287659\n",
      "Epoch: 226, Loss: 18.088899612426758,  L-Loss: 1.6545723676681519, C-Loss: 1.7261614203453064\n",
      "Epoch: 227, Loss: 18.07878589630127,  L-Loss: 1.6310229301452637, C-Loss: 1.7263274192810059\n",
      "Epoch: 228, Loss: 17.844890594482422,  L-Loss: 1.6013510823249817, C-Loss: 1.7044214606285095\n",
      "Epoch: 229, Loss: 18.1483793258667,  L-Loss: 1.6191486716270447, C-Loss: 1.7338805198669434\n",
      "Epoch: 230, Loss: 17.862903594970703,  L-Loss: 1.5942333936691284, C-Loss: 1.7065786719322205\n",
      "Epoch: 231, Loss: 17.95938014984131,  L-Loss: 1.6074104309082031, C-Loss: 1.7155675292015076\n",
      "Epoch: 232, Loss: 17.723752975463867,  L-Loss: 1.6079739928245544, C-Loss: 1.6919764876365662\n",
      "Epoch: 233, Loss: 17.695903778076172,  L-Loss: 1.61421138048172, C-Loss: 1.6888797879219055\n",
      "Epoch: 234, Loss: 17.66812038421631,  L-Loss: 1.5963004231452942, C-Loss: 1.6869970560073853\n",
      "Epoch: 235, Loss: 17.961194038391113,  L-Loss: 1.5993674397468567, C-Loss: 1.7161511182785034\n",
      "Epoch: 236, Loss: 17.826501846313477,  L-Loss: 1.5904431343078613, C-Loss: 1.7031280398368835\n",
      "Epoch: 237, Loss: 17.68081760406494,  L-Loss: 1.577237069606781, C-Loss: 1.6892198324203491\n",
      "Epoch: 238, Loss: 17.79190731048584,  L-Loss: 1.5816381573677063, C-Loss: 1.700108826160431\n",
      "Epoch: 239, Loss: 17.761274337768555,  L-Loss: 1.5929142832756042, C-Loss: 1.6964816451072693\n",
      "Epoch: 240, Loss: 17.86795997619629,  L-Loss: 1.6099085211753845, C-Loss: 1.706300675868988\n",
      "Epoch: 241, Loss: 17.791597366333008,  L-Loss: 1.6199453473091125, C-Loss: 1.6981624364852905\n",
      "Epoch: 242, Loss: 17.874055862426758,  L-Loss: 1.613835096359253, C-Loss: 1.706713855266571\n",
      "Epoch: 243, Loss: 18.22368812561035,  L-Loss: 1.6320862174034119, C-Loss: 1.7407644391059875\n",
      "Epoch: 244, Loss: 17.98966884613037,  L-Loss: 1.6107788681983948, C-Loss: 1.7184278964996338\n",
      "Epoch: 245, Loss: 17.85415267944336,  L-Loss: 1.5979397296905518, C-Loss: 1.7055182456970215\n",
      "Epoch: 246, Loss: 18.017292022705078,  L-Loss: 1.6159034371376038, C-Loss: 1.7209341526031494\n",
      "Epoch: 247, Loss: 18.012978553771973,  L-Loss: 1.6241053938865662, C-Loss: 1.7200925946235657\n",
      "Epoch: 248, Loss: 18.019203186035156,  L-Loss: 1.616094946861267, C-Loss: 1.721115529537201\n",
      "Epoch: 249, Loss: 17.93400478363037,  L-Loss: 1.6175472140312195, C-Loss: 1.7125231623649597\n",
      "Epoch: 250, Loss: 17.96086883544922,  L-Loss: 1.6087840795516968, C-Loss: 1.7156477570533752\n",
      "Epoch: 251, Loss: 18.036635398864746,  L-Loss: 1.6195752620697021, C-Loss: 1.7226847410202026\n",
      "Epoch: 252, Loss: 18.04810905456543,  L-Loss: 1.645283043384552, C-Loss: 1.7225467562675476\n",
      "Epoch: 253, Loss: 18.093708992004395,  L-Loss: 1.6467085480690002, C-Loss: 1.7270354628562927\n",
      "Epoch: 254, Loss: 18.397133827209473,  L-Loss: 1.6691421270370483, C-Loss: 1.7562562823295593\n",
      "Epoch: 255, Loss: 18.10468864440918,  L-Loss: 1.6184625029563904, C-Loss: 1.729545772075653\n",
      "Epoch: 256, Loss: 18.148730278015137,  L-Loss: 1.6338884234428406, C-Loss: 1.7331786155700684\n",
      "Epoch: 257, Loss: 18.200550079345703,  L-Loss: 1.6176552772521973, C-Loss: 1.7391722202301025\n",
      "Epoch: 258, Loss: 18.191370964050293,  L-Loss: 1.6249128580093384, C-Loss: 1.7378913760185242\n",
      "Epoch: 259, Loss: 18.02726936340332,  L-Loss: 1.6320913434028625, C-Loss: 1.72112238407135\n",
      "Epoch: 260, Loss: 17.972150802612305,  L-Loss: 1.6213581562042236, C-Loss: 1.7161471843719482\n",
      "Epoch: 261, Loss: 18.098037719726562,  L-Loss: 1.6327027678489685, C-Loss: 1.7281686663627625\n",
      "Epoch: 262, Loss: 18.04833221435547,  L-Loss: 1.6289211511611938, C-Loss: 1.7233871221542358\n",
      "Epoch: 263, Loss: 18.066410064697266,  L-Loss: 1.6236189007759094, C-Loss: 1.7254599928855896\n",
      "Epoch: 264, Loss: 18.023866653442383,  L-Loss: 1.629969835281372, C-Loss: 1.7208882570266724\n",
      "Epoch: 265, Loss: 18.025842666625977,  L-Loss: 1.636601746082306, C-Loss: 1.7207542061805725\n",
      "Epoch: 266, Loss: 18.08657455444336,  L-Loss: 1.6322970986366272, C-Loss: 1.7270426750183105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 267, Loss: 18.079489707946777,  L-Loss: 1.621586263179779, C-Loss: 1.7268697023391724\n",
      "Epoch: 268, Loss: 18.308140754699707,  L-Loss: 1.6358827948570251, C-Loss: 1.7490199208259583\n",
      "Epoch: 269, Loss: 18.13763427734375,  L-Loss: 1.627117931842804, C-Loss: 1.7324075102806091\n",
      "Epoch: 270, Loss: 18.068217277526855,  L-Loss: 1.6335572600364685, C-Loss: 1.725143849849701\n",
      "Epoch: 271, Loss: 18.13173198699951,  L-Loss: 1.6269174218177795, C-Loss: 1.7318273186683655\n",
      "Epoch: 272, Loss: 17.98744297027588,  L-Loss: 1.6130133867263794, C-Loss: 1.718093752861023\n",
      "Epoch: 273, Loss: 18.013243675231934,  L-Loss: 1.6146379113197327, C-Loss: 1.7205924987792969\n",
      "Epoch: 274, Loss: 17.87507915496826,  L-Loss: 1.610712707042694, C-Loss: 1.7069723010063171\n",
      "Epoch: 275, Loss: 17.891782760620117,  L-Loss: 1.5905780792236328, C-Loss: 1.709649384021759\n",
      "Epoch: 276, Loss: 17.79736328125,  L-Loss: 1.5743290185928345, C-Loss: 1.7010198831558228\n",
      "Out of patience at epoch 276\n",
      "Guardando mejor modelo en  ..\\models\\C2AE_alexnet_retrained\\base\\2\\26L.pth\n",
      "HS fold 2: 0.2383\n",
      "Predicciones guardadas en ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\2\\predictions.csv\n",
      "Usando top_labels previamente generados para 26 labels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 4096])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 26])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!\n",
      "Epoch: 0, Loss: 19.66266632080078,  L-Loss: 0.7443947792053223, C-Loss: 1.9290469288825989\n",
      "Epoch: 1, Loss: 19.56455421447754,  L-Loss: 0.5829387903213501, C-Loss: 1.9273083806037903\n",
      "Epoch: 2, Loss: 19.520183563232422,  L-Loss: 0.4964718371629715, C-Loss: 1.9271947741508484\n",
      "Epoch: 3, Loss: 19.466544151306152,  L-Loss: 0.4234028160572052, C-Loss: 1.925484299659729\n",
      "Epoch: 4, Loss: 19.437355041503906,  L-Loss: 0.3574696183204651, C-Loss: 1.9258620142936707\n",
      "Epoch: 5, Loss: 19.389156341552734,  L-Loss: 0.3114152103662491, C-Loss: 1.9233447909355164\n",
      "Epoch: 6, Loss: 19.368175506591797,  L-Loss: 0.2823478579521179, C-Loss: 1.9227001070976257\n",
      "Epoch: 7, Loss: 19.35400390625,  L-Loss: 0.2594934105873108, C-Loss: 1.9224257469177246\n",
      "Epoch: 8, Loss: 19.330224990844727,  L-Loss: 0.23939882963895798, C-Loss: 1.9210525155067444\n",
      "Epoch: 9, Loss: 19.31157684326172,  L-Loss: 0.22381868213415146, C-Loss: 1.9199668169021606\n",
      "Epoch: 10, Loss: 19.316139221191406,  L-Loss: 0.2116161286830902, C-Loss: 1.9210331439971924\n",
      "Epoch: 11, Loss: 19.296571731567383,  L-Loss: 0.20327214151620865, C-Loss: 1.919493556022644\n",
      "Epoch: 12, Loss: 19.26704978942871,  L-Loss: 0.19402741640806198, C-Loss: 1.917003571987152\n",
      "Epoch: 13, Loss: 19.260884284973145,  L-Loss: 0.18620024621486664, C-Loss: 1.916778326034546\n",
      "Epoch: 14, Loss: 19.245267868041992,  L-Loss: 0.18035201728343964, C-Loss: 1.9155091643333435\n",
      "Epoch: 15, Loss: 19.240169525146484,  L-Loss: 0.17548198997974396, C-Loss: 1.9152427911758423\n",
      "Epoch: 16, Loss: 19.22640895843506,  L-Loss: 0.1735076904296875, C-Loss: 1.9139655232429504\n",
      "Epoch: 17, Loss: 19.203868865966797,  L-Loss: 0.17105009406805038, C-Loss: 1.9118343591690063\n",
      "Epoch: 18, Loss: 19.196703910827637,  L-Loss: 0.16913507133722305, C-Loss: 1.9112135767936707\n",
      "Epoch: 19, Loss: 19.1836519241333,  L-Loss: 0.1718316450715065, C-Loss: 1.909773588180542\n",
      "Epoch: 20, Loss: 19.182600021362305,  L-Loss: 0.17178744077682495, C-Loss: 1.909670650959015\n",
      "Epoch: 21, Loss: 19.16471290588379,  L-Loss: 0.1724851205945015, C-Loss: 1.9078470468521118\n",
      "Epoch: 22, Loss: 19.156429290771484,  L-Loss: 0.16891862452030182, C-Loss: 1.9071969985961914\n",
      "Epoch: 23, Loss: 19.133028984069824,  L-Loss: 0.16926802694797516, C-Loss: 1.9048395156860352\n",
      "Epoch: 24, Loss: 19.120524406433105,  L-Loss: 0.17302045971155167, C-Loss: 1.903401494026184\n",
      "Epoch: 25, Loss: 19.126920700073242,  L-Loss: 0.18047375977039337, C-Loss: 1.9036682844161987\n",
      "Epoch: 26, Loss: 19.111824989318848,  L-Loss: 0.18376588821411133, C-Loss: 1.9019942879676819\n",
      "Epoch: 27, Loss: 19.095041275024414,  L-Loss: 0.18736902624368668, C-Loss: 1.9001357555389404\n",
      "Epoch: 28, Loss: 19.071099281311035,  L-Loss: 0.19261478632688522, C-Loss: 1.8974791765213013\n",
      "Epoch: 29, Loss: 19.04880142211914,  L-Loss: 0.1988205909729004, C-Loss: 1.894939124584198\n",
      "Epoch: 30, Loss: 19.048274993896484,  L-Loss: 0.21024786680936813, C-Loss: 1.8943151235580444\n",
      "Epoch: 31, Loss: 19.030010223388672,  L-Loss: 0.22413817793130875, C-Loss: 1.8917940855026245\n",
      "Epoch: 32, Loss: 19.00840950012207,  L-Loss: 0.23561003804206848, C-Loss: 1.8890604376792908\n",
      "Epoch: 33, Loss: 19.01973056793213,  L-Loss: 0.2495511844754219, C-Loss: 1.8894954919815063\n",
      "Epoch: 34, Loss: 18.974306106567383,  L-Loss: 0.2580065429210663, C-Loss: 1.8845303654670715\n",
      "Epoch: 35, Loss: 18.998744010925293,  L-Loss: 0.2738398611545563, C-Loss: 1.8861823678016663\n",
      "Epoch: 36, Loss: 18.95406436920166,  L-Loss: 0.27670717239379883, C-Loss: 1.8815710544586182\n",
      "Epoch: 37, Loss: 18.954236030578613,  L-Loss: 0.2837163954973221, C-Loss: 1.881237804889679\n",
      "Epoch: 38, Loss: 18.919971466064453,  L-Loss: 0.2922914922237396, C-Loss: 1.877382516860962\n",
      "Epoch: 39, Loss: 18.898859977722168,  L-Loss: 0.3125723898410797, C-Loss: 1.8742573261260986\n",
      "Epoch: 40, Loss: 18.923688888549805,  L-Loss: 0.3467358350753784, C-Loss: 1.8750321865081787\n",
      "Epoch: 41, Loss: 18.878012657165527,  L-Loss: 0.3647143393754959, C-Loss: 1.8695655465126038\n",
      "Epoch: 42, Loss: 18.905875205993652,  L-Loss: 0.39526909589767456, C-Loss: 1.8708240389823914\n",
      "Epoch: 43, Loss: 18.859309196472168,  L-Loss: 0.41028040647506714, C-Loss: 1.8654168844223022\n",
      "Epoch: 44, Loss: 18.865580558776855,  L-Loss: 0.4323756545782089, C-Loss: 1.8649393320083618\n",
      "Epoch: 45, Loss: 18.82222270965576,  L-Loss: 0.4517563283443451, C-Loss: 1.8596343994140625\n",
      "Epoch: 46, Loss: 18.828225135803223,  L-Loss: 0.48098908364772797, C-Loss: 1.8587730526924133\n",
      "Epoch: 47, Loss: 18.82827663421631,  L-Loss: 0.5113144069910049, C-Loss: 1.8572619557380676\n",
      "Epoch: 48, Loss: 18.800405502319336,  L-Loss: 0.5235331803560257, C-Loss: 1.8538638949394226\n",
      "Epoch: 49, Loss: 18.768481254577637,  L-Loss: 0.5369949638843536, C-Loss: 1.8499983549118042\n",
      "Epoch: 50, Loss: 18.741947174072266,  L-Loss: 0.5485454797744751, C-Loss: 1.8467673659324646\n",
      "Epoch: 51, Loss: 18.69472885131836,  L-Loss: 0.5709919035434723, C-Loss: 1.8409233689308167\n",
      "Epoch: 52, Loss: 18.758999824523926,  L-Loss: 0.6224719285964966, C-Loss: 1.8447763323783875\n",
      "Epoch: 53, Loss: 18.687134742736816,  L-Loss: 0.6385992169380188, C-Loss: 1.8367834687232971\n",
      "Epoch: 54, Loss: 18.686095237731934,  L-Loss: 0.656732976436615, C-Loss: 1.8357728123664856\n",
      "Epoch: 55, Loss: 18.61875629425049,  L-Loss: 0.6681777536869049, C-Loss: 1.828466773033142\n",
      "Epoch: 56, Loss: 18.620420455932617,  L-Loss: 0.6993980705738068, C-Loss: 1.8270721435546875\n",
      "Epoch: 57, Loss: 18.587825775146484,  L-Loss: 0.7374893724918365, C-Loss: 1.8219080567359924\n",
      "Epoch: 58, Loss: 18.54389762878418,  L-Loss: 0.7724328339099884, C-Loss: 1.8157681226730347\n",
      "Epoch: 59, Loss: 18.501665115356445,  L-Loss: 0.8001418113708496, C-Loss: 1.8101593852043152\n",
      "Epoch: 60, Loss: 18.46597957611084,  L-Loss: 0.8250548243522644, C-Loss: 1.8053451180458069\n",
      "Epoch: 61, Loss: 18.46328353881836,  L-Loss: 0.8315419554710388, C-Loss: 1.8047511577606201\n",
      "Epoch: 62, Loss: 18.43068218231201,  L-Loss: 0.8421801030635834, C-Loss: 1.800959289073944\n",
      "Epoch: 63, Loss: 18.40259075164795,  L-Loss: 0.8599539697170258, C-Loss: 1.797261357307434\n",
      "Epoch: 64, Loss: 18.410090446472168,  L-Loss: 0.9059043228626251, C-Loss: 1.7957138419151306\n",
      "Epoch: 65, Loss: 18.351962089538574,  L-Loss: 0.906336098909378, C-Loss: 1.7898794412612915\n",
      "Epoch: 66, Loss: 18.34185218811035,  L-Loss: 0.8973983526229858, C-Loss: 1.7893152236938477\n",
      "Epoch: 67, Loss: 18.316983222961426,  L-Loss: 0.871618002653122, C-Loss: 1.7881174683570862\n",
      "Epoch: 68, Loss: 18.21312713623047,  L-Loss: 0.8834079504013062, C-Loss: 1.777142345905304\n",
      "Epoch: 69, Loss: 18.272252082824707,  L-Loss: 0.9202931523323059, C-Loss: 1.7812106609344482\n",
      "Epoch: 70, Loss: 18.243194580078125,  L-Loss: 0.9625207483768463, C-Loss: 1.7761934399604797\n",
      "Epoch: 71, Loss: 18.23465633392334,  L-Loss: 0.9785118699073792, C-Loss: 1.7745401263237\n",
      "Epoch: 72, Loss: 18.189522743225098,  L-Loss: 1.002160519361496, C-Loss: 1.768844187259674\n",
      "Epoch: 73, Loss: 18.1441650390625,  L-Loss: 1.014778733253479, C-Loss: 1.7636775374412537\n",
      "Epoch: 74, Loss: 18.162887573242188,  L-Loss: 1.0074194371700287, C-Loss: 1.765917718410492\n",
      "Epoch: 75, Loss: 18.084532737731934,  L-Loss: 0.984445184469223, C-Loss: 1.7592310309410095\n",
      "Epoch: 76, Loss: 18.111144065856934,  L-Loss: 0.9877364039421082, C-Loss: 1.7617275714874268\n",
      "Epoch: 77, Loss: 18.058494567871094,  L-Loss: 0.9957164227962494, C-Loss: 1.7560635805130005\n",
      "Epoch: 78, Loss: 18.018856048583984,  L-Loss: 1.0007716119289398, C-Loss: 1.7518470287322998\n",
      "Epoch: 79, Loss: 18.01963996887207,  L-Loss: 1.011732131242752, C-Loss: 1.7513774037361145\n",
      "Epoch: 80, Loss: 17.99470806121826,  L-Loss: 1.039009153842926, C-Loss: 1.747520387172699\n",
      "Epoch: 81, Loss: 17.975292205810547,  L-Loss: 1.0669675469398499, C-Loss: 1.7441808581352234\n",
      "Epoch: 82, Loss: 17.97861671447754,  L-Loss: 1.0754334926605225, C-Loss: 1.744089961051941\n",
      "Epoch: 83, Loss: 17.96407985687256,  L-Loss: 1.0650759935379028, C-Loss: 1.7431541681289673\n",
      "Epoch: 84, Loss: 18.024490356445312,  L-Loss: 1.102344572544098, C-Loss: 1.7473318576812744\n",
      "Epoch: 85, Loss: 18.021625518798828,  L-Loss: 1.096026360988617, C-Loss: 1.7473612427711487\n",
      "Epoch: 86, Loss: 17.990474700927734,  L-Loss: 1.0664178729057312, C-Loss: 1.7457265853881836\n",
      "Epoch: 87, Loss: 17.942875862121582,  L-Loss: 1.0423184931278229, C-Loss: 1.7421716451644897\n",
      "Epoch: 88, Loss: 17.88968849182129,  L-Loss: 1.0579842925071716, C-Loss: 1.7360696196556091\n",
      "Epoch: 89, Loss: 17.941781997680664,  L-Loss: 1.0763933658599854, C-Loss: 1.7403584718704224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Loss: 17.851491928100586,  L-Loss: 1.0755265355110168, C-Loss: 1.7313729524612427\n",
      "Epoch: 91, Loss: 17.8331241607666,  L-Loss: 1.1030415296554565, C-Loss: 1.728160321712494\n",
      "Epoch: 92, Loss: 17.870558738708496,  L-Loss: 1.1335532069206238, C-Loss: 1.7303781509399414\n",
      "Epoch: 93, Loss: 17.87333106994629,  L-Loss: 1.1596688628196716, C-Loss: 1.7293496131896973\n",
      "Epoch: 94, Loss: 17.886521339416504,  L-Loss: 1.14767986536026, C-Loss: 1.7312681078910828\n",
      "Epoch: 95, Loss: 17.81496810913086,  L-Loss: 1.1129137873649597, C-Loss: 1.7258511185646057\n",
      "Epoch: 96, Loss: 17.808679580688477,  L-Loss: 1.1151254177093506, C-Loss: 1.7251116037368774\n",
      "Epoch: 97, Loss: 17.826704025268555,  L-Loss: 1.1283801794052124, C-Loss: 1.7262513637542725\n",
      "Epoch: 98, Loss: 17.801268577575684,  L-Loss: 1.1410135626792908, C-Loss: 1.7230761051177979\n",
      "Epoch: 99, Loss: 17.754258155822754,  L-Loss: 1.1558721661567688, C-Loss: 1.7176321744918823\n",
      "Epoch: 100, Loss: 17.811851501464844,  L-Loss: 1.1928860545158386, C-Loss: 1.7215407490730286\n",
      "Epoch: 101, Loss: 17.78005027770996,  L-Loss: 1.188164234161377, C-Loss: 1.7185968160629272\n",
      "Epoch: 102, Loss: 17.73039436340332,  L-Loss: 1.1586058735847473, C-Loss: 1.7151092290878296\n",
      "Epoch: 103, Loss: 17.731748580932617,  L-Loss: 1.1817675232887268, C-Loss: 1.7140865325927734\n",
      "Epoch: 104, Loss: 17.784226417541504,  L-Loss: 1.1969894766807556, C-Loss: 1.7185732126235962\n",
      "Epoch: 105, Loss: 17.798518180847168,  L-Loss: 1.239502489566803, C-Loss: 1.717876672744751\n",
      "Epoch: 106, Loss: 17.750922203063965,  L-Loss: 1.2276825904846191, C-Loss: 1.7137081027030945\n",
      "Epoch: 107, Loss: 17.74870204925537,  L-Loss: 1.2132620811462402, C-Loss: 1.714207112789154\n",
      "Epoch: 108, Loss: 17.74386215209961,  L-Loss: 1.2113585472106934, C-Loss: 1.713818371295929\n",
      "Epoch: 109, Loss: 17.718385696411133,  L-Loss: 1.206558108329773, C-Loss: 1.7115105986595154\n",
      "Epoch: 110, Loss: 17.708463668823242,  L-Loss: 1.2148647904396057, C-Loss: 1.7101030945777893\n",
      "Epoch: 111, Loss: 17.738234519958496,  L-Loss: 1.237219214439392, C-Loss: 1.7119624018669128\n",
      "Epoch: 112, Loss: 17.68381977081299,  L-Loss: 1.250688374042511, C-Loss: 1.7058476209640503\n",
      "Epoch: 113, Loss: 17.751626014709473,  L-Loss: 1.2633806467056274, C-Loss: 1.7119935154914856\n",
      "Epoch: 114, Loss: 17.72397232055664,  L-Loss: 1.26522958278656, C-Loss: 1.7091358304023743\n",
      "Epoch: 115, Loss: 17.71627426147461,  L-Loss: 1.2685827016830444, C-Loss: 1.7081982493400574\n",
      "Epoch: 116, Loss: 17.71162223815918,  L-Loss: 1.2798369526863098, C-Loss: 1.7071703672409058\n",
      "Epoch: 117, Loss: 17.73210906982422,  L-Loss: 1.2908803820610046, C-Loss: 1.708666980266571\n",
      "Epoch: 118, Loss: 17.69599151611328,  L-Loss: 1.2680703401565552, C-Loss: 1.706195592880249\n",
      "Epoch: 119, Loss: 17.66896629333496,  L-Loss: 1.2681906819343567, C-Loss: 1.7034871578216553\n",
      "Epoch: 120, Loss: 17.715292930603027,  L-Loss: 1.2958017587661743, C-Loss: 1.7067392468452454\n",
      "Epoch: 121, Loss: 17.719762802124023,  L-Loss: 1.3347919583320618, C-Loss: 1.7052367329597473\n",
      "Epoch: 122, Loss: 17.729745864868164,  L-Loss: 1.3609921336174011, C-Loss: 1.7049250602722168\n",
      "Epoch: 123, Loss: 17.73017120361328,  L-Loss: 1.3794057369232178, C-Loss: 1.7040468454360962\n",
      "Epoch: 124, Loss: 17.729910850524902,  L-Loss: 1.367583155632019, C-Loss: 1.7046119570732117\n",
      "Epoch: 125, Loss: 17.733223915100098,  L-Loss: 1.3713545203208923, C-Loss: 1.7047547698020935\n",
      "Epoch: 126, Loss: 17.736607551574707,  L-Loss: 1.390075147151947, C-Loss: 1.7041569948196411\n",
      "Epoch: 127, Loss: 17.74553394317627,  L-Loss: 1.4015317559242249, C-Loss: 1.7044768333435059\n",
      "Epoch: 128, Loss: 17.6807222366333,  L-Loss: 1.4023496508598328, C-Loss: 1.697954773902893\n",
      "Epoch: 129, Loss: 17.627009391784668,  L-Loss: 1.3957833647727966, C-Loss: 1.6929118037223816\n",
      "Epoch: 130, Loss: 17.618175506591797,  L-Loss: 1.414322853088379, C-Loss: 1.6911014914512634\n",
      "Epoch: 131, Loss: 17.694049835205078,  L-Loss: 1.4481830596923828, C-Loss: 1.6969958543777466\n",
      "Epoch: 132, Loss: 17.710780143737793,  L-Loss: 1.4563857316970825, C-Loss: 1.698258638381958\n",
      "Epoch: 133, Loss: 17.72557544708252,  L-Loss: 1.444638729095459, C-Loss: 1.7003256678581238\n",
      "Epoch: 134, Loss: 17.71599769592285,  L-Loss: 1.4282960295677185, C-Loss: 1.7001850605010986\n",
      "Epoch: 135, Loss: 17.68016529083252,  L-Loss: 1.446390688419342, C-Loss: 1.6956969499588013\n",
      "Epoch: 136, Loss: 17.696951866149902,  L-Loss: 1.4775651097297668, C-Loss: 1.695816993713379\n",
      "Epoch: 137, Loss: 17.72231388092041,  L-Loss: 1.501171588897705, C-Loss: 1.6971728801727295\n",
      "Epoch: 138, Loss: 17.75975227355957,  L-Loss: 1.519938349723816, C-Loss: 1.6999782919883728\n",
      "Epoch: 139, Loss: 17.711942672729492,  L-Loss: 1.5138468742370605, C-Loss: 1.6955018639564514\n",
      "Epoch: 140, Loss: 17.732938766479492,  L-Loss: 1.5097972750663757, C-Loss: 1.697804033756256\n",
      "Epoch: 141, Loss: 17.77251434326172,  L-Loss: 1.516653299331665, C-Loss: 1.7014188170433044\n",
      "Epoch: 142, Loss: 17.75430679321289,  L-Loss: 1.5194881558418274, C-Loss: 1.69945627450943\n",
      "Epoch: 143, Loss: 17.77375602722168,  L-Loss: 1.5250287652015686, C-Loss: 1.7011242508888245\n",
      "Epoch: 144, Loss: 17.808424949645996,  L-Loss: 1.5403625965118408, C-Loss: 1.7038243412971497\n",
      "Epoch: 145, Loss: 17.79945659637451,  L-Loss: 1.5500420928001404, C-Loss: 1.702443540096283\n",
      "Epoch: 146, Loss: 17.76836109161377,  L-Loss: 1.5427318215370178, C-Loss: 1.6996995210647583\n",
      "Epoch: 147, Loss: 17.65899658203125,  L-Loss: 1.5418720841407776, C-Loss: 1.6888060569763184\n",
      "Epoch: 148, Loss: 17.77495574951172,  L-Loss: 1.577889084815979, C-Loss: 1.6986011862754822\n",
      "Epoch: 149, Loss: 17.702143669128418,  L-Loss: 1.5888308882713318, C-Loss: 1.690772831439972\n",
      "Epoch: 150, Loss: 17.84122943878174,  L-Loss: 1.635113537311554, C-Loss: 1.7023673057556152\n",
      "Epoch: 151, Loss: 17.781076431274414,  L-Loss: 1.6466626524925232, C-Loss: 1.6957744359970093\n",
      "Epoch: 152, Loss: 17.741527557373047,  L-Loss: 1.6368346214294434, C-Loss: 1.6923111081123352\n",
      "Epoch: 153, Loss: 17.898176193237305,  L-Loss: 1.6515982747077942, C-Loss: 1.7072376608848572\n",
      "Epoch: 154, Loss: 17.872081756591797,  L-Loss: 1.6555599570274353, C-Loss: 1.7044301629066467\n",
      "Epoch: 155, Loss: 17.92872905731201,  L-Loss: 1.667888879776001, C-Loss: 1.709478497505188\n",
      "Epoch: 156, Loss: 17.816975593566895,  L-Loss: 1.639264464378357, C-Loss: 1.699734389781952\n",
      "Epoch: 157, Loss: 17.917948722839355,  L-Loss: 1.6837164759635925, C-Loss: 1.7076091170310974\n",
      "Epoch: 158, Loss: 17.894448280334473,  L-Loss: 1.6975879073143005, C-Loss: 1.7045655250549316\n",
      "Epoch: 159, Loss: 17.886919021606445,  L-Loss: 1.6740877628326416, C-Loss: 1.7049875855445862\n",
      "Epoch: 160, Loss: 17.868940353393555,  L-Loss: 1.6780999302864075, C-Loss: 1.702989101409912\n",
      "Epoch: 161, Loss: 17.95677661895752,  L-Loss: 1.7202930450439453, C-Loss: 1.7096630334854126\n",
      "Epoch: 162, Loss: 17.850192070007324,  L-Loss: 1.7025049328804016, C-Loss: 1.6998938918113708\n",
      "Epoch: 163, Loss: 17.828911781311035,  L-Loss: 1.6926228404045105, C-Loss: 1.6982600092887878\n",
      "Epoch: 164, Loss: 17.82441997528076,  L-Loss: 1.6797141432762146, C-Loss: 1.698456346988678\n",
      "Epoch: 165, Loss: 17.901580810546875,  L-Loss: 1.7272878885269165, C-Loss: 1.703793704509735\n",
      "Epoch: 166, Loss: 18.00800132751465,  L-Loss: 1.737104594707489, C-Loss: 1.7139447331428528\n",
      "Epoch: 167, Loss: 18.001614570617676,  L-Loss: 1.7530297636985779, C-Loss: 1.7125098705291748\n",
      "Epoch: 168, Loss: 17.94124698638916,  L-Loss: 1.7728272676467896, C-Loss: 1.7054833769798279\n",
      "Epoch: 169, Loss: 18.02244281768799,  L-Loss: 1.7865262627601624, C-Loss: 1.7129180431365967\n",
      "Epoch: 170, Loss: 18.014530181884766,  L-Loss: 1.7963163256645203, C-Loss: 1.711637258529663\n",
      "Epoch: 171, Loss: 17.96694850921631,  L-Loss: 1.7842050790786743, C-Loss: 1.7074847221374512\n",
      "Epoch: 172, Loss: 18.04039192199707,  L-Loss: 1.813579022884369, C-Loss: 1.71336030960083\n",
      "Epoch: 173, Loss: 17.895784378051758,  L-Loss: 1.7776642441749573, C-Loss: 1.7006952166557312\n",
      "Epoch: 174, Loss: 17.87749481201172,  L-Loss: 1.74684476852417, C-Loss: 1.7004072070121765\n",
      "Epoch: 175, Loss: 17.848685264587402,  L-Loss: 1.7512373328208923, C-Loss: 1.6973066329956055\n",
      "Epoch: 176, Loss: 17.989301681518555,  L-Loss: 1.7773833870887756, C-Loss: 1.710060954093933\n",
      "Epoch: 177, Loss: 18.00898265838623,  L-Loss: 1.7758126258850098, C-Loss: 1.7121077179908752\n",
      "Epoch: 178, Loss: 17.912257194519043,  L-Loss: 1.7519028782844543, C-Loss: 1.7036306262016296\n",
      "Epoch: 179, Loss: 18.004793167114258,  L-Loss: 1.7877858877182007, C-Loss: 1.7110900282859802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180, Loss: 18.121386528015137,  L-Loss: 1.8161712884902954, C-Loss: 1.721330165863037\n",
      "Epoch: 181, Loss: 17.97554588317871,  L-Loss: 1.8166930675506592, C-Loss: 1.7067200541496277\n",
      "Epoch: 182, Loss: 18.07870578765869,  L-Loss: 1.83971506357193, C-Loss: 1.715884804725647\n",
      "Epoch: 183, Loss: 17.869813919067383,  L-Loss: 1.8122708797454834, C-Loss: 1.6963678002357483\n",
      "Epoch: 184, Loss: 17.963315963745117,  L-Loss: 1.8034398555755615, C-Loss: 1.7061595916748047\n",
      "Epoch: 185, Loss: 17.997352600097656,  L-Loss: 1.8146327137947083, C-Loss: 1.7090036869049072\n",
      "Epoch: 186, Loss: 17.92420482635498,  L-Loss: 1.8104912638664246, C-Loss: 1.7018958926200867\n",
      "Epoch: 187, Loss: 17.998565673828125,  L-Loss: 1.811672568321228, C-Loss: 1.7092730402946472\n",
      "Epoch: 188, Loss: 18.05247688293457,  L-Loss: 1.8322847485542297, C-Loss: 1.713633418083191\n",
      "Epoch: 189, Loss: 17.993881225585938,  L-Loss: 1.8268662095069885, C-Loss: 1.7080448865890503\n",
      "Epoch: 190, Loss: 18.17231273651123,  L-Loss: 1.8512922525405884, C-Loss: 1.7246665358543396\n",
      "Epoch: 191, Loss: 18.03348159790039,  L-Loss: 1.8578013181686401, C-Loss: 1.7104581594467163\n",
      "Epoch: 192, Loss: 17.9545259475708,  L-Loss: 1.835792362689972, C-Loss: 1.7036629915237427\n",
      "Epoch: 193, Loss: 17.95196533203125,  L-Loss: 1.8233466148376465, C-Loss: 1.7040292024612427\n",
      "Epoch: 194, Loss: 17.88691520690918,  L-Loss: 1.816528856754303, C-Loss: 1.6978649497032166\n",
      "Epoch: 195, Loss: 17.924206733703613,  L-Loss: 1.8296682238578796, C-Loss: 1.7009372115135193\n",
      "Epoch: 196, Loss: 18.061558723449707,  L-Loss: 1.8488816022872925, C-Loss: 1.7137118577957153\n",
      "Epoch: 197, Loss: 18.137432098388672,  L-Loss: 1.8545031547546387, C-Loss: 1.7210179567337036\n",
      "Epoch: 198, Loss: 17.985328674316406,  L-Loss: 1.845449447631836, C-Loss: 1.7062603831291199\n",
      "Epoch: 199, Loss: 18.13365936279297,  L-Loss: 1.8468318581581116, C-Loss: 1.7210243344306946\n",
      "Epoch: 200, Loss: 18.241942405700684,  L-Loss: 1.8764644861221313, C-Loss: 1.730371117591858\n",
      "Epoch: 201, Loss: 18.207698822021484,  L-Loss: 1.8817667365074158, C-Loss: 1.7266814708709717\n",
      "Epoch: 202, Loss: 18.04975414276123,  L-Loss: 1.8553797602653503, C-Loss: 1.7122064232826233\n",
      "Epoch: 203, Loss: 17.970577239990234,  L-Loss: 1.845505714416504, C-Loss: 1.704782485961914\n",
      "Epoch: 204, Loss: 18.056751251220703,  L-Loss: 1.8700628876686096, C-Loss: 1.712172031402588\n",
      "Epoch: 205, Loss: 18.171961784362793,  L-Loss: 1.914189338684082, C-Loss: 1.721486747264862\n",
      "Epoch: 206, Loss: 18.394003868103027,  L-Loss: 1.9500967860221863, C-Loss: 1.7418955564498901\n",
      "Epoch: 207, Loss: 18.418707847595215,  L-Loss: 1.9473562240600586, C-Loss: 1.7445030808448792\n",
      "Epoch: 208, Loss: 18.062355041503906,  L-Loss: 1.8922863006591797, C-Loss: 1.7116212248802185\n",
      "Epoch: 209, Loss: 18.374263763427734,  L-Loss: 1.922894299030304, C-Loss: 1.7412816286087036\n",
      "Epoch: 210, Loss: 18.360172271728516,  L-Loss: 1.9314141869544983, C-Loss: 1.7394464015960693\n",
      "Epoch: 211, Loss: 18.304699897766113,  L-Loss: 1.9069375395774841, C-Loss: 1.7351231575012207\n",
      "Epoch: 212, Loss: 18.488563537597656,  L-Loss: 1.9237813353538513, C-Loss: 1.7526671886444092\n",
      "Epoch: 213, Loss: 18.34159278869629,  L-Loss: 1.906228244304657, C-Loss: 1.7388479113578796\n",
      "Epoch: 214, Loss: 18.211687088012695,  L-Loss: 1.9089075922966003, C-Loss: 1.7257233262062073\n",
      "Epoch: 215, Loss: 18.26338768005371,  L-Loss: 1.9100252389907837, C-Loss: 1.7308375239372253\n",
      "Epoch: 216, Loss: 18.187349319458008,  L-Loss: 1.9241934418678284, C-Loss: 1.7225252389907837\n",
      "Epoch: 217, Loss: 18.31372356414795,  L-Loss: 1.9431275725364685, C-Loss: 1.7342159152030945\n",
      "Epoch: 218, Loss: 18.1760892868042,  L-Loss: 1.9056949615478516, C-Loss: 1.7223241329193115\n",
      "Epoch: 219, Loss: 18.124401092529297,  L-Loss: 1.8873404264450073, C-Loss: 1.7180730700492859\n",
      "Epoch: 220, Loss: 18.378137588500977,  L-Loss: 1.932621955871582, C-Loss: 1.7411826848983765\n",
      "Epoch: 221, Loss: 18.37971782684326,  L-Loss: 1.9327621459960938, C-Loss: 1.7413336634635925\n",
      "Epoch: 222, Loss: 18.21162509918213,  L-Loss: 1.92738276720047, C-Loss: 1.7247933149337769\n",
      "Epoch: 223, Loss: 18.224748611450195,  L-Loss: 1.935238003730774, C-Loss: 1.7257129549980164\n",
      "Epoch: 224, Loss: 18.215632438659668,  L-Loss: 1.924464762210846, C-Loss: 1.7253400087356567\n",
      "Epoch: 225, Loss: 18.210739135742188,  L-Loss: 1.9213888049125671, C-Loss: 1.725004494190216\n",
      "Epoch: 226, Loss: 18.260497093200684,  L-Loss: 1.9164591431617737, C-Loss: 1.7302268147468567\n",
      "Epoch: 227, Loss: 18.1990385055542,  L-Loss: 1.923577606678009, C-Loss: 1.7237250208854675\n",
      "Epoch: 228, Loss: 18.208741188049316,  L-Loss: 1.9078738689422607, C-Loss: 1.725480318069458\n",
      "Epoch: 229, Loss: 18.07127094268799,  L-Loss: 1.8995518684387207, C-Loss: 1.7121495008468628\n",
      "Epoch: 230, Loss: 18.271252632141113,  L-Loss: 1.924302577972412, C-Loss: 1.7309101819992065\n",
      "Epoch: 231, Loss: 18.418694496154785,  L-Loss: 1.9617543816566467, C-Loss: 1.7437817454338074\n",
      "Epoch: 232, Loss: 18.490877151489258,  L-Loss: 1.9841121435165405, C-Loss: 1.749882161617279\n",
      "Epoch: 233, Loss: 18.13193130493164,  L-Loss: 1.9486311674118042, C-Loss: 1.7157616019248962\n",
      "Epoch: 234, Loss: 18.370302200317383,  L-Loss: 1.966411054134369, C-Loss: 1.7387096881866455\n",
      "Epoch: 235, Loss: 18.42521858215332,  L-Loss: 1.971348226070404, C-Loss: 1.7439544796943665\n",
      "Epoch: 236, Loss: 18.425006866455078,  L-Loss: 1.964997410774231, C-Loss: 1.7442508935928345\n",
      "Epoch: 237, Loss: 18.55697727203369,  L-Loss: 1.9742176532745361, C-Loss: 1.7569867968559265\n",
      "Epoch: 238, Loss: 18.72465991973877,  L-Loss: 2.010705053806305, C-Loss: 1.7719308137893677\n",
      "Epoch: 239, Loss: 18.63990879058838,  L-Loss: 1.9938697218894958, C-Loss: 1.7642974257469177\n",
      "Epoch: 240, Loss: 18.624533653259277,  L-Loss: 1.9790855646133423, C-Loss: 1.7634990811347961\n",
      "Epoch: 241, Loss: 18.815909385681152,  L-Loss: 2.0242764949798584, C-Loss: 1.7803770899772644\n",
      "Epoch: 242, Loss: 18.61163902282715,  L-Loss: 2.008109748363495, C-Loss: 1.760758399963379\n",
      "Epoch: 243, Loss: 18.63047695159912,  L-Loss: 1.9830154776573181, C-Loss: 1.7638969421386719\n",
      "Epoch: 244, Loss: 18.59117031097412,  L-Loss: 1.982092797756195, C-Loss: 1.7600123882293701\n",
      "Epoch: 245, Loss: 18.536640167236328,  L-Loss: 1.9823392033576965, C-Loss: 1.7545470595359802\n",
      "Epoch: 246, Loss: 18.546359062194824,  L-Loss: 1.977419137954712, C-Loss: 1.7557649612426758\n",
      "Epoch: 247, Loss: 18.748469352722168,  L-Loss: 2.0119516253471375, C-Loss: 1.7742493748664856\n",
      "Out of patience at epoch 247\n",
      "Guardando mejor modelo en  ..\\models\\C2AE_alexnet_retrained\\base\\3\\26L.pth\n",
      "HS fold 3: 0.1877\n",
      "Predicciones guardadas en ..\\outputs\\C2AE_alexnet_retrained\\base\\26L\\3\\predictions.csv\n",
      "HS Final:  0.2223\n",
      "F1 Final:  0.3251\n",
      "1MR Final:  0.6147\n",
      "5MR Final:  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum_f1 = 0\n",
    "sum_recall = 0\n",
    "sum_precision = 0\n",
    "sum_acc = 0\n",
    "sum_hl = 0\n",
    "sum_emr = 0\n",
    "sum_hs = 0\n",
    "sum_mr1 = 0\n",
    "sum_mr2 = 0\n",
    "sum_mr3 = 0\n",
    "sum_mr4 = 0\n",
    "sum_mr5 = 0\n",
    "\n",
    "for i in range(0, K):\n",
    "    fold = Kfolds[i]\n",
    "    labels_dir = fold['labels_dir']\n",
    "    output_dir = fold['output_dir']\n",
    "    model_path = fold['model_path']\n",
    "    features_dir = fold['features_dir']\n",
    "    # Carga de top labels\n",
    "    train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "    \n",
    "    if not os.path.isfile(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle')):\n",
    "        print(f\"Creando top_labels para {NUM_LABELS} labels\")\n",
    "        top_labels = pruner.filter_labels(train_labels)\n",
    "        pruner.set_top_labels(top_labels)\n",
    "        \n",
    "        save = input(f\"Se creará un archivo nuevo para {len(top_labels)} labels. Desea continuar? (y/n)\")\n",
    "        if save == \"y\":\n",
    "            with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'wb') as f:\n",
    "                pickle.dump(top_labels, f)\n",
    "            print(\"Top labels creado con éxito\")\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No se logró cargar top_labels\")\n",
    "            \n",
    "    else: \n",
    "        print(f\"Usando top_labels previamente generados para {NUM_LABELS} labels\")\n",
    "        with open(os.path.join(root_dir, 'labels', f'top_{NUM_LABELS}L.pickle'), 'rb') as f:\n",
    "            top_labels = pickle.load(f)\n",
    "\n",
    "    pruner = KunischPruner(NUM_LABELS)\n",
    "    pruner.set_top_labels(top_labels)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    if PATTERNS_AS_FEATURES:\n",
    "        train_x = np.load(os.path.join(features_dir, 'kunisch-train-features.pkl'), allow_pickle=True)\n",
    "        train_y = np.load(os.path.join(features_dir, 'kunisch-train-labels.pkl'), allow_pickle=True)\n",
    "\n",
    "        test_x = np.load(os.path.join(features_dir, 'kunisch-test-features.pkl'), allow_pickle=True)\n",
    "        test_y = np.load(os.path.join(features_dir, 'kunisch-test-labels.pkl'), allow_pickle=True)\n",
    "\n",
    "    else:\n",
    "        train_labels = pd.read_json(os.path.join(labels_dir, 'augmented_train_df.json'), orient='index')\n",
    "        train_labels = pruner.filter_df(train_labels)\n",
    "        \n",
    "        train_x = pd.read_json(os.path.join(features_dir, 'augmented_train_df.json'), orient='index').values\n",
    "        train_y = train_labels.values\n",
    "        \n",
    "        test_labels = pd.read_json(os.path.join(labels_dir, 'test_df.json'), orient='index')\n",
    "        test_labels = pruner.filter_df(test_labels)\n",
    "        \n",
    "        test_x = pd.read_json(os.path.join(features_dir, 'test_df.json'), orient='index').values\n",
    "        test_y = test_labels.values\n",
    "        \n",
    "    train_dataset = TensorDataset(torch.tensor(train_x, \n",
    "                                               device=device, \n",
    "                                               dtype=torch.float),\n",
    "                                  torch.tensor(train_y, \n",
    "                                               device=device,\n",
    "                                               dtype=torch.float))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_x, \n",
    "                                              device=device, \n",
    "                                              dtype=torch.float), \n",
    "                                 torch.tensor(test_y, \n",
    "                                              device=device, \n",
    "                                              dtype=torch.float))\n",
    "\n",
    "    display(train_dataset[:][0].shape, train_dataset[:][1].shape, test_dataset[:][0].shape, test_dataset[:][1].shape)\n",
    "\n",
    "    # Training configs.\n",
    "    num_epochs = NUM_EPOCHS\n",
    "    batch_size = BATCH_SIZE\n",
    "    lr = 0.0001\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # # Scene config\n",
    "    feat_dim = FEATURES_DIM\n",
    "    latent_dim = 70\n",
    "    num_labels = NUM_LABELS\n",
    "    fx_h_dim= 120\n",
    "    fe_h_dim= 120\n",
    "    fd_h_dim= 120\n",
    "\n",
    "    # Scene models.\n",
    "    Fx_scene = Fx(feat_dim, fx_h_dim, fx_h_dim, latent_dim)\n",
    "    Fe_scene = Fe(num_labels, fe_h_dim, latent_dim)\n",
    "    Fd_scene = Fd(latent_dim, fd_h_dim, num_labels, fin_act=torch.sigmoid)\n",
    "\n",
    "    # Initializing net.\n",
    "    net = C2AE(Fx_scene, Fe_scene, Fd_scene, beta=0.5, alpha=10, emb_lambda=0.01, latent_dim=latent_dim, device=device)\n",
    "    net = net.to(device)\n",
    "\n",
    "\n",
    "    # Doing weight_decay here is eqiv to adding the L2 norm.\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Starting training!\")\n",
    "    best_weights = None\n",
    "    best_loss = 9999999\n",
    "    patience = PATIENCE\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs+1): \n",
    "        # Training.\n",
    "        net.train()\n",
    "        loss_tracker = 0.0\n",
    "        latent_loss_tracker = 0.0\n",
    "        cor_loss_tracker = 0.0\n",
    "        for x, y in train_dataloader:\n",
    "            optimizer.zero_grad()      \n",
    "\n",
    "            # Pass x, y to network. Retrieve both encodings, and decoding of ys encoding.\n",
    "            fx_x, fe_y, fd_z = net(x, y)\n",
    "            # Calc loss.\n",
    "            l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "            # Normalize losses by batch.\n",
    "            l_loss /= x.shape[0]\n",
    "            c_loss /= x.shape[0]\n",
    "            loss = net.beta*l_loss + net.alpha*c_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_tracker+=loss.item()\n",
    "            latent_loss_tracker+=l_loss.item()\n",
    "            cor_loss_tracker+=c_loss.item()\n",
    "\n",
    "        # Evaluation\n",
    "        net.eval()\n",
    "        loss_tracker = 0.0\n",
    "        latent_loss_tracker = 0.0\n",
    "        cor_loss_tracker = 0.0\n",
    "        acc_track = 0.0\n",
    "        for x, y in test_dataloader:\n",
    "            # evaluation only requires x. As its just Fd(Fx(x))\n",
    "            fx_x, fe_y = net.Fx(x), net.Fe(y)\n",
    "            fd_z = net.Fd(fx_x)\n",
    "\n",
    "            l_loss, c_loss = net.losses(fx_x, fe_y, fd_z, y)\n",
    "            # Normalize losses by batch.\n",
    "            l_loss /= x.shape[0]\n",
    "            c_loss /= x.shape[0]\n",
    "            loss = net.beta*l_loss + net.alpha*c_loss\n",
    "\n",
    "            latent_loss_tracker += l_loss.item()\n",
    "            cor_loss_tracker += c_loss.item()\n",
    "            loss_tracker += loss.item()\n",
    "            lab_preds = torch.round(net.Fd(net.Fx(x))).cpu().detach().numpy()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss_tracker},  L-Loss: {latent_loss_tracker}, C-Loss: {cor_loss_tracker}\")\n",
    "        if cor_loss_tracker < best_loss:\n",
    "            best_loss = cor_loss_tracker\n",
    "            best_weights = net.state_dict()\n",
    "            bad_epochs = 0\n",
    "            best_epoch = epoch\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs == patience:\n",
    "                print(f\"Out of patience at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "    if SAVE:\n",
    "        print(\"Guardando mejor modelo en \", model_path)\n",
    "        torch.save(best_weights, model_path)\n",
    "\n",
    "    eval_net = load_model(C2AE, model_path, \n",
    "                          Fx=Fx_scene, Fe=Fe_scene, Fd=Fd_scene, device=device).to(device)\n",
    "\n",
    "    y_pred, y_true = get_predictions(net, [test_dataset], device)\n",
    "\n",
    "    metrics = KunischMetrics(y_true, y_pred)\n",
    "    sum_f1 += metrics.f1()\n",
    "    sum_recall += metrics.recall()\n",
    "    sum_precision += metrics.precision()\n",
    "    sum_acc += metrics.acc()\n",
    "    sum_hl += metrics.hl()\n",
    "    sum_emr += metrics.emr()\n",
    "    sum_hs += metrics.hs()\n",
    "    sum_mr1 += metrics.mr1()\n",
    "    sum_mr2 += metrics.mr2()\n",
    "    sum_mr3 += metrics.mr3()\n",
    "    sum_mr4 += metrics.mr4()\n",
    "    sum_mr5 += metrics.mr5()\n",
    "\n",
    "    print(f\"HS fold {i}: {metrics.hs()}\")\n",
    "\n",
    "    if SAVE:\n",
    "        save_df = pd.DataFrame(y_pred)\n",
    "        save_df.to_csv(os.path.join(output_dir, 'predictions.csv'))\n",
    "        print(f\"Predicciones guardadas en {os.path.join(output_dir, 'predictions.csv')}\")\n",
    "\n",
    "        \n",
    "avg_f1 = round(sum_f1/K, 4)\n",
    "avg_recall = round(sum_recall/K, 4)\n",
    "avg_precision = round(sum_precision/K, 4)\n",
    "avg_acc = round(sum_acc/K, 4)\n",
    "avg_hl = round(sum_hl/K, 4)\n",
    "avg_emr = round(sum_emr/K, 4)\n",
    "avg_hs = round(sum_hs/K, 4)\n",
    "avg_mr1 = round(sum_mr1/K, 4)\n",
    "avg_mr2 = round(sum_mr2/K, 4)\n",
    "avg_mr3 = round(sum_mr3/K, 4)\n",
    "avg_mr4 = round(sum_mr4/K, 4)\n",
    "avg_mr5 = round(sum_mr5/K, 4)\n",
    "\n",
    "metadata = {\n",
    "'data_flags': data_flags,\n",
    "'patience': PATIENCE,\n",
    "'batch_size': BATCH_SIZE,\n",
    "'optimizer': (type (optimizer).__name__),\n",
    "'epochs': num_epochs,\n",
    "'num_labels': NUM_LABELS,\n",
    "'f1': avg_f1,\n",
    "'recall': avg_recall,\n",
    "'precision': avg_precision,\n",
    "'acc': avg_acc,\n",
    "'hl': avg_hl,\n",
    "'emr': avg_emr,\n",
    "'hs': avg_hs,\n",
    "'mr1': avg_mr1,\n",
    "'mr2': avg_mr2,\n",
    "'mr3': avg_mr3,\n",
    "'mr4': avg_mr4,\n",
    "'mr5': avg_mr5\n",
    "}\n",
    "\n",
    "print(\"HS Final: \", avg_hs)\n",
    "print(\"F1 Final: \", avg_f1)\n",
    "print(\"1MR Final: \", avg_mr1)\n",
    "print(\"5MR Final: \", avg_mr5)\n",
    "\n",
    "if SAVE:\n",
    "    metadf = pd.DataFrame.from_dict(metadata, orient='index')\n",
    "    # output_dir pero sin numero de fold\n",
    "    os.makedirs(os.path.join(root_dir, 'outputs', 'C2AE_alexnet', data_flags, exp_name), exist_ok=True)\n",
    "    metadf.to_csv(os.path.join(root_dir, \"outputs\", \"C2AE_alexnet\", data_flags, exp_name, 'metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
