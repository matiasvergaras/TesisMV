{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzn7gj5P3DXt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aprendizaje Multietiqueta de Patrones Geométricos en Objetos de Herencia Cultural\n",
    "# Split and data augmentation\n",
    "## Seminario de Tesis II, Primavera 2022\n",
    "### Master of Data Science. Universidad de Chile.\n",
    "#### Prof. guía: Benjamín Bustos - Prof. coguía: Iván Sipirán\n",
    "#### Autor: Matías Vergara\n",
    "\n",
    "Performs data augmentation on patterns through the application of linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "woMl7NKb3LyB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bDe9EneU2rIG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import imgaug.augmenters as aug\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRTWM7ng3lwE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dKnmKrA63npf",
    "outputId": "1c315239-a236-44a4-9349-15ef4ac5db02",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mounting google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = 'drive/MyDrive/TesisMV/'\n",
    "except:\n",
    "    root_dir = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is enough to select ds flags and number of crops (this one will have effect depending on flags) and then run the rest of the cells. This way, two folders will be created: a labels one and a patterns one. Both of them will be named after the selected flags, separed by \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DS_FLAGS = ['ref']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'gausblur': [gausblur]\n",
    "              # 'msblur': [msblur]\n",
    "              # 'mtnblur': [mtnblur]\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 2\n",
    "RANDOM_TIMES = 1\n",
    "ELASTIC_TIMES = 1\n",
    "GAUSBLUR_TIMES = 2\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "         'gausblur': GAUSBLUR_TIMES,\n",
    "}\n",
    "\n",
    "# Variables globales\n",
    "SUBCHAPTERS = False\n",
    "K = 4 # k fold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ref']\n"
     ]
    }
   ],
   "source": [
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "if SUBCHAPTERS:\n",
    "    data_flags = 'subchapters/' + data_flags\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic', 'gausblur']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "        \n",
    "print(DS_FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<function invertX at 0x000001AEE4A137F0>]\n",
      "[<function invertY at 0x000001AEE4A13880>]\n",
      "[<function invertX at 0x000001AEE4A137F0>, <function invertY at 0x000001AEE4A13880>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<function __main__.invertX(path)>, <function __main__.invertY(path)>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate90(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"90\", rotated)\n",
    "    return rotated, \"rot90\"\n",
    "\n",
    "\n",
    "def rotate180(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    # cv2.imshow(\"180\", rotated)\n",
    "    return rotated, \"rot180\"\n",
    "\n",
    "\n",
    "def rotate270(path):\n",
    "    image = cv2.imread(path)\n",
    "    rotated = cv2.rotate(image, cv2.ROTATE_180)\n",
    "    rotated = cv2.rotate(rotated, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # cv2.imshow(\"270\", rotated)\n",
    "    return rotated, \"rot270\"\n",
    "\n",
    "\n",
    "def invertX(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 1)\n",
    "    # cv2.imshow(\"flipX\", flipped)\n",
    "    return flipped, \"invX\"\n",
    "\n",
    "\n",
    "def invertY(path):\n",
    "    image = cv2.imread(path)\n",
    "    flipped = cv2.flip(image, 0)\n",
    "    # cv2.imshow(\"flipY\", flipped)\n",
    "    return flipped, \"invY\"\n",
    "\n",
    "\n",
    "def crop(path, min_width = 1/2, min_height= 1/2, max_width = 1/1.1,\n",
    "         max_height = 1/1.1):\n",
    "    image = cv2.imread(path)\n",
    "    height, width = image.shape[0], image.shape[1] # Caution: there are images in RGB and GS\n",
    "    min_width = math.ceil(width * min_width)\n",
    "    min_height = math.ceil(height * min_height)\n",
    "    max_width = math.ceil(width * max_width)\n",
    "    max_height = math.ceil(height * max_height)\n",
    "    x1 = random.randint(0, width - min_width)\n",
    "    w = random.randint(min_width, width - x1)\n",
    "    y1 = random.randint(0, height - min_height)\n",
    "    h = random.randint(min_height, height - y1)\n",
    "    crop = image[y1:y1+h, x1:x1+w]\n",
    "    return crop, \"crop\"\n",
    "\n",
    "def blur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.AverageBlur(k=(4, 11))(image=image)\n",
    "    return image_aug, \"blur\"\n",
    "\n",
    "def gausblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.GaussianBlur(sigma=random.uniform(2,10))(image=image)\n",
    "    return image_aug, \"gausblur\"\n",
    "\n",
    "def msblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.MeanShiftBlur()(image=image)\n",
    "    return image_aug, \"msblur\"\n",
    "\n",
    "def mtnblur(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.MotionBlur(random.randint(10,359))(image=image)\n",
    "    return image_aug, \"mtnblur\"\n",
    "\n",
    "\n",
    "def emboss(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))(image = image)\n",
    "    return image_aug, \"embs\"\n",
    "\n",
    "def elastic(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.PiecewiseAffine(scale=(0.03, 0.075))(image = image)\n",
    "    return image_aug, \"elastic\"\n",
    "\n",
    "def randaug(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.RandAugment(m=(2, 9))(image = image)\n",
    "    return image_aug, \"randaug\"\n",
    "\n",
    "def snow(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Snowflakes(flake_size=(0.6, 0.5), speed=(0.2, 0.5))(image = image)\n",
    "    return image_aug, \"snow\"\n",
    "\n",
    "\n",
    "def rain(path):\n",
    "    image = cv2.imread(path)\n",
    "    image_aug = aug.Rain(speed=(0.1, 0.5))(image = image)\n",
    "    return image_aug, \"rain\"\n",
    "\n",
    "\n",
    "def apply_transformations(pin, pout, transformations):\n",
    "    # ../patterns/originals/84e/84e.png\n",
    "    new_names = []\n",
    "    i = 0\n",
    "    for transformation in transformations:\n",
    "        result, transf_name = transformation(pin)\n",
    "        if transf_name in MULTIPLE_TRANSF: # special treatment for crops and randoms\n",
    "          transf_name += str(i)\n",
    "          i+=1\n",
    "        pin = os.path.normpath(pin)\n",
    "        path_els = pin.split(os.sep)\n",
    "        obj_name = path_els[3] + \"_\" + transf_name\n",
    "        filename = obj_name + \".png\"\n",
    "        os.makedirs(pout, exist_ok = True)\n",
    "        cv2.imwrite(os.path.join(pout, filename), result)\n",
    "        new_names.append(obj_name)\n",
    "    return new_names\n",
    "\n",
    "# Select data augmentation functions based on data flags\n",
    "\n",
    "MAP_FLAGS = {'ref': [invertX, invertY],\n",
    "             'rot': [rotate90, rotate180, rotate270],\n",
    "             'crop': [crop],\n",
    "             'blur': [blur],\n",
    "             'gausblur': [gausblur],\n",
    "             'mtnblur': [mtnblur],\n",
    "             'msblur': [msblur],\n",
    "             'emboss': [emboss],\n",
    "             'randaug': [randaug],\n",
    "             'rain': [rain],\n",
    "             'elastic': [elastic]\n",
    "             # snow is not working properly\n",
    "             }\n",
    "\n",
    "ALLOWED_TRANSFORMATIONS = []\n",
    "for f in DS_FLAGS:\n",
    "    ALLOWED_TRANSFORMATIONS += MAP_FLAGS[f]\n",
    "\n",
    "HOR_TRANSFORMATIONS = [invertX, rotate180, blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "VER_TRANSFORMATIONS = [invertY, rotate180, blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "COMMON_TRANSFORMATIONS = [invertX, invertY, rotate90, rotate180, rotate270,\n",
    "                          blur, rain, emboss, mtnblur, gausblur, msblur]\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        HOR_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "        VER_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "        COMMON_TRANSFORMATIONS += MAP_FLAGS[t] * MAP_TIMES[t]\n",
    "\n",
    "\n",
    "def mergeTransformations(flags, map_flags, map_times, trans_list): \n",
    "    # could be improved a lot \n",
    "    for k, v in map_flags.items():\n",
    "        if k not in flags:\n",
    "            for el in v:\n",
    "                while el in trans_list:\n",
    "                    trans_list.remove(el)\n",
    "    print(trans_list)\n",
    "    return trans_list\n",
    "\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, MAP_TIMES, HOR_TRANSFORMATIONS)\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, MAP_TIMES, VER_TRANSFORMATIONS)\n",
    "mergeTransformations(DS_FLAGS, MAP_FLAGS, MAP_TIMES, COMMON_TRANSFORMATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando indices previamente generados\n",
      "Elementos del fold 0\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['14j' '3i' '24b' '37b' '44n']\n",
      "-- Elementos de validación: 78 - Muestra: ['64c' '89e' '51c' '76h' '32h']\n",
      "-- Elementos de test: 194 - Muestra: ['40b' '16l' '19e' '77j' '70q']\n",
      "Elementos del fold 1\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['3e' '3a' '83a' '78b' '16e']\n",
      "-- Elementos de validación: 78 - Muestra: ['10b' '42b' '70k' '45c' '37i']\n",
      "-- Elementos de test: 194 - Muestra: ['46b' '17f' '92d' '8d' '31d']\n",
      "Elementos del fold 2\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['29c' '4d' '72e' '34d' '11j']\n",
      "-- Elementos de validación: 78 - Muestra: ['77e' '65d' '81g' '16h' '16a']\n",
      "-- Elementos de test: 194 - Muestra: ['46e' '49b' '14a' '26b' '20a']\n",
      "Elementos del fold 3\n",
      "-- Elementos de entrenamiento: 504 - Muestra: ['44l' '66f' '18g' '8c' '95c']\n",
      "-- Elementos de validación: 78 - Muestra: ['64h' '88c' '27c' '91a' '45c']\n",
      "-- Elementos de test: 194 - Muestra: ['44c' '86d' '11k' '47k' '78m']\n"
     ]
    }
   ],
   "source": [
    "labels_dir = os.path.join(root_dir, \"labels\")\n",
    "df = pd.read_json(os.path.join(labels_dir, \"normalized_df.json\"), orient='index')\n",
    "classes = pd.read_csv(os.path.join(labels_dir, \"class_labels.csv\"), index_col=0)\n",
    "colnames = df.columns\n",
    "holdout_dir = os.path.join(labels_dir, \"holdout\")\n",
    "os.makedirs(holdout_dir, exist_ok = True)\n",
    "\n",
    "GENERAR = True\n",
    "ERROR = False\n",
    "\n",
    "train_sets = []\n",
    "val_sets = []\n",
    "test_sets = []\n",
    "\n",
    "for i in range(0, K):\n",
    "    found_train_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_train_\" + str(i) + \".npy\")) \n",
    "    found_val_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_val_\" + str(i) + \".npy\"))\n",
    "    found_test_elems = os.path.isfile(os.path.join(holdout_dir, \"elem_test_\" + str(i) + \".npy\"))\n",
    "    \n",
    "    if (not found_train_elems) or (not found_val_elems) or (not found_test_elems):\n",
    "        print(\"No se encontraron los datos del fold \", i)\n",
    "        ERROR = True\n",
    "\n",
    "if ERROR and not GENERAR:\n",
    "        raise Exception(\"\"\"\n",
    "        No hay particiones para CV pero GENERAR está seteado como False. \n",
    "        Revise los paths o cambie el valor de GENERAR a True.\n",
    "        \"\"\")\n",
    "        \n",
    "if not ERROR: #archivos existian desde antes\n",
    "    print(\"Cargando indices previamente generados\")\n",
    "    for i in range(0, K):\n",
    "        elem_train = elem_test = elem_val = None\n",
    "        with open(os.path.join(holdout_dir, f'elem_train_{i}.npy'), 'rb') as f:\n",
    "            elem_train = np.load(f, allow_pickle = True)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_val_{i}.npy'), 'rb') as f:\n",
    "            elem_val = np.load(f, allow_pickle = True)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_test_{i}.npy'), 'rb') as f:\n",
    "            elem_test = np.load(f, allow_pickle = True)\n",
    "\n",
    "        train_sets.append(elem_train)\n",
    "        val_sets.append(elem_val)\n",
    "        test_sets.append(elem_test)\n",
    "\n",
    "        print(f\"Elementos del fold {i}\")\n",
    "        print(f\"-- Elementos de entrenamiento: {len(elem_train)} - Muestra: {elem_train[0:5]}\" )\n",
    "        print(f\"-- Elementos de validación: {len(elem_val)} - Muestra: {elem_val[0:5]}\")\n",
    "        print(f\"-- Elementos de test: {len(elem_test)} - Muestra: {elem_test[0:5]}\")\n",
    "        \n",
    "if ERROR and GENERAR:\n",
    "    print(\"GENERAR está activado. Generando particiones nuevas\")\n",
    "    \n",
    "    df = df.sample(frac=1)\n",
    "    index = df.index.values\n",
    "\n",
    "    testNumber = math.ceil(len(index)/K)\n",
    "    valNumber = math.ceil(0.1 * len(index))\n",
    "    trainNumber = len(index) - valNumber - testNumber\n",
    "    print(valNumber, testNumber, trainNumber)\n",
    "    \n",
    "    assert (valNumber + testNumber + trainNumber) == len(index)\n",
    "\n",
    "    for i in range(0, K):\n",
    "        first_test_index = i * testNumber\n",
    "        last_test_index = (i + 1) * testNumber\n",
    "        print(first_test_index, last_test_index)\n",
    "        \n",
    "        test_set = index[first_test_index : last_test_index]\n",
    "        \n",
    "        resto = np.setdiff1d(index, test_set)\n",
    "        np.random.shuffle(resto)\n",
    "            \n",
    "        val_set = resto[0 : valNumber]\n",
    "        resto = np.setdiff1d(resto, val_set)\n",
    "        np.random.shuffle(resto)\n",
    "                \n",
    "        train_set = resto\n",
    "        \n",
    "        test_sets.append(test_set)\n",
    "        val_sets.append(val_set)\n",
    "        train_sets.append(train_set)\n",
    "\n",
    "        print(f\"Fold {i}\")\n",
    "        print(\"-- Patterns for training: {} - Muestra: {}\".format(len(train_set), sorted(train_set[0:5])))\n",
    "        print(\"- Patterns for validation: {} - Muestra: {}\".format(len(val_set), sorted(val_set[0:5])))\n",
    "        print(\"-- Patterns for testing: {} - Muestra: {}\".format(len(test_set), sorted(test_set[0:5])))\n",
    "        \n",
    "        with open(os.path.join(holdout_dir, f'elem_train_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, train_set)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_val_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, val_set)\n",
    "\n",
    "        with open(os.path.join(holdout_dir, f'elem_test_{i}.npy'), 'wb') as f:\n",
    "            np.save(f, test_set)\n",
    "\n",
    "    # Chequear que la K-Cross-validation está sin overlap en test\n",
    "    for i, set1 in enumerate(test_sets):\n",
    "        for j, set2 in enumerate(test_sets):\n",
    "            if i != j:\n",
    "                assert len( np.intersect1d(set1, set2) ) == 0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## Augmentation\n",
    "(Only over training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14j', '3i', '24b', '37b', '44n', '6e', '29f', '37d', '66a', '2h',\n",
       "       '53g', '50a', '14f', '75l', '80c', '74c', '89d', '93e', '84f',\n",
       "       '30c', '66c', '67l', '37a', '82d', '65b', '76b', '48h', '70f',\n",
       "       '59f', '16k', '94e', '39i', '43a', '67i', '24d', '40e', '16d',\n",
       "       '84e', '88f', '92e', '49f', '38i', '65d', '45a', '70o', '20g',\n",
       "       '96c', '27b', '2d', '87c', '84c', '76d', '12c', '29a', '69b',\n",
       "       '71h', '77a', '70c', '1b', '2a', '75a', '23d', '56b', '88e', '66e',\n",
       "       '84i', '78m', '58c', '91b', '53e', '78i', '39f', '53a', '75j',\n",
       "       '49b', '72a', '38d', '7a', '55c', '93k', '9a', '57f', '40a', '48f',\n",
       "       '74b', '44f', '48b', '77g', '21c', '87f', '1c', '1g', '68b', '17a',\n",
       "       '36g', '65e', '13d', '17b', '17f', '42f', '16b', '75c', '74a',\n",
       "       '49e', '63g', '8d', '33d', '47b', '91f', '84g', '3d', '33c', '80i',\n",
       "       '39e', '75i', '7e', '31d', '69i', '80g', '90d', '15a', '77b',\n",
       "       '65c', '38f', '70b', '3b', '24c', '23g', '34a', '81a', '95b',\n",
       "       '60b', '67h', '85d', '90c', '36d', '76f', '71e', '96h', '83c',\n",
       "       '56c', '63c', '28c', '95a', '85e', '16a', '88g', '15b', '8e',\n",
       "       '81b', '27c', '93g', '20i', '90f', '45e', '61d', '95d', '75f',\n",
       "       '60a', '59e', '62d', '92d', '22b', '91h', '10a', '21e', '39a',\n",
       "       '84h', '86d', '62a', '39j', '65a', '20h', '78d', '77d', '52c',\n",
       "       '40c', '73e', '85g', '68c', '34b', '83e', '41c', '47a', '45c',\n",
       "       '70k', '4c', '87d', '1d', '53d', '11h', '59b', '14g', '91e', '2f',\n",
       "       '3c', '77h', '81j', '57c', '44q', '96e', '14c', '16j', '16g',\n",
       "       '83b', '67k', '12d', '18b', '71j', '49a', '89b', '57g', '82g',\n",
       "       '10c', '78g', '5f', '64e', '66g', '44j', '58g', '82a', '48a',\n",
       "       '55a', '10e', '17g', '46i', '7d', '69c', '93f', '11k', '42c',\n",
       "       '70d', '5a', '12e', '47k', '58e', '50d', '44p', '37f', '91c',\n",
       "       '57e', '3e', '21j', '44c', '18h', '26d', '72b', '43h', '32e',\n",
       "       '55f', '6f', '19f', '93c', '24e', '6a', '46j', '23c', '40g', '95j',\n",
       "       '58h', '83f', '17c', '12a', '69e', '81f', '21b', '53i', '26b',\n",
       "       '60g', '60d', '11f', '36h', '30a', '36l', '64d', '16f', '70p',\n",
       "       '28d', '77i', '31b', '34f', '64g', '47c', '13c', '90g', '31f',\n",
       "       '32a', '70m', '89a', '47m', '46e', '81i', '14b', '67f', '17d',\n",
       "       '37h', '4e', '16i', '76c', '71k', '22a', '59g', '9b', '87e', '43g',\n",
       "       '44e', '39b', '85c', '28j', '11e', '18d', '52a', '49g', '41b',\n",
       "       '11j', '3f', '20d', '33a', '82j', '62b', '67e', '65f', '24a',\n",
       "       '61g', '25d', '59c', '50g', '38g', '20f', '27f', '44o', '36m',\n",
       "       '86b', '11a', '93b', '92a', '86a', '82i', '1h', '25c', '69h',\n",
       "       '33b', '23a', '75e', '58i', '82c', '9c', '39h', '86c', '80f',\n",
       "       '19b', '95m', '81e', '20e', '30b', '43d', '71d', '67m', '40h',\n",
       "       '76e', '38c', '37e', '14e', '50f', '23b', '6d', '2c', '34c', '84d',\n",
       "       '48e', '91d', '14a', '17e', '33e', '52d', '36e', '70h', '25a',\n",
       "       '80d', '92f', '29b', '69d', '32i', '57i', '18a', '62c', '18f',\n",
       "       '18c', '95f', '1f', '72f', '79d', '53f', '89f', '3a', '96g', '30f',\n",
       "       '66b', '70j', '13g', '92b', '19d', '95k', '1j', '78a', '11b',\n",
       "       '57a', '67b', '74d', '64i', '63a', '28a', '96b', '90h', '80e',\n",
       "       '8g', '56a', '37g', '46a', '77c', '91a', '38b', '91i', '20c',\n",
       "       '11c', '58a', '36i', '79c', '10b', '70i', '27a', '94c', '44k',\n",
       "       '82b', '36j', '44i', '35b', '90i', '47i', '88a', '1a', '39c',\n",
       "       '44d', '2i', '8f', '54c', '82e', '32f', '88c', '73b', '66h', '57d',\n",
       "       '54d', '35f', '93i', '53h', '76j', '66f', '28k', '53c', '60f',\n",
       "       '13a', '58b', '93h', '14h', '8b', '44g', '9d', '43f', '57j', '49c',\n",
       "       '78c', '2g', '86e', '71b', '94b', '47g', '84b', '17h', '3h', '40f',\n",
       "       '80h', '89h', '78l', '93a', '28i', '6c', '76g', '71l', '47d',\n",
       "       '57b', '30d', '61b', '20a', '2b', '19h', '95l', '83a', '5d', '46b',\n",
       "       '70r', '67g', '18e', '48d'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270\n",
      "330\n",
      "1273\n",
      "330\n",
      "1261\n",
      "330\n",
      "1257\n",
      "330\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, K):\n",
    "    \n",
    "    new_entries = {}\n",
    "    train_set = list(train_sets[i])\n",
    "    val_set = val_sets[i]\n",
    "    test_set = test_sets[i]\n",
    "        \n",
    "    for pattern in train_set: # only training set\n",
    "        labels = df.loc[[pattern]]\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "\n",
    "        if SUBCHAPTERS:\n",
    "            lbl_class = classes.loc[[pattern]]['subchapter'].values[0]\n",
    "\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"train\", lbl_class)\n",
    "        is_hor = labels['horizontal'].values[0]\n",
    "        is_ver = labels['vertical'].values[0]\n",
    "\n",
    "        if is_hor and is_ver: \n",
    "            pass\n",
    "        \n",
    "        if is_hor and not is_ver:\n",
    "            new_names = apply_transformations(path_in, path_out, HOR_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        elif is_ver and not is_hor:\n",
    "            new_names = apply_transformations(path_in, path_out, VER_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        else: #if not is_hor and not is_ver:\n",
    "            new_names = apply_transformations(path_in, path_out, COMMON_TRANSFORMATIONS)\n",
    "            labels = df.loc[[pattern]].values[0]\n",
    "\n",
    "        for name in new_names:\n",
    "            new_entries[name] = labels\n",
    "\n",
    "        # add the base pattern to the folder\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    for pattern in val_set:\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "        if SUBCHAPTERS:\n",
    "            lbl_class = classes.loc[[pattern]]['subchapter'].values[0]\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"val\", lbl_class)\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    for pattern in test_set:\n",
    "        lbl_class = classes.loc[[pattern]]['chapter'].values[0]\n",
    "        if SUBCHAPTERS:\n",
    "            lbl_class = classes.loc[[pattern]]['subchapter'].values[0]\n",
    "        path_in = os.path.join(root_dir, \"patterns\", \"originals\", pattern, pattern + \".png\")\n",
    "        path_out = os.path.join(root_dir, \"patterns\", data_flags, str(i), \"test\", lbl_class)\n",
    "        os.makedirs(path_out, exist_ok = True)\n",
    "        shutil.copy(path_in, path_out)\n",
    "\n",
    "    # agregar todas las entradas de train a new_entries, y crear \n",
    "    # el dataset \"augmented_train_df.json\"\n",
    "\n",
    "    for p in train_set:\n",
    "      labels = df.loc[p]\n",
    "      new_entries[p] = labels.values\n",
    "\n",
    "    labels_output = os.path.join(labels_dir, data_flags, str(i))\n",
    "\n",
    "    os.makedirs(labels_output, exist_ok = True)\n",
    "    \n",
    "    print(len(new_entries))\n",
    "    print(len(colnames))\n",
    "    \n",
    "    df_train = pd.DataFrame.from_dict(new_entries, columns=colnames, orient='index')\n",
    "    df_train.to_json(os.path.join(labels_output, \"augmented_train_df.json\"), orient='index')\n",
    "\n",
    "    # agregar todas las entradas de val a val_entries, y crear \n",
    "    # el dataset \"val_df.json\"\n",
    "    val_entries = {}\n",
    "    for p in val_set:\n",
    "      labels = df.loc[p]\n",
    "      val_entries[p] = labels.values\n",
    "\n",
    "    df_val = pd.DataFrame.from_dict(val_entries, columns=colnames, orient='index')\n",
    "    df_val.to_json(os.path.join(labels_output, \"val_df.json\"), orient='index')\n",
    "\n",
    "    # agregar todas las entradas de test a test_entries, y crear\n",
    "    # el dataset \"test_df.json\"\n",
    "    test_entries = {}\n",
    "    for p in test_set:\n",
    "      labels = df.loc[p]\n",
    "      test_entries[p] = labels.values\n",
    "\n",
    "    df_test = pd.DataFrame.from_dict(test_entries, columns=colnames, orient='index')\n",
    "    df_test.to_json(os.path.join(labels_output, \"test_df.json\"), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert(df_train.shape[0] + df_test.shape[0] + df_val.shape[0] == 776)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "woMl7NKb3LyB",
    "IRTWM7ng3lwE",
    "lVaAM8Qw3Oo-",
    "boCmALkT3gro",
    "6IsXXq4cAlQp"
   ],
   "name": "Split and augmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
