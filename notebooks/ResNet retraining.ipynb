{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpIfCtDGJ5q-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ResNet Retraining\n",
    "## Seminario de Tesis I, Primavera 2022 \n",
    "### MDS Program. University of Chile.\n",
    "#### Supervisor: Prof. Benjamín Bustos, Prof. Iván Sipirán\n",
    "#### Author: Iván Sipirán, modified by Matías Vergara\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2Z2do8XKiAQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IgrQgam9KgnN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np, scipy.io\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG0IIxH2KuS8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mounting Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    folder_path = 'drive/MyDrive/TesisMV/'\n",
    "except:\n",
    "    folder_path = '../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#modify only this cell\n",
    "USE_RN50 = False\n",
    "DS_FLAGS = ['crop', 'elastic', 'rain']\n",
    "              # 'ref': [invertX, invertY],\n",
    "              # 'rot': [rotate90, rotate180, rotate270],\n",
    "              # 'crop': [crop] * CROP_TIMES,\n",
    "              # 'blur': [blur],\n",
    "              # 'emboss': [emboss],\n",
    "              # 'randaug': [randaug],\n",
    "              # 'rain': [rain],\n",
    "              # 'elastic': [elastic]\n",
    "CROP_TIMES = 1\n",
    "RANDOM_TIMES = 2\n",
    "ELASTIC_TIMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern set encontrado en ../patterns/rain_crop1_elastic1\n",
      "Labels set encontrado en ../labels/rain_crop1_elastic1\n"
     ]
    }
   ],
   "source": [
    "# This cells builds the data_flags variable, that will be used\n",
    "# to map the requestes data treatment to folders\n",
    "MAP_TIMES = {'crop': CROP_TIMES,\n",
    "         'randaug': RANDOM_TIMES,\n",
    "         'elastic': ELASTIC_TIMES,\n",
    "}\n",
    "\n",
    "DS_FLAGS = sorted(DS_FLAGS)\n",
    "data_flags = '_'.join(DS_FLAGS) if len(DS_FLAGS) > 0 else 'base'\n",
    "MULTIPLE_TRANSF = ['crop', 'randaug', 'elastic']\n",
    "COPY_FLAGS = DS_FLAGS.copy()\n",
    "\n",
    "for t in MULTIPLE_TRANSF:\n",
    "    if t in DS_FLAGS:\n",
    "        COPY_FLAGS.remove(t)\n",
    "        COPY_FLAGS.append(t + str(MAP_TIMES[t]))\n",
    "        data_flags = '_'.join(COPY_FLAGS)\n",
    "\n",
    "patterns_path = folder_path + \"patterns/\" + data_flags\n",
    "labels_path = folder_path + \"labels/\" + data_flags\n",
    "if not (os.path.isdir(patterns_path) and os.path.isdir(labels_path)):\n",
    "    raise FileNotFoundError(\"No existen directorios de datos para el conjunto de flags seleccionado. Verifique que el dataset exista y, de lo contrario, llame a Split and Augmentation\")\n",
    "print(\"Pattern set encontrado en {}\".format(patterns_path))\n",
    "print(\"Labels set encontrado en {}\".format(labels_path))\n",
    "OUTPUT_FILENAME = f'resnet50_{data_flags}.pth' if USE_RN50 else f'resnet18_{data_flags}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_output_path = 'models/' + OUTPUT_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(pathDataset + 'train', \n",
    "                                                    transform = transforms.Compose([\n",
    "                                                        transforms.RandomVerticalFlip(),\n",
    "                                                        transforms.RandomHorizontalFlip(),\n",
    "                                                        transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(pathDataset + 'val',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(256),\n",
    "                                                                    transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=30, output_path = 'model.pth', save_each = -1):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds ==  labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "        print('Train Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(val_dataset)\n",
    "        epoch_acc = running_corrects / len(val_dataset)\n",
    "        print('Val Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if save_each > -1 and epoch%save_each == 0:\n",
    "            path = output_path.split(\"/\")\n",
    "            filename =  path[-1]\n",
    "            epoch_filename =filename.split(\".\")[0] + \"_e\" + str(epoch) + \".\" + filename.split(\".\")[1]\n",
    "            new_path = path[:-1]\n",
    "            new_path.append(epoch_filename)\n",
    "            new_path = '/'.join(new_path)\n",
    "            torch.save(model.state_dict(), new_path)\n",
    "            print(\"Saving model at epoch {} as {}\".format(epoch, new_path))\n",
    "\n",
    "    print('Best accuracy: {:.4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/149\n",
      "----------\n",
      "Train Loss: 1.4002  Acc: 0.4987\n",
      "Val Loss: 1.2084  Acc: 0.5256\n",
      "Epoch 1/149\n",
      "----------\n",
      "Train Loss: 1.1463  Acc: 0.5906\n",
      "Val Loss: 1.1555  Acc: 0.6154\n",
      "Epoch 2/149\n",
      "----------\n",
      "Train Loss: 1.0344  Acc: 0.6323\n",
      "Val Loss: 1.0869  Acc: 0.6154\n",
      "Epoch 3/149\n",
      "----------\n",
      "Train Loss: 0.9703  Acc: 0.6583\n",
      "Val Loss: 0.8463  Acc: 0.7308\n",
      "Epoch 4/149\n",
      "----------\n",
      "Train Loss: 0.8911  Acc: 0.6774\n",
      "Val Loss: 0.8249  Acc: 0.7179\n",
      "Epoch 5/149\n",
      "----------\n",
      "Train Loss: 0.8296  Acc: 0.7172\n",
      "Val Loss: 0.6493  Acc: 0.7821\n",
      "Epoch 6/149\n",
      "----------\n",
      "Train Loss: 0.8157  Acc: 0.7167\n",
      "Val Loss: 0.8791  Acc: 0.7051\n",
      "Epoch 7/149\n",
      "----------\n",
      "Train Loss: 0.7807  Acc: 0.7317\n",
      "Val Loss: 0.8142  Acc: 0.7692\n",
      "Epoch 8/149\n",
      "----------\n",
      "Train Loss: 0.7447  Acc: 0.7392\n",
      "Val Loss: 0.7996  Acc: 0.6795\n",
      "Epoch 9/149\n",
      "----------\n",
      "Train Loss: 0.7232  Acc: 0.7452\n",
      "Val Loss: 0.6393  Acc: 0.8077\n",
      "Epoch 10/149\n",
      "----------\n",
      "Train Loss: 0.6877  Acc: 0.7597\n",
      "Val Loss: 0.5829  Acc: 0.8077\n",
      "Epoch 11/149\n",
      "----------\n",
      "Train Loss: 0.6516  Acc: 0.7758\n",
      "Val Loss: 0.5939  Acc: 0.7308\n",
      "Epoch 12/149\n",
      "----------\n",
      "Train Loss: 0.6232  Acc: 0.7841\n",
      "Val Loss: 0.7023  Acc: 0.8077\n",
      "Epoch 13/149\n",
      "----------\n",
      "Train Loss: 0.6346  Acc: 0.7777\n",
      "Val Loss: 0.8535  Acc: 0.7821\n",
      "Epoch 14/149\n",
      "----------\n",
      "Train Loss: 0.5897  Acc: 0.7984\n",
      "Val Loss: 0.7441  Acc: 0.7821\n",
      "Epoch 15/149\n",
      "----------\n",
      "Train Loss: 0.5838  Acc: 0.7952\n",
      "Val Loss: 0.7452  Acc: 0.7179\n",
      "Epoch 16/149\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# change save each and output_path\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m model_ft \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_ft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# save best model\u001B[39;00m\n",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, num_epochs, output_path, save_each)\u001B[0m\n\u001B[0;32m     52\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 53\u001B[0m     running_corrects \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m  \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataset)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1962\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   1960\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m-> 1962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;167;43;01mSyntaxError\u001B[39;49;00m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1963\u001B[0m     \u001B[38;5;66;03m# Though this won't be called by syntax errors in the input\u001B[39;00m\n\u001B[0;32m   1964\u001B[0m     \u001B[38;5;66;03m# line, there may be SyntaxError cases with imported code.\u001B[39;00m\n\u001B[0;32m   1965\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshowsyntaxerror(filename, running_compiled_code)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2003\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2000\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_showtraceback(etype, value, stb)\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m-> 2003\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_exception_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstderr)\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1939\u001B[0m, in \u001B[0;36mInteractiveShell.get_exception_only\u001B[1;34m(self, exc_tuple)\u001B[0m\n\u001B[0;32m   1934\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1935\u001B[0m \u001B[38;5;124;03mReturn as a string (ending with a newline) the exception that\u001B[39;00m\n\u001B[0;32m   1936\u001B[0m \u001B[38;5;124;03mjust occurred, without any traceback.\u001B[39;00m\n\u001B[0;32m   1937\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1938\u001B[0m etype, value, tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_exc_info(exc_tuple)\n\u001B[1;32m-> 1939\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[43mtraceback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_exception_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1940\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(msg)\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\traceback.py:155\u001B[0m, in \u001B[0;36mformat_exception_only\u001B[1;34m(exc, value)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m _sentinel:\n\u001B[0;32m    154\u001B[0m     value \u001B[38;5;241m=\u001B[39m exc\n\u001B[1;32m--> 155\u001B[0m te \u001B[38;5;241m=\u001B[39m \u001B[43mTracebackException\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcompact\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(te\u001B[38;5;241m.\u001B[39mformat_exception_only())\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\traceback.py:552\u001B[0m, in \u001B[0;36mTracebackException.__init__\u001B[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, compact, _seen)\u001B[0m\n\u001B[0;32m    549\u001B[0m     need_context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (e \u001B[38;5;129;01mand\u001B[39;00m e\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m need_context \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mid\u001B[39m(e\u001B[38;5;241m.\u001B[39m__context__) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _seen):\n\u001B[1;32m--> 552\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[43mTracebackException\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__context__\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m        \u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__context__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m        \u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__context__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__traceback__\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlookup_lines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlookup_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_locals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapture_locals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    559\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_seen\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_seen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    561\u001B[0m     context \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\traceback.py:502\u001B[0m, in \u001B[0;36mTracebackException.__init__\u001B[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, compact, _seen)\u001B[0m\n\u001B[0;32m    499\u001B[0m _seen\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;28mid\u001B[39m(exc_value))\n\u001B[0;32m    501\u001B[0m \u001B[38;5;66;03m# TODO: locals.\u001B[39;00m\n\u001B[1;32m--> 502\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack \u001B[38;5;241m=\u001B[39m \u001B[43mStackSummary\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwalk_tb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexc_traceback\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlookup_lines\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlookup_lines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapture_locals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapture_locals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexc_type \u001B[38;5;241m=\u001B[39m exc_type\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# Capture now to permit freeing resources: only complication is in the\u001B[39;00m\n\u001B[0;32m    507\u001B[0m \u001B[38;5;66;03m# unofficial API _format_final_exc_line\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\traceback.py:370\u001B[0m, in \u001B[0;36mStackSummary.extract\u001B[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001B[0m\n\u001B[0;32m    367\u001B[0m name \u001B[38;5;241m=\u001B[39m co\u001B[38;5;241m.\u001B[39mco_name\n\u001B[0;32m    369\u001B[0m fnames\u001B[38;5;241m.\u001B[39madd(filename)\n\u001B[1;32m--> 370\u001B[0m \u001B[43mlinecache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlazycache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_globals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;66;03m# Must defer line lookups until we have called checkcache.\u001B[39;00m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capture_locals:\n",
      "File \u001B[1;32mD:\\Programas\\Anaconda3\\envs\\TesisMV\\lib\\linecache.py:161\u001B[0m, in \u001B[0;36mlazycache\u001B[1;34m(filename, module_globals)\u001B[0m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;124;03m\"\"\"Seed the cache for filename with module_globals.\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \n\u001B[0;32m    150\u001B[0m \u001B[38;5;124;03mThe module loader will be asked for the source only when getlines is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;124;03m    filename, and the filename must not be already cached.\u001B[39;00m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename \u001B[38;5;129;01min\u001B[39;00m cache:\n\u001B[1;32m--> 161\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcache\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    162\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "num_ft = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ft, 6)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "groups = [{'params': model_ft.conv1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.bn1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer1.parameters(),'lr':learning_rate/4},\n",
    "            {'params': model_ft.layer2.parameters(),'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer3.parameters(), 'lr':learning_rate/2},\n",
    "            {'params': model_ft.layer4.parameters(),'lr':learning_rate},\n",
    "            {'params': model_ft.fc.parameters(), 'lr':learning_rate}]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr = 0.0015)\n",
    "\n",
    "output_path = folder_path + model_output_path\n",
    "\n",
    "# change save each and output_path\n",
    "model_ft = train_model(model_ft, criterion, optimizer, num_epochs=150)\n",
    "\n",
    "# save best model\n",
    "torch.save(model_ft.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = '../' + model_output_path\n",
    "#model = '../' + 'models/resnet18_base_each5/resnet18_base_e90.pth'\n",
    "\n",
    "pathDataset = patterns_path + '/'\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(pathDataset + 'test',\n",
    "                                                    transform = transforms.Compose([ transforms.Resize(224),\n",
    "                                                                    #transforms.CenterCrop(224),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std = [0.229, 0.224, 0.225])]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if USE_RN50:\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "num_ft = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ft, 6)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "model_ft.load_state_dict(torch.load(model))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft.eval()\n",
    "running_loss = 0.0\n",
    "running_corrects = 0.0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / len(test_dataset)\n",
    "epoch_acc = running_corrects / len(test_dataset)\n",
    "print('Test Loss: {:.4f}  Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet retraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}